{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Matheart's Note ! \u00b6 site.info Post note about cs & math stuff ~ and also some random things maybe \u53ef\u80fd\u6703\u96a8\u6211\u7684\u7fd2\u6163\u51fa\u73fe\u4e2d\u82f1\u593e\u96dc\uff0c\u751a\u81f3\u7c21\u7e41\u593e\u96dc\u7684\u60c5\u6cc1\uff0c\u53cd\u6b63\u4e5f\u53ea\u662f\u70ba\u4e86\u6211\u81ea\u5df1\u770b\u8457(&\u5beb\u8457)\u8212\u670d about(site.author) HKUST CS & Math 25' B\u7ad9\u79d1\u666eUP\u4e3b HKUST Freshman 2021 Server Owner Table of Contents \u00b6 Computer Science Machine Learning Statistical Learning Generalization Computer Architecture Programming Languages Rust C++ Algorithms Markdown","title":"Welcome to Matheart's Note !"},{"location":"#welcome-to-mathearts-note","text":"site.info Post note about cs & math stuff ~ and also some random things maybe \u53ef\u80fd\u6703\u96a8\u6211\u7684\u7fd2\u6163\u51fa\u73fe\u4e2d\u82f1\u593e\u96dc\uff0c\u751a\u81f3\u7c21\u7e41\u593e\u96dc\u7684\u60c5\u6cc1\uff0c\u53cd\u6b63\u4e5f\u53ea\u662f\u70ba\u4e86\u6211\u81ea\u5df1\u770b\u8457(&\u5beb\u8457)\u8212\u670d about(site.author) HKUST CS & Math 25' B\u7ad9\u79d1\u666eUP\u4e3b HKUST Freshman 2021 Server Owner","title":"Welcome to Matheart's Note !"},{"location":"#table-of-contents","text":"Computer Science Machine Learning Statistical Learning Generalization Computer Architecture Programming Languages Rust C++ Algorithms Markdown","title":"Table of Contents"},{"location":"cs/","text":"Computer Science \u00b6 Warning The code is written under Mac environment. Please note that this note is only for recording what I have learnt so far, so they are for reference only and not 100% correct. Feel free to contact me if you got any questions. Table of Contents \u00b6 Machine Learning Statistical Learning Generalization Computer Architecture Programming Languages Rust C++ Algorithms Markdown","title":"Computer Science"},{"location":"cs/#computer-science","text":"Warning The code is written under Mac environment. Please note that this note is only for recording what I have learnt so far, so they are for reference only and not 100% correct. Feel free to contact me if you got any questions.","title":"Computer Science"},{"location":"cs/#table-of-contents","text":"Machine Learning Statistical Learning Generalization Computer Architecture Programming Languages Rust C++ Algorithms Markdown","title":"Table of Contents"},{"location":"cs/algo/","text":"Algorithm \u00b6 To be implemented in 2023 Spring when I will be taking the COMP3711 course.","title":"Algorithm"},{"location":"cs/algo/#algorithm","text":"To be implemented in 2023 Spring when I will be taking the COMP3711 course.","title":"Algorithm"},{"location":"cs/archi/","text":"Computer Architecture \u00b6 Computer Basics \u00b6 Number System \u00b6 There are mainly three number systems: decimal, binary, and hexadecimal. Decimal (base 10) is used by people, binary (base 2) is used by computers, where 0 represents OFF state (no volatge) and 1 represents ON state (have voltage), hexadecimal (base 16) is a concise representation of binary numbers. Some examples: \\(3525_{10}\\) , \\(100111_{2}\\) , FA289B \\(_{16}\\) . The convertions between them are already taught in high school. Units \u00b6 There are several units e.g.: kilo, mega, giga, tera, peta. But notice that they mean different things in different scenarios. When dealing with size such as memory or file , 1 kilo = 2^10, 1 mega = 2^20. When dealing with rate/frequency such as # instructions per sec, # clock ticks per sec, network speed, 1 kilo = 10^3, 1 mega = 10^6. Class of Computers \u00b6 Personal Computers (PC) General Purpose, variety of software Subject to cost / performance tradeoff Server Computers Network based High capacity, performance, reliability Range from small servers to building sized Supercomputers High-end scientific and engineering calculations Highest capability but represent a small fraction of the overall computer market Embedded Computers Hidden as components of systems Stringest power / performance / cost constraints Levels of Abstractions \u00b6 Hardware => System Software (e.g.: Windows, Linux) => Application Software Both hardware and software are organized into hierarchical layers, in which lower-level details are hidden to offer a simpler view at the higher levels. There are levels of program code to cope with these layers and their interactions as well, namely binary machine language program, assembly language (e.g.: MIPS), and high-level language (e.g.: C/C++). The abstract interface between hardware and software is called Instruction Set Architecture (ISA) (e.g. Intel x86, ARM, MIPS). It can allow different implementations of varying cost and performance to follow the same instruction test architecture. Example: 80x86, Pentium, Pentium II, Pentium III, Pentium 4 all implement the same ISA. Components of Computer \u00b6 Input To communicate with the computer Data and instructions transferred to the memory Output To communicate with the user Data is read from the memory Memory Large store to keep instructions and data Processor Datapath: processes data according to instructions Control: commands the operations of input, output, memory, and datapath according to the instructions Technology Trend \u00b6 Vacuum Tubes (1950s) => Transistors (1950s and 1960s) => Integrated Circuits (1960s and 70s) => Very Large Scale Integrated (VLSI) Circuit (1980s and on) Cost / performance is improving due to underlying technology development. Moore's law: the number of transistors on microclips doubles every two years. This makes novel applications possible such as AI, World Wide Web, search machines ...","title":"Computer Architecture"},{"location":"cs/archi/#computer-architecture","text":"","title":"Computer Architecture"},{"location":"cs/archi/#computer-basics","text":"","title":"Computer Basics"},{"location":"cs/archi/#number-system","text":"There are mainly three number systems: decimal, binary, and hexadecimal. Decimal (base 10) is used by people, binary (base 2) is used by computers, where 0 represents OFF state (no volatge) and 1 represents ON state (have voltage), hexadecimal (base 16) is a concise representation of binary numbers. Some examples: \\(3525_{10}\\) , \\(100111_{2}\\) , FA289B \\(_{16}\\) . The convertions between them are already taught in high school.","title":"Number System"},{"location":"cs/archi/#units","text":"There are several units e.g.: kilo, mega, giga, tera, peta. But notice that they mean different things in different scenarios. When dealing with size such as memory or file , 1 kilo = 2^10, 1 mega = 2^20. When dealing with rate/frequency such as # instructions per sec, # clock ticks per sec, network speed, 1 kilo = 10^3, 1 mega = 10^6.","title":"Units"},{"location":"cs/archi/#class-of-computers","text":"Personal Computers (PC) General Purpose, variety of software Subject to cost / performance tradeoff Server Computers Network based High capacity, performance, reliability Range from small servers to building sized Supercomputers High-end scientific and engineering calculations Highest capability but represent a small fraction of the overall computer market Embedded Computers Hidden as components of systems Stringest power / performance / cost constraints","title":"Class of Computers"},{"location":"cs/archi/#levels-of-abstractions","text":"Hardware => System Software (e.g.: Windows, Linux) => Application Software Both hardware and software are organized into hierarchical layers, in which lower-level details are hidden to offer a simpler view at the higher levels. There are levels of program code to cope with these layers and their interactions as well, namely binary machine language program, assembly language (e.g.: MIPS), and high-level language (e.g.: C/C++). The abstract interface between hardware and software is called Instruction Set Architecture (ISA) (e.g. Intel x86, ARM, MIPS). It can allow different implementations of varying cost and performance to follow the same instruction test architecture. Example: 80x86, Pentium, Pentium II, Pentium III, Pentium 4 all implement the same ISA.","title":"Levels of Abstractions"},{"location":"cs/archi/#components-of-computer","text":"Input To communicate with the computer Data and instructions transferred to the memory Output To communicate with the user Data is read from the memory Memory Large store to keep instructions and data Processor Datapath: processes data according to instructions Control: commands the operations of input, output, memory, and datapath according to the instructions","title":"Components of Computer"},{"location":"cs/archi/#technology-trend","text":"Vacuum Tubes (1950s) => Transistors (1950s and 1960s) => Integrated Circuits (1960s and 70s) => Very Large Scale Integrated (VLSI) Circuit (1980s and on) Cost / performance is improving due to underlying technology development. Moore's law: the number of transistors on microclips doubles every two years. This makes novel applications possible such as AI, World Wide Web, search machines ...","title":"Technology Trend"},{"location":"cs/markdown/","text":"Markdown \u00b6 First Paragraph \u00b6 Info This place is reserved for self-studying markdown syntax. Debug bruh Help bruh Solution bruh Notice bruh Abstract bruh Language English Mandarin Cantonese 1 Why \u4e3a\u4ec0\u4e48 \u9ede\u89e3 2 good morning \u65e9\u4e0a\u597d \u65e9\u6668 C++ int gcd ( int a , int b ) { return b == 0 ? a : gcd ( b , a % b ); } Python for i in range ( 100 ): np . sum ( a [ i ], axis = 0 ) Rust fn makes_copy ( some_integer : i32 ) { // some_integer comes into scope println! ( \"{}\" , some_integer ); } \u6e90\u7801\uff1a ridiculousfish/cdecl-blocks a b c d Hello, \\(a^2 + b^2 = c^2\\) $$ a^2 + c^2 $$","title":"Markdown"},{"location":"cs/markdown/#markdown","text":"","title":"Markdown"},{"location":"cs/markdown/#first-paragraph","text":"Info This place is reserved for self-studying markdown syntax. Debug bruh Help bruh Solution bruh Notice bruh Abstract bruh Language English Mandarin Cantonese 1 Why \u4e3a\u4ec0\u4e48 \u9ede\u89e3 2 good morning \u65e9\u4e0a\u597d \u65e9\u6668 C++ int gcd ( int a , int b ) { return b == 0 ? a : gcd ( b , a % b ); } Python for i in range ( 100 ): np . sum ( a [ i ], axis = 0 ) Rust fn makes_copy ( some_integer : i32 ) { // some_integer comes into scope println! ( \"{}\" , some_integer ); } \u6e90\u7801\uff1a ridiculousfish/cdecl-blocks a b c d Hello, \\(a^2 + b^2 = c^2\\) $$ a^2 + c^2 $$","title":"First Paragraph"},{"location":"cs/ml/","text":"Machine Learning \u00b6 Warning The code is written under Mac environment. Please note that this note is only for recording what I have learnt so far, so they are for reference only and not 100% correct. Feel free to contact me if you got any questions. Table of Contents \u00b6 Statistical Learning Generalization","title":"Machine Learning"},{"location":"cs/ml/#machine-learning","text":"Warning The code is written under Mac environment. Please note that this note is only for recording what I have learnt so far, so they are for reference only and not 100% correct. Feel free to contact me if you got any questions.","title":"Machine Learning"},{"location":"cs/ml/#table-of-contents","text":"Statistical Learning Generalization","title":"Table of Contents"},{"location":"cs/ml/generalization/","text":"Domain Generalization \u00b6 Note This is a note on domain generalization in order to prepare for entering statml research group. Introduction \u00b6 Many applications do not fulfill the iid assumption and the machine learning systems often fail to generalize out-of-distribution in some scenarios such as self-driving car system, medical data etc. This motivates the development of a new area called domain generalization , which gets access to multiple datasets collected under different domains and incorporates the invariances into a classifier, and make the classifier have better out-of-distribution performance comparing with previous literatures. Domainbed \u00b6 A popular benchmark framework for domain generalization . Data sets \u00b6 There are several data sets available in Domainbed as listed in the following and more: In the conditional offer problem I am required to do experiments on the PACS data sets, there are four domains: art, clipart, product, and photo respectively. So the experients are required to repeat four times by setting one of them as testing domain and the remaining three are training domains, this is also called leave-one-out. Baseline algorithms \u00b6","title":"Domain Generalization"},{"location":"cs/ml/generalization/#domain-generalization","text":"Note This is a note on domain generalization in order to prepare for entering statml research group.","title":"Domain Generalization"},{"location":"cs/ml/generalization/#introduction","text":"Many applications do not fulfill the iid assumption and the machine learning systems often fail to generalize out-of-distribution in some scenarios such as self-driving car system, medical data etc. This motivates the development of a new area called domain generalization , which gets access to multiple datasets collected under different domains and incorporates the invariances into a classifier, and make the classifier have better out-of-distribution performance comparing with previous literatures.","title":"Introduction"},{"location":"cs/ml/generalization/#domainbed","text":"A popular benchmark framework for domain generalization .","title":"Domainbed"},{"location":"cs/ml/generalization/#data-sets","text":"There are several data sets available in Domainbed as listed in the following and more: In the conditional offer problem I am required to do experiments on the PACS data sets, there are four domains: art, clipart, product, and photo respectively. So the experients are required to repeat four times by setting one of them as testing domain and the remaining three are training domains, this is also called leave-one-out.","title":"Data sets"},{"location":"cs/ml/generalization/#baseline-algorithms","text":"","title":"Baseline algorithms"},{"location":"cs/ml/statml/","text":"Machine Learning \u00b6 Info Some of the examples and sentences here are directly adopted from MATH4432 Statistical Machine Learning lecture slides. The note is not complete and may have frequent modifications, and the knowledge about regression analysis is ignored as this is not the focus of the course. Statistical learning (\u7edf\u8ba1\u5b66\u4e60) refers to learn from data, can be classified as supervised, semi-supervised and unsupervised: supervised (with labeled output): prediction, estimation unsupervised (without labeled output): clustering semi-supervised (large amount of unlabeled data and small amount of labeled data) For prediction there are two types as well, regression (\u56de\u5f52) refers to continuous or quantitative output value, otherwise would be classification (\u5206\u7c7b). Linear regression is the simplest regression method by using linear equations to approximate a certain function. Mathematical Formulations \u00b6 \\(n\\) : Number of distinct data points \\(p\\) : Number of features / predictors / variables For instance, regarding Wage , there are 300 sample points where they are 10 factors like year , age , race , ... Then we have \\(n = 300\\) , \\(p = 10\\) . We can use matrix \\(\\textbf{X}\\) to denote the data, and vector \\(\\textbf{y}\\) to denote output: \\[\\begin{aligned} \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1 p} \\\\ x_{21} & x_{22} & \\cdots & x_{2 p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n 1} & x_{n 2} & \\cdots & x_{n p} \\end{array}\\right] \\ \\ \\ \\mathbf{y}=\\left(\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{array}\\right) \\end{aligned}\\] The rows of \\(\\textbf{X}\\) refer to each data point, we can denote it as \\(x_i\\) : x i = ( x i 1 x i 2 \u22ee x i p ) The columns of \\(\\textbf{x}\\) refer to the collection of data points in terms of a certain feature, we can denote it as \\(\\textbf{x}_j\\) : x j = ( x 1 j x 2 j \u22ee x n j ) Therefore, X = ( x 1 x 2 \u22ef x p ) = ( x 1 T x 2 T \u22ee x n T ) We can give this formula: $$ \\textbf{y} = f(\\textbf{X}) + \\varepsilon $$ \\(\\textbf{y}\\) is the observed output while \\(\\textbf{X}\\) is the observed input, \\(f\\) is the ground truth function that maps \\(\\textbf{X}\\) into the ideal output, there's an error between observed and ideal output, and we have the assumption that \\(\\mathbb{E}(\\varepsilon)=0\\) . A set of inputs \\(\\textbf{X}\\) is always available but it is not always easy for us to obtain the corresponding \\(\\textbf{y}\\) . We denote \\(\\hat{\\textbf{y}}\\) as an estimate of \\(\\textbf{y}\\) , and \\(\\hat{f}(\\textbf{X})\\) an estimate of \\(f(\\textbf{X})\\) . Basics \u00b6 Reducible and irreducible error \u00b6 Then with \\(\\mathbb{E}(\\varepsilon)=0\\) and an additional assumption that both \\(\\hat{f}\\) and \\(X\\) is fixed for a moment, we have: \\[\\begin{align} \\mathbb{E}(Y-\\hat{Y})^2 & = \\mathbb{E}[f(X) +\\varepsilon - \\hat{f}(X)]^2\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + 2 \\mathbb{E}((f(X)-\\hat{f}(X))\\varepsilon) + \\mathbb{E}(\\varepsilon^2)\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\mathbb{E}(\\varepsilon^2) - \\mathbb{E}(\\varepsilon)^2\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\text{Var}(\\varepsilon) \\end{align}\\] \\(\\mathbb{E}[f(X) - \\hat{f}(X)]^2\\) is the error that we could reduce by selecting the most appropriate model, but \\(\\text{Var}(\\varepsilon)\\) is the irreducible error. Model choosing \u00b6 To minimize the reducible error, we need to pick a suitable model to estimate model, there are two types of model, namely parametric and non-parametric. For the parametric method, we first make an assumption about the form of the ground truth function e.g. linear. After the model has been selected, we use the training data to fit the model (train the parameters). A simple example: we assume \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\) . The the model is trained to find the values of the parameters: \\(\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_p\\) s.t. \\(Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\) . The most common approach is called ordinary least squares which we will introduce in next section. Non-parametric methods do not make explicit assumptions about the function form of \\(f\\) , so it is more flexible than non-parametric methods. Instead, they seek an estimate of \\(f\\) s.t. it gets as close to the data points as possible without being too rough or wiggly. As the photo has shown, the top-left is the ground truth function, the top-right is the parametric liner model (which is underfitting in this case), the bottom-left is the fitted spline model, and the bottom-right is a rough spline model with zero errors on the training data (which is called overfitting ). The definition of underfitting (not flexible enough) is that it performs poor in both training and testing data, while the definition of overfitting (too flexible) is that it performs very well in training data but poor in the testing data. We will further illustrate this in the bias-varience tradeoff part. The red curve is testing error and grey curve is training error. As you can see, when flexibility increases, the grey curve gradually fits into the noise, the training error reaches minimum point when flexibility equals to 5. Note Note that model flexibility is not always proportional to the number of parameters. This holds for linear regression though. Although we have many available models, there is no free lunch in statistics , which means there doesn't exist the so-called best model for all cases. On a particular data set, one specific method may work best, but some other method may work better on a similar but different set. Measuring the quality of fit \u00b6 We typically use mean-squared-error (MSE) given by \\[\\text{MSE} = \\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}}\\] There are train and test MSEs and our main interest is to choose the method that minimise test MSE . To avoid the curve being too flexible, we normally use the smoothing spline method , where we add a special regularisation term: \\[\\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}} + \\lambda \\int \\hat{f}''(t) \\ dt\\] \\(\\lambda\\) is the tuning parameter, when it is zero, we could allow the intergal value to be very large, so the model can be very flexible i.e. \\(y_i = \\hat{f}(x_i)\\) , when it tends to infinity, as we want to minimize MSE, the model would be very rigid i.e. \\(y_i = \\hat{a}x_i + \\hat{b}\\) . So we can adjust the flexibility of the model by tuning the parameter. Bias & Variance tradeoff \u00b6 As it is impossible for us to obtain all data, we could only obtain some training data \\(D\\) , but we want the model to work for all possible scenarios, so we want to find \\(\\hat{f}\\) to minimise \\[\\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\] ( \\(\\hat{f}(x;D)\\) is the estimated function from the data set \\(D\\) ) Notice Actually we can take one additional expectation w.r.t. \\(x_0\\) here but it would make the formula too complicated. Then we can derive the formula: \\[\\begin{align} & \\ [f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) + \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) \\ | \\ X = x_0]^2 + [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ &+ 2 \\ [f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0] \\ [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] \\end{align}\\] The first term is a constant. Inside the second term \\(\\hat{f}(x_0;D)\\) is still a random variable. For the third term, \\([f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0]\\) is a constant but \\([\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]\\) is a random variable. Then we take expectation w.r.t. \\(D\\) on both sides and we got the third term equal to 0 since \\[\\mathbb{E}_{D} [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] = \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) = 0\\] And the final result is \\[\\begin{align} & \\ \\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ \\underbrace{[f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D))] \\ | \\ X = x_0]^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_{D}{[\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2}}_{\\text{Variance}}\\\\ \\end{align}\\] This formula tells us that we need to balance both bias and variance instead of only focusing on minimizing bias. Bias refers to the error that is introduced by approximating a real-life problem, which may ne extremely complicated, by a much simpler model. The formula is quite easy to understand, is the difference between the ground truth value and the predicted value. Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. Ideally the estimate for \\(f\\) should not vary too much between training sets. The formula is basically a slightly complex version of \\(\\mathbb{E}[(X - \\mathbb{E}(X))^2]\\) . For simple model, it has low variance but large bias. Example: \\(Y = f(X) + \\varepsilon\\) , we simply let \\(\\hat{f}(X) = 0\\) , then variance is 0, but bias is very large \\(f(X)^2\\) . For very flexible model, it has high variance but low bias. Example: \\(Y = f(X) + \\varepsilon = 0 + \\varepsilon\\) , where the ground truth function is \\(0\\) , we let \\(\\hat{f}(x_i) = y_i\\) , which means it fits into the noise. Then \\(\\text{Var}(\\hat{f}(x_i)) = \\text{Var}(y_i) = \\text{Var}(\\varepsilon_i) = \\sigma_i^2\\) , bias is \\(f(x_i) - \\mathbb{E}(\\hat{f}(x_i)) = f(x_i) - \\mathbb{E}(y_i) = 0\\) . Regression \u00b6 Simple Linear Regression \u00b6 Let \\((x_1,y_1)\\) , \\((x_2, y_2)\\) , \\(\\cdots\\) , \\((x_n,y_n)\\) represent \\(n\\) observation pairs, and we use simple linear regression \\(\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i\\) to fit the data. The error is defined as \\(e_i = y_i - \\hat{y_i}\\) , then the residual sum of squares (RSS) is $$ e_1^2 + e_2^2 + \\cdots + e_n^2 = \\sum_{i=1}^{n}{(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2} $$ To minimise the error, we simply let \\(\\frac{\\partial (\\text{RSS})}{\\partial \\hat{\\beta_0}} = \\frac{\\partial (\\text{RSS})}{\\partial \\hat{\\beta_1}} = 0\\) , then with some algebraic operations we obtain: $$ \\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}{(x_i - \\overline{x})(y_i - \\overline{y})}}{\\sum_{i=1}^{n}{(x_i - \\overline{x})^2}} \\text{, } \\hat{\\beta_0} = \\overline{y} - \\hat{\\beta_1} \\overline{x} $$ Actually, when we are drawing the contour map of RSS w.r.t. \\(\\beta_0, \\beta_1\\) , we can obtain the following ellipsoid shape: This is actually because of the squared term \\((y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2\\) , term like this such as \\(\\beta_0^2 + \\beta_1^2 = 1\\) can be written in the quadratic form (\u4e8c\u6b21\u578b): \\[\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}^{T} \\begin{bmatrix} 1&0 \\\\ 0&1 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\] The quadratic form like \\(\\beta^{T}A \\beta = C\\) with A being positive and definite (\u6b63\u5b9a) has ellipsoid shape. After we have computed the values of coefficients, we can use our statistical knowledge to assess the accuracy of estimates. We assume the true relationship between \\(X\\) and \\(Y\\) is the following: $$ Y = f(X) + \\varepsilon, f(X) = \\beta_0 + \\beta_1 X $$ Then as the graph shows Red line is the true relationship, which is known as population regression line , other lines are least squares lines computed based on separate random set of observations. Each line is different but on average the least squares lines are quite close to the population regression line. We can use the residual standard error (RSE) to provide an absolute measure of lack of fit of the linear model (number of freedoms equals to the total number of parameters minus the two we want to estimate): $$ \\text{RSE} = \\sqrt{\\frac{1}{\\text{degree of freedom}} \\text{RSS}} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}} $$ R^2 statistics provides an alternative measure of fit. $$ \\text{TSS} = \\sum{(y_i - \\overline{y})^2}, \\ \\text{RSS} = \\sum{(y_i - \\hat{y_i})^2} $$ And \\(R^2\\) is given by \\[ R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\] The value is always between 0 and 1, if it is closed to 0, it means the prediction has similar effect as mean value, so it doesn't fit the data well; when it is closed to 1, it means the RSS is very small, that means it fits the data well. Multiple Linear Regression \u00b6 Derivation of formula \u00b6 The equation becomes much more complicated than before: $$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon $$ Then our goal is to find \\(\\hat{\\beta} = [\\hat{\\beta_0}, \\hat{\\beta_1}, \\cdots, \\hat{\\beta_p}]\\) be the least squares \\[ \\hat{\\beta} = \\text{argmin}_{\\beta = [\\beta_0, \\beta_1, \\cdots, \\beta_p]}{\\sum_{i=1}^{n}{(y_i - \\beta_0 - \\beta_1 x_{i1} - \\cdots - \\beta_{p} x_{ip})^2}} \\] We know \\[\\begin{aligned} \\mathbf{X}=\\left[\\begin{array}{ccccc} \\ 1 & x_{11} & x_{12} & \\cdots & x_{1 p} \\\\ \\ 1 & x_{21} & x_{22} & \\cdots & x_{2 p} \\\\ \\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\ 1 & x_{n 1} & x_{n 2} & \\cdots & x_{n p} \\end{array}\\right] \\ \\ \\ \\mathbf{y}=\\left(\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{array}\\right) \\end{aligned}\\] where the first column is representing the constant part (would time \\(\\beta_0\\) later). Then the sum we want to minimise can be simplified as \\(\\sum_{i=1}^{n}{(y_i - x_{\\text{row i}} \\ \\beta)^2} \\Rightarrow ||\\mathbf{y} - \\mathbf{X} \\beta||_2^2\\) Note \\(x_{\\text{row i}}\\) is row vector, \\(\\beta\\) is column vector \\(\\mathbf{X}: n \\times (1 + p)\\) , \\(\\mathbf{y}: n \\times 1\\) , \\(\\beta: (1 + p) \\times 1\\) \\[\\begin{align} &||\\mathbf{y}-\\mathbf{X}\\beta||\\\\ =& \\ (\\mathbf{y}-\\mathbf{X}\\beta)^{T} (\\mathbf{y}-\\mathbf{X}\\beta)\\\\ =& \\ (\\mathbf{y}^T - \\beta^T \\mathbf{X}^T) (\\mathbf{y}-\\mathbf{X}\\beta)\\\\ =& \\ \\mathbf{y}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} \\beta \\ - \\beta^T \\mathbf{X}^T \\mathbf{y} + \\beta^T \\mathbf{X}^T \\mathbf{X}\\beta\\\\ \\end{align}\\] By dimension of \\(\\mathbf{X}\\) , \\(\\mathbf{y}\\) and \\(\\beta\\) , we know all four of the above are scalar, and in which \\(\\mathbf{y}^T \\mathbf{X} \\beta\\) is the transpose of \\(\\beta^T \\mathbf{X}^T \\mathbf{y}\\) , so they are supposed to have the same value: $$ \\hat{\\beta} = \\text{argmin}_{\\beta}{ (\\mathbf{y}^T \\mathbf{y} - 2\\mathbf{y}^T \\mathbf{X} \\beta + \\beta^T \\mathbf{X}^T \\mathbf{X}\\beta)} $$ We take partial derivative w.r.t. \\(\\beta\\) : \\[ 0 - 2 \\mathbf{X}^T \\mathbf{y} + 2 \\mathbf{X}^T \\mathbf{X} \\beta = 0 \\] Note \\[\\frac{\\partial a^T X}{\\partial X} = a\\] \\[\\frac{\\partial X^T A X}{\\partial X} = (A + A^{T}) X\\] Then \\[ \\mathbf{X}^T \\mathbf{X} \\beta = \\mathbf{X}^T \\mathbf{y} \\] So we finally obtain \\[\\begin{align} \\hat{\\beta} & = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^{T} \\textbf{y}\\\\ \\hat{y} & = X \\hat{\\beta} \\end{align}\\] Derive the mean and variance of beta estimate \u00b6 By \\(\\mathbf{y} = \\mathbf{X}\\beta + \\varepsilon\\) and \\(\\varepsilon\\) is a random noise vector, we know \\(\\mathbf{y}\\) is a random vector, so \\(\\hat \\beta\\) is a random vector, and what follows is that \\(\\hat y\\) is a random vector as well. With this, we can compute the mean and variance of \\(\\hat \\beta\\) . \\[\\begin{align} \\mathbb{E}(\\hat \\beta) &= \\mathbb{E} [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}]\\\\ &= \\mathbb{E} [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{X} \\beta + \\varepsilon)]\\\\ &= \\mathbb{E} [\\mathbf{X}^{-1} (\\mathbf{X}^T)^{-1} \\mathbf{X}^T \\mathbf{X} \\beta \\ + (\\mathbf{X}^T \\mathbf{X}^{-1})\\mathbf{X}^T \\varepsilon]\\\\ &= \\beta + (\\mathbf{X}^T \\mathbf{X}^{-1})\\mathbf{X}^T\\mathbb{E}(\\varepsilon)\\\\ &= \\beta \\end{align}\\] Note \\[\\text{Var}(\\mathbf{z}) = \\mathbb{E}(\\mathbf{z} \\mathbf{z}^T) - \\mathbb{E}(\\mathbf{z}) \\mathbb{E}^{T}(\\mathbf{z})\\] \\[\\begin{align} \\text{Var}(\\hat \\beta) &= \\text{Var}\\ [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}]\\\\ &= \\text{Var}\\ [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{X}\\beta + \\varepsilon)]\\\\ &= \\text{Var}\\ [\\beta + (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon]\\\\ &= \\text{Var}\\ [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon]\\\\ &= \\mathbb{E}[(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon \\ \\cdot \\varepsilon^T \\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1}] -\\underbrace{\\mathbb{E}[(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon]}_{0} \\cdot \\mathbb{E}^{T}[(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon]\\\\ &= (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\ \\mathbb{E}[ \\varepsilon \\cdot \\varepsilon^T] \\ \\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1}\\\\ &= (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\ \\sigma_{\\varepsilon}^{2} \\ \\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1}\\\\ &= \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^T \\mathbf{X})^{-1} \\end{align}\\] Derivation By independence of each element inside the random vector \\(\\varepsilon\\) , \\(\\text{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^{2} \\ I_{n \\times n}\\) . \\[ \\sigma_{\\varepsilon}^{2} \\ I_{n \\times n} = \\text{Var}(\\varepsilon) = \\mathbb{E}(\\varepsilon \\varepsilon^T) - \\mathbb{E}(\\varepsilon) \\mathbb{E}^{T}(\\varepsilon) = \\mathbb{E}(\\varepsilon \\varepsilon^T) \\] e.g.: \\(\\varepsilon = \\begin{pmatrix} \\varepsilon_1\\\\ \\varepsilon_2 \\end{pmatrix}\\) , \\(\\text{cov}\\ \\varepsilon = \\begin{pmatrix} \\sigma_{\\varepsilon}^{2} & 0\\\\ 0 & \\sigma_{\\varepsilon}^{2} \\end{pmatrix}\\) Geometrical Interpretation of least squares regression \u00b6 Like the graph suggests, \\(\\text{col} \\ \\mathbf{X}\\) is the hyperplane spanned by column vectors of \\(\\mathbf{X}\\) . \\(\\mathbf{X} \\hat\\beta\\) could be considered as the linear combination of column vectors. Then our goal is to find \\(\\hat \\beta\\) to minimise the norm of \\(\\mathbf{y} - \\mathbf{X} \\hat{\\beta}\\) , which can only be achieved when it is perpendicular to the hyperplane, that is, \\(\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\hat{\\beta}) = 0\\) . Verification: \\[\\begin{align} &\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y})\\\\ =& \\ \\mathbf{X}^T (\\mathbf{I} - \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T) \\mathbf{y}\\\\ =& \\ (\\mathbf{X}^T - \\mathbf{X}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T) \\mathbf{y}\\\\ =& \\ 0 \\end{align}\\] There are several important issues we need to consider for linear regression. The first issue is that whether there is a relationship between the response and predictors, in other word, is at least one of the predictors useful in predicting the response. We can test the null hypothesis \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\) versus the alternative \\(H_a\\) : at least one \\(\\beta_j\\) is non-zero. This can be performed by using the F-statistics. Sometimes we only to test a particular subset of coefficients though. In hypothesis testing, type I error (expected error) and type II error (false negative) matter as well. The second issue is deciding on important variables, this process is called variable selection . Ideally we can do that by trying a lot of different models and evaluating their qualities using methods like Akaike information criterion (AIC), Bayesian information criterion (BIC), adjusted \\(R^2\\) etc. But in reality we cannot really try and test so many models, there are three classical approaches for this task: forward selection, backward selection, mixed selection. Lasso is one of the approach appeared in later years. The third issue is model fit. The fourth issue is prediction. Classification \u00b6 This is the special case where outputs are discrete. Logistic regression, linear discrimant analysis and KNN would be investigated. // image here Under such scenario, linear regression does not work ideally, so we need to introduce new models. Logistic Regression \u00b6 We let \\[\\begin{cases} \\mathbb{P}(y_i = 1 | x_i) = \\frac{1}{1 + \\exp(-x_i^T \\beta)} \\textbf{ (sample version)}\\\\ \\mathbb{P}(Y = 1 | X) = \\frac{1}{1 + \\exp(-\\beta^T X)} \\textbf{ (population version)} \\end{cases}\\]","title":"Machine Learning"},{"location":"cs/ml/statml/#machine-learning","text":"Info Some of the examples and sentences here are directly adopted from MATH4432 Statistical Machine Learning lecture slides. The note is not complete and may have frequent modifications, and the knowledge about regression analysis is ignored as this is not the focus of the course. Statistical learning (\u7edf\u8ba1\u5b66\u4e60) refers to learn from data, can be classified as supervised, semi-supervised and unsupervised: supervised (with labeled output): prediction, estimation unsupervised (without labeled output): clustering semi-supervised (large amount of unlabeled data and small amount of labeled data) For prediction there are two types as well, regression (\u56de\u5f52) refers to continuous or quantitative output value, otherwise would be classification (\u5206\u7c7b). Linear regression is the simplest regression method by using linear equations to approximate a certain function.","title":"Machine Learning"},{"location":"cs/ml/statml/#mathematical-formulations","text":"\\(n\\) : Number of distinct data points \\(p\\) : Number of features / predictors / variables For instance, regarding Wage , there are 300 sample points where they are 10 factors like year , age , race , ... Then we have \\(n = 300\\) , \\(p = 10\\) . We can use matrix \\(\\textbf{X}\\) to denote the data, and vector \\(\\textbf{y}\\) to denote output: \\[\\begin{aligned} \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1 p} \\\\ x_{21} & x_{22} & \\cdots & x_{2 p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n 1} & x_{n 2} & \\cdots & x_{n p} \\end{array}\\right] \\ \\ \\ \\mathbf{y}=\\left(\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{array}\\right) \\end{aligned}\\] The rows of \\(\\textbf{X}\\) refer to each data point, we can denote it as \\(x_i\\) : x i = ( x i 1 x i 2 \u22ee x i p ) The columns of \\(\\textbf{x}\\) refer to the collection of data points in terms of a certain feature, we can denote it as \\(\\textbf{x}_j\\) : x j = ( x 1 j x 2 j \u22ee x n j ) Therefore, X = ( x 1 x 2 \u22ef x p ) = ( x 1 T x 2 T \u22ee x n T ) We can give this formula: $$ \\textbf{y} = f(\\textbf{X}) + \\varepsilon $$ \\(\\textbf{y}\\) is the observed output while \\(\\textbf{X}\\) is the observed input, \\(f\\) is the ground truth function that maps \\(\\textbf{X}\\) into the ideal output, there's an error between observed and ideal output, and we have the assumption that \\(\\mathbb{E}(\\varepsilon)=0\\) . A set of inputs \\(\\textbf{X}\\) is always available but it is not always easy for us to obtain the corresponding \\(\\textbf{y}\\) . We denote \\(\\hat{\\textbf{y}}\\) as an estimate of \\(\\textbf{y}\\) , and \\(\\hat{f}(\\textbf{X})\\) an estimate of \\(f(\\textbf{X})\\) .","title":"Mathematical Formulations"},{"location":"cs/ml/statml/#basics","text":"","title":"Basics"},{"location":"cs/ml/statml/#reducible-and-irreducible-error","text":"Then with \\(\\mathbb{E}(\\varepsilon)=0\\) and an additional assumption that both \\(\\hat{f}\\) and \\(X\\) is fixed for a moment, we have: \\[\\begin{align} \\mathbb{E}(Y-\\hat{Y})^2 & = \\mathbb{E}[f(X) +\\varepsilon - \\hat{f}(X)]^2\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + 2 \\mathbb{E}((f(X)-\\hat{f}(X))\\varepsilon) + \\mathbb{E}(\\varepsilon^2)\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\mathbb{E}(\\varepsilon^2) - \\mathbb{E}(\\varepsilon)^2\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\text{Var}(\\varepsilon) \\end{align}\\] \\(\\mathbb{E}[f(X) - \\hat{f}(X)]^2\\) is the error that we could reduce by selecting the most appropriate model, but \\(\\text{Var}(\\varepsilon)\\) is the irreducible error.","title":"Reducible and irreducible error"},{"location":"cs/ml/statml/#model-choosing","text":"To minimize the reducible error, we need to pick a suitable model to estimate model, there are two types of model, namely parametric and non-parametric. For the parametric method, we first make an assumption about the form of the ground truth function e.g. linear. After the model has been selected, we use the training data to fit the model (train the parameters). A simple example: we assume \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\) . The the model is trained to find the values of the parameters: \\(\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_p\\) s.t. \\(Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\) . The most common approach is called ordinary least squares which we will introduce in next section. Non-parametric methods do not make explicit assumptions about the function form of \\(f\\) , so it is more flexible than non-parametric methods. Instead, they seek an estimate of \\(f\\) s.t. it gets as close to the data points as possible without being too rough or wiggly. As the photo has shown, the top-left is the ground truth function, the top-right is the parametric liner model (which is underfitting in this case), the bottom-left is the fitted spline model, and the bottom-right is a rough spline model with zero errors on the training data (which is called overfitting ). The definition of underfitting (not flexible enough) is that it performs poor in both training and testing data, while the definition of overfitting (too flexible) is that it performs very well in training data but poor in the testing data. We will further illustrate this in the bias-varience tradeoff part. The red curve is testing error and grey curve is training error. As you can see, when flexibility increases, the grey curve gradually fits into the noise, the training error reaches minimum point when flexibility equals to 5. Note Note that model flexibility is not always proportional to the number of parameters. This holds for linear regression though. Although we have many available models, there is no free lunch in statistics , which means there doesn't exist the so-called best model for all cases. On a particular data set, one specific method may work best, but some other method may work better on a similar but different set.","title":"Model choosing"},{"location":"cs/ml/statml/#measuring-the-quality-of-fit","text":"We typically use mean-squared-error (MSE) given by \\[\\text{MSE} = \\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}}\\] There are train and test MSEs and our main interest is to choose the method that minimise test MSE . To avoid the curve being too flexible, we normally use the smoothing spline method , where we add a special regularisation term: \\[\\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}} + \\lambda \\int \\hat{f}''(t) \\ dt\\] \\(\\lambda\\) is the tuning parameter, when it is zero, we could allow the intergal value to be very large, so the model can be very flexible i.e. \\(y_i = \\hat{f}(x_i)\\) , when it tends to infinity, as we want to minimize MSE, the model would be very rigid i.e. \\(y_i = \\hat{a}x_i + \\hat{b}\\) . So we can adjust the flexibility of the model by tuning the parameter.","title":"Measuring the quality of fit"},{"location":"cs/ml/statml/#bias-variance-tradeoff","text":"As it is impossible for us to obtain all data, we could only obtain some training data \\(D\\) , but we want the model to work for all possible scenarios, so we want to find \\(\\hat{f}\\) to minimise \\[\\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\] ( \\(\\hat{f}(x;D)\\) is the estimated function from the data set \\(D\\) ) Notice Actually we can take one additional expectation w.r.t. \\(x_0\\) here but it would make the formula too complicated. Then we can derive the formula: \\[\\begin{align} & \\ [f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) + \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) \\ | \\ X = x_0]^2 + [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ &+ 2 \\ [f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0] \\ [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] \\end{align}\\] The first term is a constant. Inside the second term \\(\\hat{f}(x_0;D)\\) is still a random variable. For the third term, \\([f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0]\\) is a constant but \\([\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]\\) is a random variable. Then we take expectation w.r.t. \\(D\\) on both sides and we got the third term equal to 0 since \\[\\mathbb{E}_{D} [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] = \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) = 0\\] And the final result is \\[\\begin{align} & \\ \\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ \\underbrace{[f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D))] \\ | \\ X = x_0]^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_{D}{[\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2}}_{\\text{Variance}}\\\\ \\end{align}\\] This formula tells us that we need to balance both bias and variance instead of only focusing on minimizing bias. Bias refers to the error that is introduced by approximating a real-life problem, which may ne extremely complicated, by a much simpler model. The formula is quite easy to understand, is the difference between the ground truth value and the predicted value. Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. Ideally the estimate for \\(f\\) should not vary too much between training sets. The formula is basically a slightly complex version of \\(\\mathbb{E}[(X - \\mathbb{E}(X))^2]\\) . For simple model, it has low variance but large bias. Example: \\(Y = f(X) + \\varepsilon\\) , we simply let \\(\\hat{f}(X) = 0\\) , then variance is 0, but bias is very large \\(f(X)^2\\) . For very flexible model, it has high variance but low bias. Example: \\(Y = f(X) + \\varepsilon = 0 + \\varepsilon\\) , where the ground truth function is \\(0\\) , we let \\(\\hat{f}(x_i) = y_i\\) , which means it fits into the noise. Then \\(\\text{Var}(\\hat{f}(x_i)) = \\text{Var}(y_i) = \\text{Var}(\\varepsilon_i) = \\sigma_i^2\\) , bias is \\(f(x_i) - \\mathbb{E}(\\hat{f}(x_i)) = f(x_i) - \\mathbb{E}(y_i) = 0\\) .","title":"Bias &amp; Variance tradeoff"},{"location":"cs/ml/statml/#regression","text":"","title":"Regression"},{"location":"cs/ml/statml/#simple-linear-regression","text":"Let \\((x_1,y_1)\\) , \\((x_2, y_2)\\) , \\(\\cdots\\) , \\((x_n,y_n)\\) represent \\(n\\) observation pairs, and we use simple linear regression \\(\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i\\) to fit the data. The error is defined as \\(e_i = y_i - \\hat{y_i}\\) , then the residual sum of squares (RSS) is $$ e_1^2 + e_2^2 + \\cdots + e_n^2 = \\sum_{i=1}^{n}{(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2} $$ To minimise the error, we simply let \\(\\frac{\\partial (\\text{RSS})}{\\partial \\hat{\\beta_0}} = \\frac{\\partial (\\text{RSS})}{\\partial \\hat{\\beta_1}} = 0\\) , then with some algebraic operations we obtain: $$ \\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}{(x_i - \\overline{x})(y_i - \\overline{y})}}{\\sum_{i=1}^{n}{(x_i - \\overline{x})^2}} \\text{, } \\hat{\\beta_0} = \\overline{y} - \\hat{\\beta_1} \\overline{x} $$ Actually, when we are drawing the contour map of RSS w.r.t. \\(\\beta_0, \\beta_1\\) , we can obtain the following ellipsoid shape: This is actually because of the squared term \\((y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2\\) , term like this such as \\(\\beta_0^2 + \\beta_1^2 = 1\\) can be written in the quadratic form (\u4e8c\u6b21\u578b): \\[\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}^{T} \\begin{bmatrix} 1&0 \\\\ 0&1 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\] The quadratic form like \\(\\beta^{T}A \\beta = C\\) with A being positive and definite (\u6b63\u5b9a) has ellipsoid shape. After we have computed the values of coefficients, we can use our statistical knowledge to assess the accuracy of estimates. We assume the true relationship between \\(X\\) and \\(Y\\) is the following: $$ Y = f(X) + \\varepsilon, f(X) = \\beta_0 + \\beta_1 X $$ Then as the graph shows Red line is the true relationship, which is known as population regression line , other lines are least squares lines computed based on separate random set of observations. Each line is different but on average the least squares lines are quite close to the population regression line. We can use the residual standard error (RSE) to provide an absolute measure of lack of fit of the linear model (number of freedoms equals to the total number of parameters minus the two we want to estimate): $$ \\text{RSE} = \\sqrt{\\frac{1}{\\text{degree of freedom}} \\text{RSS}} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}} $$ R^2 statistics provides an alternative measure of fit. $$ \\text{TSS} = \\sum{(y_i - \\overline{y})^2}, \\ \\text{RSS} = \\sum{(y_i - \\hat{y_i})^2} $$ And \\(R^2\\) is given by \\[ R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\] The value is always between 0 and 1, if it is closed to 0, it means the prediction has similar effect as mean value, so it doesn't fit the data well; when it is closed to 1, it means the RSS is very small, that means it fits the data well.","title":"Simple Linear Regression"},{"location":"cs/ml/statml/#multiple-linear-regression","text":"","title":"Multiple Linear Regression"},{"location":"cs/ml/statml/#classification","text":"This is the special case where outputs are discrete. Logistic regression, linear discrimant analysis and KNN would be investigated. // image here Under such scenario, linear regression does not work ideally, so we need to introduce new models.","title":"Classification"},{"location":"cs/ml/statml/#logistic-regression","text":"We let \\[\\begin{cases} \\mathbb{P}(y_i = 1 | x_i) = \\frac{1}{1 + \\exp(-x_i^T \\beta)} \\textbf{ (sample version)}\\\\ \\mathbb{P}(Y = 1 | X) = \\frac{1}{1 + \\exp(-\\beta^T X)} \\textbf{ (population version)} \\end{cases}\\]","title":"Logistic Regression"},{"location":"cs/pl/","text":"Programming Languages \u00b6 Table of Contents \u00b6 Rust C++ The Proramming Paradigms \u00b6","title":"Programming Languages"},{"location":"cs/pl/#programming-languages","text":"","title":"Programming Languages"},{"location":"cs/pl/#table-of-contents","text":"Rust C++","title":"Table of Contents"},{"location":"cs/pl/#the-proramming-paradigms","text":"","title":"The Proramming Paradigms"},{"location":"cs/pl/c%2B%2B/","text":"Notice This is COMP2012H Revision Note. Please it doesn't record all the knowledge taught, instead it only covers the part that I am not so clear about. Basics \u00b6 type \u00b6 both char and bool occupie one byte, for ascii it only has 128 characters, raw strings are of const char * type, all floating literals is treated as double by default unless there is a suffix 'f'. A narrowing conversion will result in a warning, can use static-cast to make it disappear. const vs #define \u00b6 const can be type-checked during compilation, and memory is not allocated for constant definition with only a few exceptions. Simply const int x = 10 still stores x in memory, but not in O3 optimization. If we print &x at the end, then both unoptimised and O3 optimised program stores x in memory. for #define , it simply substitutes value inside the program. Variable naming \u00b6 Allowed Characters: 0-9, a-z, A-Z, _ The first character cannot be a digit cannot use reserved word identifiers start with '_' are always used for system variables Reference \u00b6 int a ; double & m = a ; This results in error: non-const lvalue reference to type 'double' cannot bind to a value of unrelated type 'int'. The casting from int to double result in creating a temporary variable, and we cannot create non-const lvalue reference to a remporary variable. ++x is lvalue, x++ is rvalue Operators \u00b6 For the modulo operator %: 5 % 2 = 1 5 % -2 = 1 -5 % 2 = -1 -5 % -2 = -1 Control Flow \u00b6 Remember to add break / return for switch , default is for not catching anything, not any cases can have the same value, the expression must be evaluated to an integral value ie (integer, char, bool) Function \u00b6 Return type is not part of the signature, if included, hard to infer which type to return. https://chortle.ccsu.edu/java5/Notes/chap34A/ch34A_14.html Overloading int f ( int a , double b ) { return 1989 ;} int f ( double a , int b ) { return 2022 ;} int main () { cout << f ( 89.64 , 19.89 ); return 0 ; } will pop out an error: call of overloaded 'f(double, double)' is ambiguous Array \u00b6 The array elements cannot be reference variables, because references are not objects and doesn't occupy the memory so doesn't have the address. Reference to an array, int *b = a would make it lose information about the underlying array (eg: iterators), should better use int (&b)[4] = a , for functions: int (&func())[5] // int *b = a; int ( & b )[ 4 ] = a ; for ( int v : a ) cout << v << endl ; cout << \"========= \\n \" ; for ( int v : b ) cout << v << endl ; int a [ 10 ]; int main () { cout << typeid ( a ). name () << \" \" << typeid ( & a ). name () << \" \" << typeid ( & a [ 0 ]). name () << endl ; cout << a << \" \" << & a << \" \" << & a [ 0 ] << endl ; } // A10_i PA10_i Pi // 0x558fe4de8160 0x558fe4de8160 0x558fe4de8160 char [] vs char * #include <iostream> using namespace std ; int main () { char s [ 6 ] = \"hello\" ; cout << reinterpret_cast < void *> ( s ) << \" \" << & s << endl ; const char * ss = \"hello\" ; cout << reinterpret_cast < const void *> ( ss ) << \" \" << & ss << endl ; } // 0x7ffcad02ac0a 0x7ffcad02ac0a // 0x402006 0x7ffcad02ac00 ss is a pointer the address of a pointer is not equal to the address it's holding. s is an array, and &arr == arr, but not the same for pointers Recursion vs Non-recursive Approach \u00b6 Recursion needs more memory and more computational name - has to memorize its current state, and passes control from the caller to the callee. - sets up a new data structure (you may think of it as a scratch paper for rough work) called activation record which contains information such as * where the caller stops * what actual parameters are passed to the callee * create new local variables required by the callee function * the return value of the function at the end - removes the activation record of the callee when it finishes. - passes control back to the caller. Struct \u00b6 struct-struct assignment by default is memberwise copy (bit by bit), even array can be copied by deep copy, for pointer it is shallow copy only. #include <iostream> using namespace std ; struct example { int hi ; int arr [ 4 ]; int * ptr ; }; int main () { example bruh = { 5 , { 1 , 2 , 3 , 4 }, new int ( 5 )}; example copy_bruh = bruh ; cout << bruh . arr << endl ; cout << copy_bruh . arr << endl ; cout << endl ; cout << bruh . ptr << endl ; cout << copy_bruh . ptr << endl ; return 0 ; } /* 0x7fffc8c90c44 0x7fffc8c90c64 0x56498b106eb0 0x56498b106eb0 */ g++ compilation command & makefile \u00b6 myclass.h , libmyclass.a (library file of the object code of implementation of all class member functions, this enforces information hiding) Build library command: g++ -c xxx.cpp first, then ar rsuv libxxx.a xxx.o xxx.o xxx.o . Compilation Command: g++ -o main main.cpp -L -lmyclass OOP \u00b6 C++ Class \u00b6 Be careful for deleting char* , if it is a raw string, it should be delete [] . delete [] will call the class destructor on each array element in reverse order. #include <iostream> using namespace std ; int times = 0 ; struct A { int v ; A () { v = times ++ ;} ~ A () { cout << v << endl ;} }; int main () { A * arr = new A [ 5 ]; delete [] arr ; } Forward definition (same applies to function) class Cell ; class List { int size ; Cell * data ; Cell x ; // need Cell } class Cell { int info ; Cell * next ; } Non-static data members that are not having like float real = 1.3; , float imag {0.5} will have the values of their default member initializer, and undefined if there's no. class A { private : int v ; public : A () { cout << v << endl ;} }; int main () { A * arr = new A [ 5 ]; delete [] arr ; } inline is used for inside-header implementation, if implementation is inside class, the keyword is optional, otherwise it is compulsory. Another usage of inline is that by declaring a very short function inline , we serve a hint to the compiler to make it inline as function calling is expensive, but the compiler still has the freedom to choose it or not. protected : accessible to - member functions and friends of the class, as well as - member functions and friends of its derived classes (subclasses) We can use Word movie = {1, \"Titanic\"} only if all data members are public Default behavior: copy constructor: perform memberwise assignment by calling the assignment operator A a(); does not actually mean initialization MIL: The order of the members in the list doesn\u2019t matter; the actual execution order is their order in the class declaration. Advantage: avoid using assignment operator if it is not appropriately defined (error-prone) reinterpret_cast<void *>(\") , output the address even though it is char * Construction and Destruction \u00b6 has-a-relationship vs own-a-relationship Has-a-relationship: class A {}; class B { A a ; B () = default ; // B(): a() {} }; Own-a-relationship class A {}; class B { A * a ; B () = default ; }; The Clock object is constructed in the Postoffice constructor, but it is never destructed, since we have not implemented that, we need to manually delete it in the destructor. Inheritance & Polymorphism \u00b6 Remember to include the father into MIL: struct A { int val ; A ( int val ) : val ( val + 10 ) {} virtual void print_label () { cout << \"A \" << val << \" \\n \" ; } }; struct B : A { int val ; B ( int val ) : A ( val ), val ( val ) {} void play () { cout << \"play \\n \" ; } virtual void print_label () override { cout << \"B \" << val << \" \\n \" ; } }; Construction Order: 1. its parent 2. its data members (in order of their appearance) 3. itself struct A { virtual void print () const { cout << \"A \\n \" ; } }; struct B : A { virtual void print () const override { cout << \"B \\n \" ; } }; void plv ( A a ) { a . print (); } void plr ( const A & a ) { a . print (); } void plp ( const A * a ) { a -> print (); } int main () { B b ; plv ( b ); plr ( b ); plp ( & b ); } Using virtual function only helps with pointer + reference to avoid slicing, can't help with PBV. (this uses the dynamic binding technique, determine the called function at run time). A virtual function is declared using the keyword virtual in the class definition, and not in the member function implementation, if it is defined outside the class. Once a member function is declared virtual in the base class, it is automatically virtual in all directly or indirectly derived classes. Even though it is not necessary to use the virtual keyword in the derived classes, it is a good style to do so because it improves the readability of header files. static_cast vs dynamic_cast vs reinterpret_cast There's no any checking of static_cast and it does not consult RTTI so it runs faster than dynamic_cast. Dynamic_cast only works on pointer and reference of polymorphic class (with virtual functions), it checks and returns a null pointer if conversion of pointer fails (a valid complete object of the target type), for reference it will trigger runtime error. It's fine to use static_cast for upper casting ie inherited=>base, but for base=>inherited better use dynamic_cast. Do not rely on the virtual function mechanism during the execution of a constructor. class Base { public : Base () { cout << \"Base's constructor \\n \" ; f (); } virtual void f () { cout << \"Base::f()\" << endl ; } }; class Derived : public Base { public : Derived () { cout << \"Derived's constructor \\n \" ; } virtual void f () override { cout << \"Derived::f()\" << endl ; } }; int main () { Base * p = new Derived ; cout << \"Derived-class object created\" << endl ; p -> f (); } This is not a bug, but necessary \u2014 how can the derived object provide services if it has not been constructed yet? Similarly, if a virtual function is called inside the base class destructor, it represents base class\u2019 virtual function: when a derived class is being deleted, the derived-specific portion has already been deleted before the base class destructor is called! pointers and references to an ABC can be declared, but use it as PBV is prohibited. public / protected / private inheritance 1. Public inheritance preserves the original accessibility of inherited members: public \u21d2 public protected \u21d2 protected private \u21d2 private 2. Protected inheritance affects only public members and renders them protected. public \u21d2 protected protected \u21d2 protected private \u21d2 private 3. Private inheritance renders all inherited members private. public \u21d2 private protected \u21d2 private private \u21d2 private Private and protected inheritance do not allow casting of objects of derived classes back to the base class. /* General case */ template < typename T > const T & larger ( const T & a , const T & b ) { cout << \"general case: \" ; return ( a < b ) ? b : a ; } /* Exceptional case */ template <> const char * const & larger ( const char * const & a , const char * const & b ) { cout << \"special case: \" ; return ( strcmp ( a , b ) < 0 ) ? b : a ; } const pointers match const T& , if delete the second const it would result in type mismatch // Here, x is a reference to an array of N objects of type T template < typename T , int N > int f ( T ( & x ) [ N ]) { return N ; } Return size of an array Generic Programming \u00b6 Operator Overloading \u00b6 = copy assignment operator vs copy constructor copy assignment need to delete old data (rmb to avoid self-assignment) actually we can use copy and swap +=, = returns const reference avoid situation like a = b = c, this implicitly means (a.equal(b)).equal(c), which is not intended (it makes a = b, then a = c) +: member function vs global function Vector Vector :: operator + ( const Vector & a ); Vector operator + ( const Vector & a , const Vector & b ); Data Structure \u00b6 rvalue Reference and Move Semantics \u00b6 Revisit that a variable has dual rules: lvalue (read-write) or rvalue (read-only). More kinds of values: xvalue , glvalue , prvalue , this would be introduced later. rvalue Reference Definition T && < variable > = < temporary object >","title":"C++"},{"location":"cs/pl/c%2B%2B/#basics","text":"","title":"Basics"},{"location":"cs/pl/c%2B%2B/#type","text":"both char and bool occupie one byte, for ascii it only has 128 characters, raw strings are of const char * type, all floating literals is treated as double by default unless there is a suffix 'f'. A narrowing conversion will result in a warning, can use static-cast to make it disappear.","title":"type"},{"location":"cs/pl/c%2B%2B/#const-vs-define","text":"const can be type-checked during compilation, and memory is not allocated for constant definition with only a few exceptions. Simply const int x = 10 still stores x in memory, but not in O3 optimization. If we print &x at the end, then both unoptimised and O3 optimised program stores x in memory. for #define , it simply substitutes value inside the program.","title":"const vs #define"},{"location":"cs/pl/c%2B%2B/#variable-naming","text":"Allowed Characters: 0-9, a-z, A-Z, _ The first character cannot be a digit cannot use reserved word identifiers start with '_' are always used for system variables","title":"Variable naming"},{"location":"cs/pl/c%2B%2B/#reference","text":"int a ; double & m = a ; This results in error: non-const lvalue reference to type 'double' cannot bind to a value of unrelated type 'int'. The casting from int to double result in creating a temporary variable, and we cannot create non-const lvalue reference to a remporary variable. ++x is lvalue, x++ is rvalue","title":"Reference"},{"location":"cs/pl/c%2B%2B/#operators","text":"For the modulo operator %: 5 % 2 = 1 5 % -2 = 1 -5 % 2 = -1 -5 % -2 = -1","title":"Operators"},{"location":"cs/pl/c%2B%2B/#control-flow","text":"Remember to add break / return for switch , default is for not catching anything, not any cases can have the same value, the expression must be evaluated to an integral value ie (integer, char, bool)","title":"Control Flow"},{"location":"cs/pl/c%2B%2B/#function","text":"Return type is not part of the signature, if included, hard to infer which type to return. https://chortle.ccsu.edu/java5/Notes/chap34A/ch34A_14.html Overloading int f ( int a , double b ) { return 1989 ;} int f ( double a , int b ) { return 2022 ;} int main () { cout << f ( 89.64 , 19.89 ); return 0 ; } will pop out an error: call of overloaded 'f(double, double)' is ambiguous","title":"Function"},{"location":"cs/pl/c%2B%2B/#array","text":"The array elements cannot be reference variables, because references are not objects and doesn't occupy the memory so doesn't have the address. Reference to an array, int *b = a would make it lose information about the underlying array (eg: iterators), should better use int (&b)[4] = a , for functions: int (&func())[5] // int *b = a; int ( & b )[ 4 ] = a ; for ( int v : a ) cout << v << endl ; cout << \"========= \\n \" ; for ( int v : b ) cout << v << endl ; int a [ 10 ]; int main () { cout << typeid ( a ). name () << \" \" << typeid ( & a ). name () << \" \" << typeid ( & a [ 0 ]). name () << endl ; cout << a << \" \" << & a << \" \" << & a [ 0 ] << endl ; } // A10_i PA10_i Pi // 0x558fe4de8160 0x558fe4de8160 0x558fe4de8160 char [] vs char * #include <iostream> using namespace std ; int main () { char s [ 6 ] = \"hello\" ; cout << reinterpret_cast < void *> ( s ) << \" \" << & s << endl ; const char * ss = \"hello\" ; cout << reinterpret_cast < const void *> ( ss ) << \" \" << & ss << endl ; } // 0x7ffcad02ac0a 0x7ffcad02ac0a // 0x402006 0x7ffcad02ac00 ss is a pointer the address of a pointer is not equal to the address it's holding. s is an array, and &arr == arr, but not the same for pointers","title":"Array"},{"location":"cs/pl/c%2B%2B/#recursion-vs-non-recursive-approach","text":"Recursion needs more memory and more computational name - has to memorize its current state, and passes control from the caller to the callee. - sets up a new data structure (you may think of it as a scratch paper for rough work) called activation record which contains information such as * where the caller stops * what actual parameters are passed to the callee * create new local variables required by the callee function * the return value of the function at the end - removes the activation record of the callee when it finishes. - passes control back to the caller.","title":"Recursion vs Non-recursive Approach"},{"location":"cs/pl/c%2B%2B/#struct","text":"struct-struct assignment by default is memberwise copy (bit by bit), even array can be copied by deep copy, for pointer it is shallow copy only. #include <iostream> using namespace std ; struct example { int hi ; int arr [ 4 ]; int * ptr ; }; int main () { example bruh = { 5 , { 1 , 2 , 3 , 4 }, new int ( 5 )}; example copy_bruh = bruh ; cout << bruh . arr << endl ; cout << copy_bruh . arr << endl ; cout << endl ; cout << bruh . ptr << endl ; cout << copy_bruh . ptr << endl ; return 0 ; } /* 0x7fffc8c90c44 0x7fffc8c90c64 0x56498b106eb0 0x56498b106eb0 */","title":"Struct"},{"location":"cs/pl/c%2B%2B/#g-compilation-command-makefile","text":"myclass.h , libmyclass.a (library file of the object code of implementation of all class member functions, this enforces information hiding) Build library command: g++ -c xxx.cpp first, then ar rsuv libxxx.a xxx.o xxx.o xxx.o . Compilation Command: g++ -o main main.cpp -L -lmyclass","title":"g++ compilation command &amp; makefile"},{"location":"cs/pl/c%2B%2B/#oop","text":"","title":"OOP"},{"location":"cs/pl/c%2B%2B/#c-class","text":"Be careful for deleting char* , if it is a raw string, it should be delete [] . delete [] will call the class destructor on each array element in reverse order. #include <iostream> using namespace std ; int times = 0 ; struct A { int v ; A () { v = times ++ ;} ~ A () { cout << v << endl ;} }; int main () { A * arr = new A [ 5 ]; delete [] arr ; } Forward definition (same applies to function) class Cell ; class List { int size ; Cell * data ; Cell x ; // need Cell } class Cell { int info ; Cell * next ; } Non-static data members that are not having like float real = 1.3; , float imag {0.5} will have the values of their default member initializer, and undefined if there's no. class A { private : int v ; public : A () { cout << v << endl ;} }; int main () { A * arr = new A [ 5 ]; delete [] arr ; } inline is used for inside-header implementation, if implementation is inside class, the keyword is optional, otherwise it is compulsory. Another usage of inline is that by declaring a very short function inline , we serve a hint to the compiler to make it inline as function calling is expensive, but the compiler still has the freedom to choose it or not. protected : accessible to - member functions and friends of the class, as well as - member functions and friends of its derived classes (subclasses) We can use Word movie = {1, \"Titanic\"} only if all data members are public Default behavior: copy constructor: perform memberwise assignment by calling the assignment operator A a(); does not actually mean initialization MIL: The order of the members in the list doesn\u2019t matter; the actual execution order is their order in the class declaration. Advantage: avoid using assignment operator if it is not appropriately defined (error-prone) reinterpret_cast<void *>(\") , output the address even though it is char *","title":"C++ Class"},{"location":"cs/pl/c%2B%2B/#construction-and-destruction","text":"has-a-relationship vs own-a-relationship Has-a-relationship: class A {}; class B { A a ; B () = default ; // B(): a() {} }; Own-a-relationship class A {}; class B { A * a ; B () = default ; }; The Clock object is constructed in the Postoffice constructor, but it is never destructed, since we have not implemented that, we need to manually delete it in the destructor.","title":"Construction and Destruction"},{"location":"cs/pl/c%2B%2B/#inheritance-polymorphism","text":"Remember to include the father into MIL: struct A { int val ; A ( int val ) : val ( val + 10 ) {} virtual void print_label () { cout << \"A \" << val << \" \\n \" ; } }; struct B : A { int val ; B ( int val ) : A ( val ), val ( val ) {} void play () { cout << \"play \\n \" ; } virtual void print_label () override { cout << \"B \" << val << \" \\n \" ; } }; Construction Order: 1. its parent 2. its data members (in order of their appearance) 3. itself struct A { virtual void print () const { cout << \"A \\n \" ; } }; struct B : A { virtual void print () const override { cout << \"B \\n \" ; } }; void plv ( A a ) { a . print (); } void plr ( const A & a ) { a . print (); } void plp ( const A * a ) { a -> print (); } int main () { B b ; plv ( b ); plr ( b ); plp ( & b ); } Using virtual function only helps with pointer + reference to avoid slicing, can't help with PBV. (this uses the dynamic binding technique, determine the called function at run time). A virtual function is declared using the keyword virtual in the class definition, and not in the member function implementation, if it is defined outside the class. Once a member function is declared virtual in the base class, it is automatically virtual in all directly or indirectly derived classes. Even though it is not necessary to use the virtual keyword in the derived classes, it is a good style to do so because it improves the readability of header files. static_cast vs dynamic_cast vs reinterpret_cast There's no any checking of static_cast and it does not consult RTTI so it runs faster than dynamic_cast. Dynamic_cast only works on pointer and reference of polymorphic class (with virtual functions), it checks and returns a null pointer if conversion of pointer fails (a valid complete object of the target type), for reference it will trigger runtime error. It's fine to use static_cast for upper casting ie inherited=>base, but for base=>inherited better use dynamic_cast. Do not rely on the virtual function mechanism during the execution of a constructor. class Base { public : Base () { cout << \"Base's constructor \\n \" ; f (); } virtual void f () { cout << \"Base::f()\" << endl ; } }; class Derived : public Base { public : Derived () { cout << \"Derived's constructor \\n \" ; } virtual void f () override { cout << \"Derived::f()\" << endl ; } }; int main () { Base * p = new Derived ; cout << \"Derived-class object created\" << endl ; p -> f (); } This is not a bug, but necessary \u2014 how can the derived object provide services if it has not been constructed yet? Similarly, if a virtual function is called inside the base class destructor, it represents base class\u2019 virtual function: when a derived class is being deleted, the derived-specific portion has already been deleted before the base class destructor is called! pointers and references to an ABC can be declared, but use it as PBV is prohibited. public / protected / private inheritance 1. Public inheritance preserves the original accessibility of inherited members: public \u21d2 public protected \u21d2 protected private \u21d2 private 2. Protected inheritance affects only public members and renders them protected. public \u21d2 protected protected \u21d2 protected private \u21d2 private 3. Private inheritance renders all inherited members private. public \u21d2 private protected \u21d2 private private \u21d2 private Private and protected inheritance do not allow casting of objects of derived classes back to the base class. /* General case */ template < typename T > const T & larger ( const T & a , const T & b ) { cout << \"general case: \" ; return ( a < b ) ? b : a ; } /* Exceptional case */ template <> const char * const & larger ( const char * const & a , const char * const & b ) { cout << \"special case: \" ; return ( strcmp ( a , b ) < 0 ) ? b : a ; } const pointers match const T& , if delete the second const it would result in type mismatch // Here, x is a reference to an array of N objects of type T template < typename T , int N > int f ( T ( & x ) [ N ]) { return N ; } Return size of an array","title":"Inheritance &amp; Polymorphism"},{"location":"cs/pl/c%2B%2B/#generic-programming","text":"","title":"Generic Programming"},{"location":"cs/pl/c%2B%2B/#operator-overloading","text":"= copy assignment operator vs copy constructor copy assignment need to delete old data (rmb to avoid self-assignment) actually we can use copy and swap +=, = returns const reference avoid situation like a = b = c, this implicitly means (a.equal(b)).equal(c), which is not intended (it makes a = b, then a = c) +: member function vs global function Vector Vector :: operator + ( const Vector & a ); Vector operator + ( const Vector & a , const Vector & b );","title":"Operator Overloading"},{"location":"cs/pl/c%2B%2B/#data-structure","text":"","title":"Data Structure"},{"location":"cs/pl/c%2B%2B/#rvalue-reference-and-move-semantics","text":"Revisit that a variable has dual rules: lvalue (read-write) or rvalue (read-only). More kinds of values: xvalue , glvalue , prvalue , this would be introduced later. rvalue Reference Definition T && < variable > = < temporary object >","title":"rvalue Reference and Move Semantics"},{"location":"cs/pl/rust/","text":"Rust \u00b6 Abstract Rust is a statically typed language so it must know the types of all variables at compile time. The language does not have a GC (Garbage Collector) but achieve the purposes by its unique functionalities called ownership and lifetime . Many of the examples and sentences here are directly adopted from The Rust Programming Language . Basics \u00b6 Compile and Run \u00b6 When there's only one file, we can use rustc main.rs to compile (just like javac in Java ) and then use ./main.rs to run it. If things get complicated, we could make use of the powerful package manager of Rust, Cargo. It is easy to use, only several commands can satisfy most of the daily usage. Useful Commands \u00b6 Create a new project $ cargo new hello_cargo Build (compile) the project $ cargo build Compile and Run the project $ cargo run Hello World Program \u00b6 fn main () { println! ( \"Hello, world!\" ); } As you can see, the syntax here is quite similar to C/C++, there are some differences though. fn here refers to function, and also ! is added behind println as println in Rust is a macro. (The meaning of macro would be introduced later). Compilation / Runtime error of Rust \u00b6 Rust can give very powerful static checking during compilation and it can help you figure out most of the bugs before actually running the code with useful error message. Runtime error in Rust is called panic . Variables and Mutability \u00b6 we use let to define new variables, and we call this kind of value assignment as variable binding . Like in C++ there are variables and constants, in Scala there are var and val , we have mutable and immutable variables in Rust as well , but the variables are by default immutable as immutability has a lot of advantages (check Wikipedia page for detailed explanations). To make the variable mutable, we should add the keyword mut at the beginning. let x = 5 ; let mut y = 6 ; const THREE_HOURS_IN_SECONDS : u32 = 60 * 60 * 3 ; Notice However, note that constants and immutable variables are different. The naming convention for constants is using all upper cases. Constants are valid for the entire time when a program runs, within the scope they were declared in. Also, constants can only be set to a constant expression, not the result of a value that could only be computed during runtime. Variable scope is a concept that is frequently used, it refers to the range within a program for which an item is valid. For example: { let s = \"hello\" ; } Inside the curly bracket there defines a scope, and s is valid inside, after going out of the scope, it becomes invalid, just like the local variable. We can declare a new variable with the same name as previous variable and we call this kind of operation as shadowing . let y = 5 ; let y = y + 1 ; { let y = y * 2 ; println! ( \"The value of y in the inner scope is: {y}\" ); } Like this one, we first initialize the immutable variable y by binding 5 to it. Then we shadow y by y + 1 . Inside the curly brackets, we shadow y again, when the scope is over, the inner shadowing ends. So the output value should be 6. The following code can run smoothly: let spaces = \" \" ; spaces = spaces . len (); However, if we add mut to spaces . It would pop out errors because the compiler perceives it as mutating the type of the variable instead of shadowing, which is not allowed. Data Type I \u00b6 Info Here I will only introduce some basic data types, more complicated ones like String, Vector, Map would be introduced later as Data Type II . As shown previously, we don't always need to write out the type explicitly ( let y = 5; ) unless the compiler requires more about the type information of the variable. If doing so, it would be like: let y: i32 = 5; , it is called type annotation . There are two data type subsets, namely Scalar Types and Compound Types . Scalar Type \u00b6 Integer Type \u00b6 u32 is one of the integer types, where u refers to unsigned, 32 refers to the number of bits. Similarly, there are i32 , u16 , u32 , i128 , ... By basic CS knowledge, it is trivial to calculate the range of each type. The isize and usize types depend on the architecture of the computer your program is running on, which is denoted in the table as \u201carch\u201d: 64 bits if you\u2019re on a 64-bit architecture and 32 bits if you\u2019re on a 32-bit architecture. We can call the functions usize::MIN , usize::MAX , isize::MIN , isize::MAX . The number literals can be represented under different basis. For example: 0xff , 0o77 , 0b1100_0100 , b'A' , 345_678 etc. One of the unique features of Rust is that it can insert _ inside the numbers to improve the readability like the binary number given above. We still need to handle the integer overflow issue in Rust. Wrap in all modes with the wrapping_* methods, such as wrapping_add Return the None value if there is overflow with the checked_* methods Return the value and a boolean indicating whether there was overflow with the overflowing_* methods Saturate at the value\u2019s minimum or maximum values with saturating_* methods let of_x : u8 = 233 ; let of_y : u8 = 133 ; of_x . wrapping_add ( of_y ); // 110 of_x . checked_add ( of_y ); // None of_x . overflowing_add ( of_y ). 1 ; // true of_x . saturating_add ( of_y )); // 255 Floating-point Type \u00b6 Only f32 and f64 . The boolean type and char type are similiar to C/C++. Compound Type \u00b6 Tuple Type \u00b6 Tuple has fixed length but allows its elements to have different types. We can use pattern matching to destructure a tuple value as following: fn main () { let tup = ( 500 , 6.4 , 1 ); let ( x , y , z ) = tup ; println! ( \"The value of y is: {y}\" ); } We could also access the components of them using the following syntax: tup . 0 ; tup . 1 ; tup . 2 ; Array Type \u00b6 The array still has fixed length but all elements inside must have the same type. It is not as flexible as the vector type as it has varible length. However, the arrays are useful if you want your data to be allocated on the stack rather than the heap. let a : [ i32 ; 5 ] = [ 1 , 2 , 3 , 4 , 5 ]; let b = [ 3 ; 5 ]; // [3, 3, 3, 3, 3] We could access the elements by using the following syntax: a [ 0 ]; a [ 1 ]; a [ 2 ]; Rust can pop out runtime errors when we have invalid array element access. use std :: io ; fn main () { let a = [ 1 , 2 , 3 , 4 , 5 ]; println! ( \"Please enter an array index.\" ); let mut index = String :: new (); io :: stdin () . read_line ( & mut index ) . expect ( \"Failed to read line\" ); let index : usize = index . trim () . parse () . expect ( \"Index entered was not a number\" ); let element = a [ index ]; println! ( \"The value of the element at index {index} is: {element}\" ); } We neglect the very detail of this program at this moment, we only care about if we input 6 as index, the following error would pop out while C / C++ cannot: thread 'main' panicked at 'index out of bounds: the len is 5 but the index is 10', src/main.rs:19:19 note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace This actually shows Rust's memory safety principles in action. It is very hard to debug if having invalid array elements access in C/C++ because it would trigger unexpected modification of data in other side of the program. Function \u00b6 A typical example: fn plus_two ( x : i32 , y : i32 ) -> i32 { x + y } We use : i32 to give type annotation on the incoming arguments, and -> refers to the return type. () (unit type) stands for nothing is returned. You may notice no ; is put behind x + y , this shows the difference between statement and expression . Statements are instructions that perform a certain action, while expressions evaluate to a resulting value. Like the one above is an expression, where ; should noe be added. Another example: let x = ( let y = 6 ); This one is incorrect, if you want to bind the value of y to x, the correct code should be: let x = { let y = 6 ; y }; Comments \u00b6 Comments in Rust are just similiar to C/C++. // Hello, // we are comments! Documentation Comments would be written later here. Control Flows \u00b6 if Expressions \u00b6 One Example (no parenthesis is needed): if number < 5 { println! ( \"condition was true\" ); } else { println! ( \"condition was false\" ); } Note that the following code cannot compiled unlike C/C++: fn main () { let number = 3 ; if number { println! ( \"number was three\" ); } } The integer cannot be directly casted into bool. So should write if number != 0 . Nested if is the same as C/C++. There's one useful syntactic sugar: let number = if condition { 5 } else { 6 }; Reminders The types of results of two expressions should be the same since number can only have one type. loops \u00b6 loop \u00b6 This is the endless loop which can only be stopped by the break command: loop { bruhbruhbruh ; if flag { break ; } } If there are nested loops, we could label the loops and indicate their names during break. fn main () { let mut count = 0 ; ' counting_up : loop { println! ( \"count = {count}\" ); let mut remaining = 10 ; loop { println! ( \"remaining = {remaining}\" ); if remaining == 9 { break ; } if count == 2 { break 'counting_up ; } remaining -= 1 ; } count += 1 ; } println! ( \"End count = {count}\" ); } Besides, we could use while and for loop as well: While loop: while number != 0 { println! ( \"{number}!\" ); number -= 1 ; } for loop: let a = [ 10 , 20 , 30 , 40 , 50 ]; for element in a { println! ( \"the value is: {element}\" ); } for number in ( 1 .. 4 ) { println! ( \"{number}!\" ); } Here (1..4) refers to [1, 4) just like Python, to refer to [1, 4] , we should use (1..=4) instead. * Ownership \u00b6 Info Ownership is Rust\u2019s most unique feature and has deep implications for the rest of the language. It enables Rust to make memory safety guarantees without needing a garbage collector, so it\u2019s important to understand how ownership works. It governs how a Rust program manages memory. Stack and Heap are two important data structures in memory management. Stack is FILO, all data stored on the stack must have a known, fixed size. Heap is less organized and stores data with unknown size at compile time or a size that might change. Efficency of putting new data \u00b6 The memory allocator of heap would find an empty spot that is big enough if new data to be inserted, this is called allocating on the heap . Therefore, pushing to the stack is faster than allocating on the heap as the allocator does not need to search for a place to store new data, the new location is always at the top of stack. Efficency of accessing data \u00b6 It is still faster on the stack as it does not need to follow a pointer to get there. The main purpose of ownership is keeping track of what parts of code are using what data on the heap, minimizing the amount of duplicate data on the heap, and cleaning up unused data on the heap so you don\u2019t run out of space are all problems that ownership addresses. There are three ownership rules: Each value in Rust has an owner. There can only be one owner at a time. When the owner goes out of scope, the value will be dropped. Data Type II \u00b6 String Type \u00b6 The String type is far more complicated than it seems. let s1 = String :: from ( \"hello\" ); let s2 = \"world\" . to_string (); We can convert raw string to the String type either by calling String::from or .to_string() . The internal structure looks like this: The left part is stored on stack, consisting of: the length and capacity (we can ignore capacity at this moment) of the string, and the pointer ptr pointing to string content on the heap. We store it on the heap because the length of string might be changed later (e.g.: extension). We use different methods to append new raw string / char. let mut s1 = String :: from ( \"foo\" ); let s2 = \"bar\" ; s1 . push_str ( s2 ); println! ( \"s2 is {}\" , s2 ); let mut s = String :: from ( \"lo\" ); s . push ( 'l' ); We will cover the shallow and deep copying issue. Vector \u00b6 Map \u00b6","title":"Rust"},{"location":"cs/pl/rust/#rust","text":"Abstract Rust is a statically typed language so it must know the types of all variables at compile time. The language does not have a GC (Garbage Collector) but achieve the purposes by its unique functionalities called ownership and lifetime . Many of the examples and sentences here are directly adopted from The Rust Programming Language .","title":"Rust"},{"location":"cs/pl/rust/#basics","text":"","title":"Basics"},{"location":"cs/pl/rust/#compile-and-run","text":"When there's only one file, we can use rustc main.rs to compile (just like javac in Java ) and then use ./main.rs to run it. If things get complicated, we could make use of the powerful package manager of Rust, Cargo. It is easy to use, only several commands can satisfy most of the daily usage.","title":"Compile and Run"},{"location":"cs/pl/rust/#hello-world-program","text":"fn main () { println! ( \"Hello, world!\" ); } As you can see, the syntax here is quite similar to C/C++, there are some differences though. fn here refers to function, and also ! is added behind println as println in Rust is a macro. (The meaning of macro would be introduced later).","title":"Hello World Program"},{"location":"cs/pl/rust/#compilation-runtime-error-of-rust","text":"Rust can give very powerful static checking during compilation and it can help you figure out most of the bugs before actually running the code with useful error message. Runtime error in Rust is called panic .","title":"Compilation / Runtime error of Rust"},{"location":"cs/pl/rust/#variables-and-mutability","text":"we use let to define new variables, and we call this kind of value assignment as variable binding . Like in C++ there are variables and constants, in Scala there are var and val , we have mutable and immutable variables in Rust as well , but the variables are by default immutable as immutability has a lot of advantages (check Wikipedia page for detailed explanations). To make the variable mutable, we should add the keyword mut at the beginning. let x = 5 ; let mut y = 6 ; const THREE_HOURS_IN_SECONDS : u32 = 60 * 60 * 3 ; Notice However, note that constants and immutable variables are different. The naming convention for constants is using all upper cases. Constants are valid for the entire time when a program runs, within the scope they were declared in. Also, constants can only be set to a constant expression, not the result of a value that could only be computed during runtime. Variable scope is a concept that is frequently used, it refers to the range within a program for which an item is valid. For example: { let s = \"hello\" ; } Inside the curly bracket there defines a scope, and s is valid inside, after going out of the scope, it becomes invalid, just like the local variable. We can declare a new variable with the same name as previous variable and we call this kind of operation as shadowing . let y = 5 ; let y = y + 1 ; { let y = y * 2 ; println! ( \"The value of y in the inner scope is: {y}\" ); } Like this one, we first initialize the immutable variable y by binding 5 to it. Then we shadow y by y + 1 . Inside the curly brackets, we shadow y again, when the scope is over, the inner shadowing ends. So the output value should be 6. The following code can run smoothly: let spaces = \" \" ; spaces = spaces . len (); However, if we add mut to spaces . It would pop out errors because the compiler perceives it as mutating the type of the variable instead of shadowing, which is not allowed.","title":"Variables and Mutability"},{"location":"cs/pl/rust/#data-type-i","text":"Info Here I will only introduce some basic data types, more complicated ones like String, Vector, Map would be introduced later as Data Type II . As shown previously, we don't always need to write out the type explicitly ( let y = 5; ) unless the compiler requires more about the type information of the variable. If doing so, it would be like: let y: i32 = 5; , it is called type annotation . There are two data type subsets, namely Scalar Types and Compound Types .","title":"Data Type I"},{"location":"cs/pl/rust/#scalar-type","text":"","title":"Scalar Type"},{"location":"cs/pl/rust/#compound-type","text":"","title":"Compound Type"},{"location":"cs/pl/rust/#function","text":"A typical example: fn plus_two ( x : i32 , y : i32 ) -> i32 { x + y } We use : i32 to give type annotation on the incoming arguments, and -> refers to the return type. () (unit type) stands for nothing is returned. You may notice no ; is put behind x + y , this shows the difference between statement and expression . Statements are instructions that perform a certain action, while expressions evaluate to a resulting value. Like the one above is an expression, where ; should noe be added. Another example: let x = ( let y = 6 ); This one is incorrect, if you want to bind the value of y to x, the correct code should be: let x = { let y = 6 ; y };","title":"Function"},{"location":"cs/pl/rust/#comments","text":"Comments in Rust are just similiar to C/C++. // Hello, // we are comments! Documentation Comments would be written later here.","title":"Comments"},{"location":"cs/pl/rust/#control-flows","text":"","title":"Control Flows"},{"location":"cs/pl/rust/#if-expressions","text":"One Example (no parenthesis is needed): if number < 5 { println! ( \"condition was true\" ); } else { println! ( \"condition was false\" ); } Note that the following code cannot compiled unlike C/C++: fn main () { let number = 3 ; if number { println! ( \"number was three\" ); } } The integer cannot be directly casted into bool. So should write if number != 0 . Nested if is the same as C/C++. There's one useful syntactic sugar: let number = if condition { 5 } else { 6 }; Reminders The types of results of two expressions should be the same since number can only have one type.","title":"if Expressions"},{"location":"cs/pl/rust/#loops","text":"","title":"loops"},{"location":"cs/pl/rust/#ownership","text":"Info Ownership is Rust\u2019s most unique feature and has deep implications for the rest of the language. It enables Rust to make memory safety guarantees without needing a garbage collector, so it\u2019s important to understand how ownership works. It governs how a Rust program manages memory. Stack and Heap are two important data structures in memory management. Stack is FILO, all data stored on the stack must have a known, fixed size. Heap is less organized and stores data with unknown size at compile time or a size that might change.","title":"* Ownership"},{"location":"cs/pl/rust/#data-type-ii","text":"","title":"Data Type II"},{"location":"cs/pl/rust/#string-type","text":"The String type is far more complicated than it seems. let s1 = String :: from ( \"hello\" ); let s2 = \"world\" . to_string (); We can convert raw string to the String type either by calling String::from or .to_string() . The internal structure looks like this: The left part is stored on stack, consisting of: the length and capacity (we can ignore capacity at this moment) of the string, and the pointer ptr pointing to string content on the heap. We store it on the heap because the length of string might be changed later (e.g.: extension). We use different methods to append new raw string / char. let mut s1 = String :: from ( \"foo\" ); let s2 = \"bar\" ; s1 . push_str ( s2 ); println! ( \"s2 is {}\" , s2 ); let mut s = String :: from ( \"lo\" ); s . push ( 'l' ); We will cover the shallow and deep copying issue.","title":"String Type"},{"location":"cs/pl/rust/#vector","text":"","title":"Vector"},{"location":"cs/pl/rust/#map","text":"","title":"Map"}]}