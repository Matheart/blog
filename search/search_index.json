{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Matheart's Note ! \u00b6 site.info Post note about cs & math stuff ~ and also some random things maybe \u53ef\u80fd\u6703\u96a8\u6211\u7684\u7fd2\u6163\u51fa\u73fe\u4e2d\u82f1\u593e\u96dc\uff0c\u751a\u81f3\u7c21\u7e41\u593e\u96dc\u7684\u60c5\u6cc1\uff0c\u53cd\u6b63\u4e5f\u53ea\u662f\u70ba\u4e86\u6211\u81ea\u5df1\u770b\u8457(&\u5beb\u8457)\u8212\u670d about(site.author) HKUST CS & Math 25' HKUST Freshman 2021 Server Owner B\u7ad9\u4e0d\u77e5\u540d\u77e5\u8bc6\u533aUP\u4e3b \u7eaf\u826f3u Table of Contents \u00b6 Computer Science Machine Learning Computer Architecture Programming Languages Rust Algorithms (TO-DO) Markdown Mathematics","title":"Welcome to Matheart's Note !"},{"location":"#welcome-to-mathearts-note","text":"site.info Post note about cs & math stuff ~ and also some random things maybe \u53ef\u80fd\u6703\u96a8\u6211\u7684\u7fd2\u6163\u51fa\u73fe\u4e2d\u82f1\u593e\u96dc\uff0c\u751a\u81f3\u7c21\u7e41\u593e\u96dc\u7684\u60c5\u6cc1\uff0c\u53cd\u6b63\u4e5f\u53ea\u662f\u70ba\u4e86\u6211\u81ea\u5df1\u770b\u8457(&\u5beb\u8457)\u8212\u670d about(site.author) HKUST CS & Math 25' HKUST Freshman 2021 Server Owner B\u7ad9\u4e0d\u77e5\u540d\u77e5\u8bc6\u533aUP\u4e3b \u7eaf\u826f3u","title":"Welcome to Matheart's Note !"},{"location":"#table-of-contents","text":"Computer Science Machine Learning Computer Architecture Programming Languages Rust Algorithms (TO-DO) Markdown Mathematics","title":"Table of Contents"},{"location":"carol/","text":"\u4e8c\u521b\u72e9\u730e \u00b6","title":"\u4e8c\u521b\u72e9\u730e"},{"location":"carol/#_1","text":"","title":"\u4e8c\u521b\u72e9\u730e"},{"location":"cs/","text":"Computer Science \u00b6 Warning The code is written under Mac environment. Please note that this note is only for recording what I have learnt so far, so they are for reference only and not 100% correct. Feel free to contact me if you got any questions. Table of Contents \u00b6 Machine Learning Computer Architecture Programming Languages Rust Algorithms Markdown","title":"Computer Science"},{"location":"cs/#computer-science","text":"Warning The code is written under Mac environment. Please note that this note is only for recording what I have learnt so far, so they are for reference only and not 100% correct. Feel free to contact me if you got any questions.","title":"Computer Science"},{"location":"cs/#table-of-contents","text":"Machine Learning Computer Architecture Programming Languages Rust Algorithms Markdown","title":"Table of Contents"},{"location":"cs/algo/","text":"Algorithm \u00b6 To be implemented in 2023 Spring when I will be taking the COMP3711 course.","title":"Algorithm"},{"location":"cs/algo/#algorithm","text":"To be implemented in 2023 Spring when I will be taking the COMP3711 course.","title":"Algorithm"},{"location":"cs/archi/","text":"Computer Architecture \u00b6 Computer Basics \u00b6 Number System \u00b6 There are mainly three number systems: decimal, binary, and hexadecimal. Decimal (base 10) is used by people, binary (base 2) is used by computers, where 0 represents OFF state (no volatge) and 1 represents ON state (have voltage), hexadecimal (base 16) is a concise representation of binary numbers. Some examples: \\(3525_{10}\\) , \\(100111_{2}\\) , FA289B \\(_{16}\\) . The convertions between them are already taught in high school. Units \u00b6 There are several units e.g.: kilo, mega, giga, tera, peta. But notice that they mean different things in different scenarios. When dealing with size such as memory or file , 1 kilo = 2^10, 1 mega = 2^20. When dealing with rate/frequency such as # instructions per sec, # clock ticks per sec, network speed, 1 kilo = 10^3, 1 mega = 10^6. Class of Computers \u00b6 Personal Computers (PC) General Purpose, variety of software Subject to cost / performance tradeoff Server Computers Network based High capacity, performance, reliability Range from small servers to building sized Supercomputers High-end scientific and engineering calculations Highest capability but represent a small fraction of the overall computer market Embedded Computers Hidden as components of systems Stringest power / performance / cost constraints Levels of Abstractions \u00b6 Hardware => System Software (e.g.: Windows, Linux) => Application Software Both hardware and software are organized into hierarchical layers, in which lower-level details are hidden to offer a simpler view at the higher levels. There are levels of program code to cope with these layers and their interactions as well, namely binary machine language program, assembly language (e.g.: MIPS), and high-level language (e.g.: C/C++). The abstract interface between hardware and software is called Instruction Set Architecture (ISA) (e.g. Intel x86, ARM, MIPS). It can allow different implementations of varying cost and performance to follow the same instruction test architecture. Example: 80x86, Pentium, Pentium II, Pentium III, Pentium 4 all implement the same ISA. Components of Computer \u00b6 Input To communicate with the computer Data and instructions transferred to the memory Output To communicate with the user Data is read from the memory Memory Large store to keep instructions and data Processor Datapath: processes data according to instructions Control: commands the operations of input, output, memory, and datapath according to the instructions Technology Trend \u00b6 Vacuum Tubes (1950s) => Transistors (1950s and 1960s) => Integrated Circuits (1960s and 70s) => Very Large Scale Integrated (VLSI) Circuit (1980s and on) Cost / performance is improving due to underlying technology development. Moore's law: the number of transistors on microclips doubles every two years. This makes novel applications possible such as AI, World Wide Web, search machines ...","title":"Computer Architecture"},{"location":"cs/archi/#computer-architecture","text":"","title":"Computer Architecture"},{"location":"cs/archi/#computer-basics","text":"","title":"Computer Basics"},{"location":"cs/archi/#number-system","text":"There are mainly three number systems: decimal, binary, and hexadecimal. Decimal (base 10) is used by people, binary (base 2) is used by computers, where 0 represents OFF state (no volatge) and 1 represents ON state (have voltage), hexadecimal (base 16) is a concise representation of binary numbers. Some examples: \\(3525_{10}\\) , \\(100111_{2}\\) , FA289B \\(_{16}\\) . The convertions between them are already taught in high school.","title":"Number System"},{"location":"cs/archi/#units","text":"There are several units e.g.: kilo, mega, giga, tera, peta. But notice that they mean different things in different scenarios. When dealing with size such as memory or file , 1 kilo = 2^10, 1 mega = 2^20. When dealing with rate/frequency such as # instructions per sec, # clock ticks per sec, network speed, 1 kilo = 10^3, 1 mega = 10^6.","title":"Units"},{"location":"cs/archi/#class-of-computers","text":"Personal Computers (PC) General Purpose, variety of software Subject to cost / performance tradeoff Server Computers Network based High capacity, performance, reliability Range from small servers to building sized Supercomputers High-end scientific and engineering calculations Highest capability but represent a small fraction of the overall computer market Embedded Computers Hidden as components of systems Stringest power / performance / cost constraints","title":"Class of Computers"},{"location":"cs/archi/#levels-of-abstractions","text":"Hardware => System Software (e.g.: Windows, Linux) => Application Software Both hardware and software are organized into hierarchical layers, in which lower-level details are hidden to offer a simpler view at the higher levels. There are levels of program code to cope with these layers and their interactions as well, namely binary machine language program, assembly language (e.g.: MIPS), and high-level language (e.g.: C/C++). The abstract interface between hardware and software is called Instruction Set Architecture (ISA) (e.g. Intel x86, ARM, MIPS). It can allow different implementations of varying cost and performance to follow the same instruction test architecture. Example: 80x86, Pentium, Pentium II, Pentium III, Pentium 4 all implement the same ISA.","title":"Levels of Abstractions"},{"location":"cs/archi/#components-of-computer","text":"Input To communicate with the computer Data and instructions transferred to the memory Output To communicate with the user Data is read from the memory Memory Large store to keep instructions and data Processor Datapath: processes data according to instructions Control: commands the operations of input, output, memory, and datapath according to the instructions","title":"Components of Computer"},{"location":"cs/archi/#technology-trend","text":"Vacuum Tubes (1950s) => Transistors (1950s and 1960s) => Integrated Circuits (1960s and 70s) => Very Large Scale Integrated (VLSI) Circuit (1980s and on) Cost / performance is improving due to underlying technology development. Moore's law: the number of transistors on microclips doubles every two years. This makes novel applications possible such as AI, World Wide Web, search machines ...","title":"Technology Trend"},{"location":"cs/markdown/","text":"Markdown \u00b6 First Paragraph \u00b6 Info This place is reserved for self-studying markdown syntax. Debug bruh Help bruh Solution bruh Notice bruh Abstract bruh Language English Mandarin Cantonese 1 Why \u4e3a\u4ec0\u4e48 \u9ede\u89e3 2 good morning \u65e9\u4e0a\u597d \u65e9\u6668 C++ int gcd ( int a , int b ) { return b == 0 ? a : gcd ( b , a % b ); } Python for i in range ( 100 ): np . sum ( a [ i ], axis = 0 ) Rust fn makes_copy ( some_integer : i32 ) { // some_integer comes into scope println! ( \"{}\" , some_integer ); } \u6e90\u7801\uff1a ridiculousfish/cdecl-blocks a b c d Hello, \\(a^2 + b^2 = c^2\\) $$ a^2 + c^2 $$","title":"Markdown"},{"location":"cs/markdown/#markdown","text":"","title":"Markdown"},{"location":"cs/markdown/#first-paragraph","text":"Info This place is reserved for self-studying markdown syntax. Debug bruh Help bruh Solution bruh Notice bruh Abstract bruh Language English Mandarin Cantonese 1 Why \u4e3a\u4ec0\u4e48 \u9ede\u89e3 2 good morning \u65e9\u4e0a\u597d \u65e9\u6668 C++ int gcd ( int a , int b ) { return b == 0 ? a : gcd ( b , a % b ); } Python for i in range ( 100 ): np . sum ( a [ i ], axis = 0 ) Rust fn makes_copy ( some_integer : i32 ) { // some_integer comes into scope println! ( \"{}\" , some_integer ); } \u6e90\u7801\uff1a ridiculousfish/cdecl-blocks a b c d Hello, \\(a^2 + b^2 = c^2\\) $$ a^2 + c^2 $$","title":"First Paragraph"},{"location":"cs/ml/","text":"Machine Learning \u00b6 Info Some of the examples and sentences here are directly adopted from ISML, ESL, and COMP4432 Statistical Machine Learning lecture slides. The note is not complete and may have frequent modifications. Statistical learning (\u7edf\u8ba1\u5b66\u4e60) refers to learn from data, can be classified as supervised, semi-supervised and unsupervised: supervised (with labeled output): prediction, estimation unsupervised (without labeled output): clustering semi-supervised (large amount of unlabeled data and small amount of labeled data) For prediction there are two types as well, regression (\u56de\u5f52) refers to continuous or quantitative output value, otherwise would be classification (\u5206\u7c7b). Linear regression is the simplest regression method by using linear equations to approximate a certain function. Mathematical Formulations \u00b6 \\(n\\) : Number of distinct data points \\(p\\) : Number of features / predictors / variables For instance, regarding Wage , there are 300 sample points where they are 10 factors like year , age , race , ... Then we have \\(n = 300\\) , \\(p = 10\\) . We can use matrix \\(\\textbf{X}\\) to denote the data, and vector \\(\\textbf{y}\\) to denote output: \\[\\begin{aligned} \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1 p} \\\\ x_{21} & x_{22} & \\cdots & x_{2 p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n 1} & x_{n 2} & \\cdots & x_{n p} \\end{array}\\right] \\ \\ \\ y=\\left(\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{array}\\right) \\end{aligned}\\] The rows of \\(\\textbf{X}\\) refer to each data point, we can denote it as \\(x_i\\) : x i = ( x i 1 x i 2 \u22ee x i p ) The columns of \\(\\textbf{x}\\) refer to the collection of data points in terms of a certain feature, we can denote it as \\(\\textbf{x}_j\\) : x j = ( x 1 j x 2 j \u22ee x n j ) Therefore, X = ( x 1 x 2 \u22ef x p ) = ( x 1 T x 2 T \u22ee x n T ) We can give this formula: $$ \\textbf{y} = f(\\textbf{X}) + \\varepsilon $$ \\(\\textbf{y}\\) is the observed output while \\(\\textbf{X}\\) is the observed input, \\(f\\) is the ground truth function that maps \\(\\textbf{X}\\) into the ideal output, there's an error between observed and ideal output, and we have the assumption that \\(\\mathbb{E}(\\varepsilon)=0\\) . A set of inputs \\(\\textbf{X}\\) is always available but it is not always easy for us to obtain the corresponding \\(\\textbf{y}\\) . We denote \\(\\hat{\\textbf{y}}\\) as an estimate of \\(\\textbf{y}\\) , and \\(\\hat{f}(\\textbf{X})\\) an estimate of \\(f(\\textbf{X})\\) . Basics \u00b6 Reducible and irreducible error \u00b6 Then with \\(\\mathbb{E}(\\varepsilon)=0\\) and an additional assumption that both \\(\\hat{f}\\) and \\(X\\) is fixed, we have: \\[\\begin{align} \\mathbb{E}(Y-\\hat{Y})^2 & = \\mathbb{E}[f(X) +\\varepsilon - \\hat{f}(X)]^2\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + 2 \\mathbb{E}((f(X)-\\hat{f}(X))\\varepsilon) + \\mathbb{E}(\\varepsilon^2)\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\mathbb{E}(\\varepsilon^2) - \\mathbb{E}(\\varepsilon)^2\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\text{Var}(\\varepsilon) \\end{align}\\] \\(\\mathbb{E}[f(X) - \\hat{f}(X)]^2\\) is the error that we could reduce by selecting the most appropriate model, but \\(\\text{Var}(\\varepsilon)\\) is the irreducible error. Model choosing \u00b6 To minimize the reducible error, we need to pick a suitable model to estimate model, there are two types of model, namely parametric and non-parametric. For the parametric method, we first make an assumption about the form of the ground truth function e.g. linear. After the model has been selected, we use the training data to fit the model (train the parameters). A simple example: we assume \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\) . The the model is trained to find the values of the parameters: \\(\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_p\\) s.t. \\(Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\) . The most common approach is called ordinary least squares which we will introduce in next section. Non-parametric methods do not make explicit assumptions about the function form of \\(f\\) , so it is more flexible than non-parametric methods. Instead, they seek an estimate of \\(f\\) s.t. it gets as close to the data points as possible without being too rough or wiggly. As the photo has shown, the top-left is the ground truth function, the top-right is the parametric liner model (which is underfitting in this case), the bottom-left is the fitted spline model, and the bottom-right is a rough spline model with zero errors on the training data (which is called overfitting ). The definition of underfitting (not flexible enough) is that it performs poor in both training and testing data, while the definition of overfitting (too flexible) is that it performs very well in training data but poor in the testing data. We will further illustrate this in the bias-varience tradeoff part. The red curve is testing error and grey curve is training error. As you can see, when flexibility increases, the grey curve gradually fits into the noise, the training error reaches minimum point when flexibility equals to 5. Note Note that model flexibility is not always proportional to the number of parameters. This holds for linear regression though. Although we have many available models, there is no free lunch in statistics , which means there doesn't exist the so-called best model for all cases. On a particular data set, one specific method may work best, but some other method may work better on a similar but different set. Measuring the quality of fit \u00b6 We typically use mean-squared-error (MSE) given by \\[\\text{MSE} = \\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}}\\] There are train and test MSEs and our main interest is to choose the method that minimise test MSE . To avoid the curve being too flexible, we normally use the smoothing spline method , where we add a special regularisation term: \\[\\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}} + \\lambda \\int \\hat{f}''(t) \\ dt\\] \\(\\lambda\\) is the tuning parameter, when it is zero, we could allow the intergal value to be very large, so the model can be very flexible i.e. \\(y_i = \\hat{f}(x_i)\\) , when it tends to infinity, as we want to minimize MSE, the model would be very rigid i.e. \\(y_i = \\hat{a}x_i + \\hat{b}\\) . So we can adjust the flexibility of the model by tuning the parameter. Bias & Variance tradeoff \u00b6 As it is impossible for us to obtain all data, we could only obtain some training data \\(D\\) , but we want the model to work for all possible scenarios, so we want to find \\(\\hat{f}\\) to minimise \\[\\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\] ( \\(\\hat{f}(x;D)\\) is the estimated function from the data set \\(D\\) ) Notice Actually we can take one additional expectation w.r.t. \\(x_0\\) here but it would make the formula too complicated. Then we can derive the formula: \\[\\begin{align} & \\ [f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) + \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) \\ | \\ X = x_0]^2 + [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ &+ 2 \\ [f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0] \\ [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] \\end{align}\\] The first term is a constant. Inside the second term \\(\\hat{f}(x_0;D)\\) is still a random variable. For the third term, \\([f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0\\) is a constant but [ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) | X = x_0] is a random variable. Then we take expectation w.r.t. \\(D\\) on both sides and we got the third term equal to 0 since \\[\\mathbb{E_D} [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] = \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) = 0\\] And the final result is \\[\\begin{align} & \\ \\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ \\underbrace{[f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D))] \\ | \\ X = x_0]^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_{D}{[\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2}}_{\\text{Variance}}\\\\ \\end{align}\\] This formula tells us that we need to balance both bias and variance instead of only focusing on minimizing bias. Bias refers to the error that is introduced by approximating a real-life problem, which may ne extremely complicated, by a much simpler model. The formula is quite easy to understand, is the difference between the ground truth value and the predicted value. Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. Ideally the estimate for \\(f\\) should not vary too much between training sets. The formula is basically a slightly complex version of \\(\\mathbb{E}[(X - \\mathbb{E}(X))^2]\\) . For simple model, it has low variance but large bias. Example: \\(Y = f(X) + \\varepsilon\\) , we simply let \\(\\hat{f}(X) = 0\\) , then variance is 0, but bias is very large \\(f(X)^2\\) . For very flexible model, it has high variance but low bias. Example: \\(Y = f(X) + \\varepsilon = 0 + \\varepsilon\\) , where the ground truth function is \\(0\\) , we let \\(\\hat{f}(x_i) = y_i\\) , which means it fits into the noise. Then \\(\\text{Var}(\\hat{f}(x_i)) = \\text{Var}(y_i) = \\text{Var}(\\varepsilon_i) = \\sigma_i^2\\) , bias is \\(f(x_i) - \\mathbb{E}(\\hat{f}(x_i)) = f(x_i) - \\mathbb{E}(y_i) = 0\\) . Regression \u00b6 Simple Linear Regression \u00b6 Multiple Linear Regression \u00b6 least square solution brief mention of matrix calculus Classification \u00b6","title":"Machine Learning"},{"location":"cs/ml/#machine-learning","text":"Info Some of the examples and sentences here are directly adopted from ISML, ESL, and COMP4432 Statistical Machine Learning lecture slides. The note is not complete and may have frequent modifications. Statistical learning (\u7edf\u8ba1\u5b66\u4e60) refers to learn from data, can be classified as supervised, semi-supervised and unsupervised: supervised (with labeled output): prediction, estimation unsupervised (without labeled output): clustering semi-supervised (large amount of unlabeled data and small amount of labeled data) For prediction there are two types as well, regression (\u56de\u5f52) refers to continuous or quantitative output value, otherwise would be classification (\u5206\u7c7b). Linear regression is the simplest regression method by using linear equations to approximate a certain function.","title":"Machine Learning"},{"location":"cs/ml/#mathematical-formulations","text":"\\(n\\) : Number of distinct data points \\(p\\) : Number of features / predictors / variables For instance, regarding Wage , there are 300 sample points where they are 10 factors like year , age , race , ... Then we have \\(n = 300\\) , \\(p = 10\\) . We can use matrix \\(\\textbf{X}\\) to denote the data, and vector \\(\\textbf{y}\\) to denote output: \\[\\begin{aligned} \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} & x_{12} & \\cdots & x_{1 p} \\\\ x_{21} & x_{22} & \\cdots & x_{2 p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n 1} & x_{n 2} & \\cdots & x_{n p} \\end{array}\\right] \\ \\ \\ y=\\left(\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{array}\\right) \\end{aligned}\\] The rows of \\(\\textbf{X}\\) refer to each data point, we can denote it as \\(x_i\\) : x i = ( x i 1 x i 2 \u22ee x i p ) The columns of \\(\\textbf{x}\\) refer to the collection of data points in terms of a certain feature, we can denote it as \\(\\textbf{x}_j\\) : x j = ( x 1 j x 2 j \u22ee x n j ) Therefore, X = ( x 1 x 2 \u22ef x p ) = ( x 1 T x 2 T \u22ee x n T ) We can give this formula: $$ \\textbf{y} = f(\\textbf{X}) + \\varepsilon $$ \\(\\textbf{y}\\) is the observed output while \\(\\textbf{X}\\) is the observed input, \\(f\\) is the ground truth function that maps \\(\\textbf{X}\\) into the ideal output, there's an error between observed and ideal output, and we have the assumption that \\(\\mathbb{E}(\\varepsilon)=0\\) . A set of inputs \\(\\textbf{X}\\) is always available but it is not always easy for us to obtain the corresponding \\(\\textbf{y}\\) . We denote \\(\\hat{\\textbf{y}}\\) as an estimate of \\(\\textbf{y}\\) , and \\(\\hat{f}(\\textbf{X})\\) an estimate of \\(f(\\textbf{X})\\) .","title":"Mathematical Formulations"},{"location":"cs/ml/#basics","text":"","title":"Basics"},{"location":"cs/ml/#reducible-and-irreducible-error","text":"Then with \\(\\mathbb{E}(\\varepsilon)=0\\) and an additional assumption that both \\(\\hat{f}\\) and \\(X\\) is fixed, we have: \\[\\begin{align} \\mathbb{E}(Y-\\hat{Y})^2 & = \\mathbb{E}[f(X) +\\varepsilon - \\hat{f}(X)]^2\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + 2 \\mathbb{E}((f(X)-\\hat{f}(X))\\varepsilon) + \\mathbb{E}(\\varepsilon^2)\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\mathbb{E}(\\varepsilon^2) - \\mathbb{E}(\\varepsilon)^2\\\\ &= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\text{Var}(\\varepsilon) \\end{align}\\] \\(\\mathbb{E}[f(X) - \\hat{f}(X)]^2\\) is the error that we could reduce by selecting the most appropriate model, but \\(\\text{Var}(\\varepsilon)\\) is the irreducible error.","title":"Reducible and irreducible error"},{"location":"cs/ml/#model-choosing","text":"To minimize the reducible error, we need to pick a suitable model to estimate model, there are two types of model, namely parametric and non-parametric. For the parametric method, we first make an assumption about the form of the ground truth function e.g. linear. After the model has been selected, we use the training data to fit the model (train the parameters). A simple example: we assume \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\) . The the model is trained to find the values of the parameters: \\(\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_p\\) s.t. \\(Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\) . The most common approach is called ordinary least squares which we will introduce in next section. Non-parametric methods do not make explicit assumptions about the function form of \\(f\\) , so it is more flexible than non-parametric methods. Instead, they seek an estimate of \\(f\\) s.t. it gets as close to the data points as possible without being too rough or wiggly. As the photo has shown, the top-left is the ground truth function, the top-right is the parametric liner model (which is underfitting in this case), the bottom-left is the fitted spline model, and the bottom-right is a rough spline model with zero errors on the training data (which is called overfitting ). The definition of underfitting (not flexible enough) is that it performs poor in both training and testing data, while the definition of overfitting (too flexible) is that it performs very well in training data but poor in the testing data. We will further illustrate this in the bias-varience tradeoff part. The red curve is testing error and grey curve is training error. As you can see, when flexibility increases, the grey curve gradually fits into the noise, the training error reaches minimum point when flexibility equals to 5. Note Note that model flexibility is not always proportional to the number of parameters. This holds for linear regression though. Although we have many available models, there is no free lunch in statistics , which means there doesn't exist the so-called best model for all cases. On a particular data set, one specific method may work best, but some other method may work better on a similar but different set.","title":"Model choosing"},{"location":"cs/ml/#measuring-the-quality-of-fit","text":"We typically use mean-squared-error (MSE) given by \\[\\text{MSE} = \\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}}\\] There are train and test MSEs and our main interest is to choose the method that minimise test MSE . To avoid the curve being too flexible, we normally use the smoothing spline method , where we add a special regularisation term: \\[\\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}} + \\lambda \\int \\hat{f}''(t) \\ dt\\] \\(\\lambda\\) is the tuning parameter, when it is zero, we could allow the intergal value to be very large, so the model can be very flexible i.e. \\(y_i = \\hat{f}(x_i)\\) , when it tends to infinity, as we want to minimize MSE, the model would be very rigid i.e. \\(y_i = \\hat{a}x_i + \\hat{b}\\) . So we can adjust the flexibility of the model by tuning the parameter.","title":"Measuring the quality of fit"},{"location":"cs/ml/#bias-variance-tradeoff","text":"As it is impossible for us to obtain all data, we could only obtain some training data \\(D\\) , but we want the model to work for all possible scenarios, so we want to find \\(\\hat{f}\\) to minimise \\[\\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\] ( \\(\\hat{f}(x;D)\\) is the estimated function from the data set \\(D\\) ) Notice Actually we can take one additional expectation w.r.t. \\(x_0\\) here but it would make the formula too complicated. Then we can derive the formula: \\[\\begin{align} & \\ [f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) + \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) \\ | \\ X = x_0]^2 + [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ &+ 2 \\ [f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0] \\ [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] \\end{align}\\] The first term is a constant. Inside the second term \\(\\hat{f}(x_0;D)\\) is still a random variable. For the third term, \\([f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0\\) is a constant but [ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) | X = x_0] is a random variable. Then we take expectation w.r.t. \\(D\\) on both sides and we got the third term equal to 0 since \\[\\mathbb{E_D} [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] = \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) = 0\\] And the final result is \\[\\begin{align} & \\ \\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =& \\ \\underbrace{[f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D))] \\ | \\ X = x_0]^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_{D}{[\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2}}_{\\text{Variance}}\\\\ \\end{align}\\] This formula tells us that we need to balance both bias and variance instead of only focusing on minimizing bias. Bias refers to the error that is introduced by approximating a real-life problem, which may ne extremely complicated, by a much simpler model. The formula is quite easy to understand, is the difference between the ground truth value and the predicted value. Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. Ideally the estimate for \\(f\\) should not vary too much between training sets. The formula is basically a slightly complex version of \\(\\mathbb{E}[(X - \\mathbb{E}(X))^2]\\) . For simple model, it has low variance but large bias. Example: \\(Y = f(X) + \\varepsilon\\) , we simply let \\(\\hat{f}(X) = 0\\) , then variance is 0, but bias is very large \\(f(X)^2\\) . For very flexible model, it has high variance but low bias. Example: \\(Y = f(X) + \\varepsilon = 0 + \\varepsilon\\) , where the ground truth function is \\(0\\) , we let \\(\\hat{f}(x_i) = y_i\\) , which means it fits into the noise. Then \\(\\text{Var}(\\hat{f}(x_i)) = \\text{Var}(y_i) = \\text{Var}(\\varepsilon_i) = \\sigma_i^2\\) , bias is \\(f(x_i) - \\mathbb{E}(\\hat{f}(x_i)) = f(x_i) - \\mathbb{E}(y_i) = 0\\) .","title":"Bias &amp; Variance tradeoff"},{"location":"cs/ml/#regression","text":"","title":"Regression"},{"location":"cs/ml/#simple-linear-regression","text":"","title":"Simple Linear Regression"},{"location":"cs/ml/#multiple-linear-regression","text":"least square solution brief mention of matrix calculus","title":"Multiple Linear Regression"},{"location":"cs/ml/#classification","text":"","title":"Classification"},{"location":"cs/pl/","text":"Programming Languages \u00b6 Table of Contents \u00b6 Rust The Proramming Paradigms \u00b6","title":"Programming Languages"},{"location":"cs/pl/#programming-languages","text":"","title":"Programming Languages"},{"location":"cs/pl/#table-of-contents","text":"Rust","title":"Table of Contents"},{"location":"cs/pl/#the-proramming-paradigms","text":"","title":"The Proramming Paradigms"},{"location":"cs/pl/rust/","text":"Rust \u00b6 Abstract Rust is a statically typed language so it must know the types of all variables at compile time. The language does not have a GC (Garbage Collector) but achieve the purposes by its unique functionalities called ownership and lifetime . Many of the examples and sentences here are directly adopted from The Rust Programming Language . Basics \u00b6 Compile and Run \u00b6 When there's only one file, we can use rustc main.rs to compile (just like javac in Java ) and then use ./main.rs to run it. If things get complicated, we could make use of the powerful package manager of Rust, Cargo. It is easy to use, only several commands can satisfy most of the daily usage. Useful Commands \u00b6 Create a new project $ cargo new hello_cargo Build (compile) the project $ cargo build Compile and Run the project $ cargo run Hello World Program \u00b6 fn main () { println! ( \"Hello, world!\" ); } As you can see, the syntax here is quite similar to C/C++, there are some differences though. fn here refers to function, and also ! is added behind println as println in Rust is a macro. (The meaning of macro would be introduced later). Compilation / Runtime error of Rust \u00b6 Rust can give very powerful static checking during compilation and it can help you figure out most of the bugs before actually running the code with useful error message. Runtime error in Rust is called panic . Variables and Mutability \u00b6 we use let to define new variables, and we call this kind of value assignment as variable binding . Like in C++ there are variables and constants, in Scala there are var and val , we have mutable and immutable variables in Rust as well , but the variables are by default immutable as immutability has a lot of advantages (check Wikipedia page for detailed explanations). To make the variable mutable, we should add the keyword mut at the beginning. let x = 5 ; let mut y = 6 ; const THREE_HOURS_IN_SECONDS : u32 = 60 * 60 * 3 ; Notice However, note that constants and immutable variables are different. The naming convention for constants is using all upper cases. Constants are valid for the entire time when a program runs, within the scope they were declared in. Also, constants can only be set to a constant expression, not the result of a value that could only be computed during runtime. Variable scope is a concept that is frequently used, it refers to the range within a program for which an item is valid. For example: { let s = \"hello\" ; } Inside the curly bracket there defines a scope, and s is valid inside, after going out of the scope, it becomes invalid, just like the local variable. We can declare a new variable with the same name as previous variable and we call this kind of operation as shadowing . let y = 5 ; let y = y + 1 ; { let y = y * 2 ; println! ( \"The value of y in the inner scope is: {y}\" ); } Like this one, we first initialize the immutable variable y by binding 5 to it. Then we shadow y by y + 1 . Inside the curly brackets, we shadow y again, when the scope is over, the inner shadowing ends. So the output value should be 6. The following code can run smoothly: let spaces = \" \" ; spaces = spaces . len (); However, if we add mut to spaces . It would pop out errors because the compiler perceives it as mutating the type of the variable instead of shadowing, which is not allowed. Data Type I \u00b6 Info Here I will only introduce some basic data types, more complicated ones like String, Vector, Map would be introduced later as Data Type II . As shown previously, we don't always need to write out the type explicitly ( let y = 5; ) unless the compiler requires more about the type information of the variable. If doing so, it would be like: let y: i32 = 5; , it is called type annotation . There are two data type subsets, namely Scalar Types and Compound Types . Scalar Type \u00b6 Integer Type \u00b6 u32 is one of the integer types, where u refers to unsigned, 32 refers to the number of bits. Similarly, there are i32 , u16 , u32 , i128 , ... By basic CS knowledge, it is trivial to calculate the range of each type. The isize and usize types depend on the architecture of the computer your program is running on, which is denoted in the table as \u201carch\u201d: 64 bits if you\u2019re on a 64-bit architecture and 32 bits if you\u2019re on a 32-bit architecture. We can call the functions usize::MIN , usize::MAX , isize::MIN , isize::MAX . The number literals can be represented under different basis. For example: 0xff , 0o77 , 0b1100_0100 , b'A' , 345_678 etc. One of the unique features of Rust is that it can insert _ inside the numbers to improve the readability like the binary number given above. We still need to handle the integer overflow issue in Rust. Wrap in all modes with the wrapping_* methods, such as wrapping_add Return the None value if there is overflow with the checked_* methods Return the value and a boolean indicating whether there was overflow with the overflowing_* methods Saturate at the value\u2019s minimum or maximum values with saturating_* methods let of_x : u8 = 233 ; let of_y : u8 = 133 ; of_x . wrapping_add ( of_y ); // 110 of_x . checked_add ( of_y ); // None of_x . overflowing_add ( of_y ). 1 ; // true of_x . saturating_add ( of_y )); // 255 Floating-point Type \u00b6 Only f32 and f64 . The boolean type and char type are similiar to C/C++. Compound Type \u00b6 Tuple Type \u00b6 Tuple has fixed length but allows its elements to have different types. We can use pattern matching to destructure a tuple value as following: fn main () { let tup = ( 500 , 6.4 , 1 ); let ( x , y , z ) = tup ; println! ( \"The value of y is: {y}\" ); } We could also access the components of them using the following syntax: tup . 0 ; tup . 1 ; tup . 2 ; Array Type \u00b6 The array still has fixed length but all elements inside must have the same type. It is not as flexible as the vector type as it has varible length. However, the arrays are useful if you want your data to be allocated on the stack rather than the heap. let a : [ i32 ; 5 ] = [ 1 , 2 , 3 , 4 , 5 ]; let b = [ 3 ; 5 ]; // [3, 3, 3, 3, 3] We could access the elements by using the following syntax: a [ 0 ]; a [ 1 ]; a [ 2 ]; Rust can pop out runtime errors when we have invalid array element access. use std :: io ; fn main () { let a = [ 1 , 2 , 3 , 4 , 5 ]; println! ( \"Please enter an array index.\" ); let mut index = String :: new (); io :: stdin () . read_line ( & mut index ) . expect ( \"Failed to read line\" ); let index : usize = index . trim () . parse () . expect ( \"Index entered was not a number\" ); let element = a [ index ]; println! ( \"The value of the element at index {index} is: {element}\" ); } We neglect the very detail of this program at this moment, we only care about if we input 6 as index, the following error would pop out while C / C++ cannot: thread 'main' panicked at 'index out of bounds: the len is 5 but the index is 10', src/main.rs:19:19 note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace This actually shows Rust's memory safety principles in action. It is very hard to debug if having invalid array elements access in C/C++ because it would trigger unexpected modification of data in other side of the program. Function \u00b6 A typical example: fn plus_two ( x : i32 , y : i32 ) -> i32 { x + y } We use : i32 to give type annotation on the incoming arguments, and -> refers to the return type. () (unit type) stands for nothing is returned. You may notice no ; is put behind x + y , this shows the difference between statement and expression . Statements are instructions that perform a certain action, while expressions evaluate to a resulting value. Like the one above is an expression, where ; should noe be added. Another example: let x = ( let y = 6 ); This one is incorrect, if you want to bind the value of y to x, the correct code should be: let x = { let y = 6 ; y }; Comments \u00b6 Comments in Rust are just similiar to C/C++. // Hello, // we are comments! Documentation Comments would be written later here. Control Flows \u00b6 if Expressions \u00b6 One Example (no parenthesis is needed): if number < 5 { println! ( \"condition was true\" ); } else { println! ( \"condition was false\" ); } Note that the following code cannot compiled unlike C/C++: fn main () { let number = 3 ; if number { println! ( \"number was three\" ); } } The integer cannot be directly casted into bool. So should write if number != 0 . Nested if is the same as C/C++. There's one useful syntactic sugar: let number = if condition { 5 } else { 6 }; Reminders The types of results of two expressions should be the same since number can only have one type. loops \u00b6 loop \u00b6 This is the endless loop which can only be stopped by the break command: loop { bruhbruhbruh ; if flag { break ; } } If there are nested loops, we could label the loops and indicate their names during break. fn main () { let mut count = 0 ; ' counting_up : loop { println! ( \"count = {count}\" ); let mut remaining = 10 ; loop { println! ( \"remaining = {remaining}\" ); if remaining == 9 { break ; } if count == 2 { break 'counting_up ; } remaining -= 1 ; } count += 1 ; } println! ( \"End count = {count}\" ); } Besides, we could use while and for loop as well: While loop: while number != 0 { println! ( \"{number}!\" ); number -= 1 ; } for loop: let a = [ 10 , 20 , 30 , 40 , 50 ]; for element in a { println! ( \"the value is: {element}\" ); } for number in ( 1 .. 4 ) { println! ( \"{number}!\" ); } Here (1..4) refers to [1, 4) just like Python, to refer to [1, 4] , we should use (1..=4) instead. * Ownership \u00b6 Info Ownership is Rust\u2019s most unique feature and has deep implications for the rest of the language. It enables Rust to make memory safety guarantees without needing a garbage collector, so it\u2019s important to understand how ownership works. It governs how a Rust program manages memory. Stack and Heap are two important data structures in memory management. Stack is FILO, all data stored on the stack must have a known, fixed size. Heap is less organized and stores data with unknown size at compile time or a size that might change. Efficency of putting new data \u00b6 The memory allocator of heap would find an empty spot that is big enough if new data to be inserted, this is called allocating on the heap . Therefore, pushing to the stack is faster than allocating on the heap as the allocator does not need to search for a place to store new data, the new location is always at the top of stack. Efficency of accessing data \u00b6 It is still faster on the stack as it does not need to follow a pointer to get there. The main purpose of ownership is keeping track of what parts of code are using what data on the heap, minimizing the amount of duplicate data on the heap, and cleaning up unused data on the heap so you don\u2019t run out of space are all problems that ownership addresses. There are three ownership rules: Each value in Rust has an owner. There can only be one owner at a time. When the owner goes out of scope, the value will be dropped. Data Type II \u00b6 String Type \u00b6 The String type is far more complicated than it seems. let s1 = String :: from ( \"hello\" ); let s2 = \"world\" . to_string (); We can convert raw string to the String type either by calling String::from or .to_string() . The internal structure looks like this: The left part is stored on stack, consisting of: the length and capacity (we can ignore capacity at this moment) of the string, and the pointer ptr pointing to string content on the heap. We store it on the heap because the length of string might be changed later (e.g.: extension). We use different methods to append new raw string / char. let mut s1 = String :: from ( \"foo\" ); let s2 = \"bar\" ; s1 . push_str ( s2 ); println! ( \"s2 is {}\" , s2 ); let mut s = String :: from ( \"lo\" ); s . push ( 'l' ); We will cover the shallow and deep copying issue. Vector \u00b6 Map \u00b6","title":"Rust"},{"location":"cs/pl/rust/#rust","text":"Abstract Rust is a statically typed language so it must know the types of all variables at compile time. The language does not have a GC (Garbage Collector) but achieve the purposes by its unique functionalities called ownership and lifetime . Many of the examples and sentences here are directly adopted from The Rust Programming Language .","title":"Rust"},{"location":"cs/pl/rust/#basics","text":"","title":"Basics"},{"location":"cs/pl/rust/#compile-and-run","text":"When there's only one file, we can use rustc main.rs to compile (just like javac in Java ) and then use ./main.rs to run it. If things get complicated, we could make use of the powerful package manager of Rust, Cargo. It is easy to use, only several commands can satisfy most of the daily usage.","title":"Compile and Run"},{"location":"cs/pl/rust/#hello-world-program","text":"fn main () { println! ( \"Hello, world!\" ); } As you can see, the syntax here is quite similar to C/C++, there are some differences though. fn here refers to function, and also ! is added behind println as println in Rust is a macro. (The meaning of macro would be introduced later).","title":"Hello World Program"},{"location":"cs/pl/rust/#compilation-runtime-error-of-rust","text":"Rust can give very powerful static checking during compilation and it can help you figure out most of the bugs before actually running the code with useful error message. Runtime error in Rust is called panic .","title":"Compilation / Runtime error of Rust"},{"location":"cs/pl/rust/#variables-and-mutability","text":"we use let to define new variables, and we call this kind of value assignment as variable binding . Like in C++ there are variables and constants, in Scala there are var and val , we have mutable and immutable variables in Rust as well , but the variables are by default immutable as immutability has a lot of advantages (check Wikipedia page for detailed explanations). To make the variable mutable, we should add the keyword mut at the beginning. let x = 5 ; let mut y = 6 ; const THREE_HOURS_IN_SECONDS : u32 = 60 * 60 * 3 ; Notice However, note that constants and immutable variables are different. The naming convention for constants is using all upper cases. Constants are valid for the entire time when a program runs, within the scope they were declared in. Also, constants can only be set to a constant expression, not the result of a value that could only be computed during runtime. Variable scope is a concept that is frequently used, it refers to the range within a program for which an item is valid. For example: { let s = \"hello\" ; } Inside the curly bracket there defines a scope, and s is valid inside, after going out of the scope, it becomes invalid, just like the local variable. We can declare a new variable with the same name as previous variable and we call this kind of operation as shadowing . let y = 5 ; let y = y + 1 ; { let y = y * 2 ; println! ( \"The value of y in the inner scope is: {y}\" ); } Like this one, we first initialize the immutable variable y by binding 5 to it. Then we shadow y by y + 1 . Inside the curly brackets, we shadow y again, when the scope is over, the inner shadowing ends. So the output value should be 6. The following code can run smoothly: let spaces = \" \" ; spaces = spaces . len (); However, if we add mut to spaces . It would pop out errors because the compiler perceives it as mutating the type of the variable instead of shadowing, which is not allowed.","title":"Variables and Mutability"},{"location":"cs/pl/rust/#data-type-i","text":"Info Here I will only introduce some basic data types, more complicated ones like String, Vector, Map would be introduced later as Data Type II . As shown previously, we don't always need to write out the type explicitly ( let y = 5; ) unless the compiler requires more about the type information of the variable. If doing so, it would be like: let y: i32 = 5; , it is called type annotation . There are two data type subsets, namely Scalar Types and Compound Types .","title":"Data Type I"},{"location":"cs/pl/rust/#scalar-type","text":"","title":"Scalar Type"},{"location":"cs/pl/rust/#compound-type","text":"","title":"Compound Type"},{"location":"cs/pl/rust/#function","text":"A typical example: fn plus_two ( x : i32 , y : i32 ) -> i32 { x + y } We use : i32 to give type annotation on the incoming arguments, and -> refers to the return type. () (unit type) stands for nothing is returned. You may notice no ; is put behind x + y , this shows the difference between statement and expression . Statements are instructions that perform a certain action, while expressions evaluate to a resulting value. Like the one above is an expression, where ; should noe be added. Another example: let x = ( let y = 6 ); This one is incorrect, if you want to bind the value of y to x, the correct code should be: let x = { let y = 6 ; y };","title":"Function"},{"location":"cs/pl/rust/#comments","text":"Comments in Rust are just similiar to C/C++. // Hello, // we are comments! Documentation Comments would be written later here.","title":"Comments"},{"location":"cs/pl/rust/#control-flows","text":"","title":"Control Flows"},{"location":"cs/pl/rust/#if-expressions","text":"One Example (no parenthesis is needed): if number < 5 { println! ( \"condition was true\" ); } else { println! ( \"condition was false\" ); } Note that the following code cannot compiled unlike C/C++: fn main () { let number = 3 ; if number { println! ( \"number was three\" ); } } The integer cannot be directly casted into bool. So should write if number != 0 . Nested if is the same as C/C++. There's one useful syntactic sugar: let number = if condition { 5 } else { 6 }; Reminders The types of results of two expressions should be the same since number can only have one type.","title":"if Expressions"},{"location":"cs/pl/rust/#loops","text":"","title":"loops"},{"location":"cs/pl/rust/#ownership","text":"Info Ownership is Rust\u2019s most unique feature and has deep implications for the rest of the language. It enables Rust to make memory safety guarantees without needing a garbage collector, so it\u2019s important to understand how ownership works. It governs how a Rust program manages memory. Stack and Heap are two important data structures in memory management. Stack is FILO, all data stored on the stack must have a known, fixed size. Heap is less organized and stores data with unknown size at compile time or a size that might change.","title":"* Ownership"},{"location":"cs/pl/rust/#data-type-ii","text":"","title":"Data Type II"},{"location":"cs/pl/rust/#string-type","text":"The String type is far more complicated than it seems. let s1 = String :: from ( \"hello\" ); let s2 = \"world\" . to_string (); We can convert raw string to the String type either by calling String::from or .to_string() . The internal structure looks like this: The left part is stored on stack, consisting of: the length and capacity (we can ignore capacity at this moment) of the string, and the pointer ptr pointing to string content on the heap. We store it on the heap because the length of string might be changed later (e.g.: extension). We use different methods to append new raw string / char. let mut s1 = String :: from ( \"foo\" ); let s2 = \"bar\" ; s1 . push_str ( s2 ); println! ( \"s2 is {}\" , s2 ); let mut s = String :: from ( \"lo\" ); s . push ( 'l' ); We will cover the shallow and deep copying issue.","title":"String Type"},{"location":"cs/pl/rust/#vector","text":"","title":"Vector"},{"location":"cs/pl/rust/#map","text":"","title":"Map"},{"location":"math/","text":"Mathematics \u00b6","title":"Mathematics"},{"location":"math/#mathematics","text":"","title":"Mathematics"}]}