{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Welcome to Matheart's Blog !","text":"<p>site.info</p> <p>\u53d1\u4e9b\u751f\u6d3b\u91cc\u7684\u5c0f\u611f\u60f3\uff0c\u8bed\u6587\u6c34\u5e73\u5f88\u4e00\u822c\u6240\u4ee5\u6587\u7b14\u70c2\u8bf7\u89c1\u8c05\uff0c\u4ee5\u53ca\u4f5c\u4e3a\u9e3d\u5b50\u4e0d\u5b9a\u65f6\u66f4\u65b0(x\uff0c\u7b97\u662f\u4f5c\u4e3a\u4e00\u4e2a\u534a\u516c\u5f00/\u534a\u6b63\u5f0f\u7684\u5c0f\u7a9d\u3002\u731c\u770b\u7684\u5927\u90e8\u5206\u90fd\u662f\u6211\u73b0\u5b9e\u6216\u8005\u7f51\u7edc\u4e0a\u7684\u670b\u53cb\uff08\uff09\u5982\u679c\u4e0d\u662f\u7684\u8bdd\u6216\u8005\u60f3\u8054\u7cfb\u6211\u7684\u8bdd\u53ef\u79c1\u4fe1B\u7ad9\u6578\u5fc3\u8d26\u53f7</p> about(site.author) <ul> <li>\u6e05\u6c34\u6e7e\u8ba1\u7b97\u673a\u53ca\u6570\u5b66\u4e13\u4e1a\u5927\u4e09\u5b66\u751f\u4e00\u679a</li> <li>\u559c\u6b22\u542c\u4e00\u70b9\u4e2dV\uff08\u6d1b\u5929\u4f9d/\u8bd7\u5cb8\uff09\uff0c\u770b\u4e00\u4e9b\u756a/\u89c6\u89c9\u5c0f\u8bf4\uff0c\u559c\u6b22\u542c\u4e00\u4e9b\u7ca4\u8bed\u6b4c</li> <li>B\u7ad9\u77e5\u8bc6\u533aUP\u4e3b\uff0c\u4f5b\u7cfb\u505a\u4e9b\u6709\u8da3\u7684\u79d1\u666e\u5185\u5bb9\uff08\u867d\u7136\u5df2\u7ecf\u9e3d\u4e86\u4e00\u5e74\u591a\u4e86\uff09</li> <li>\u72ec\u6765\u72ec\u5f80\u4f46\u662f\u6df7\u719f\u4e86\u5f88\u597d\u76f8\u5904\u7684\u793e\u6050</li> </ul>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li> <p>[\u65e5\u5e38] </p> <ul> <li>\u4e00\u70b9\u5c0fupdate</li> <li>2023\u5e74\u603b\u7ed3</li> </ul> </li> <li> <p>[\u5b66\u672f]</p> <ul> <li>\u6cdb\u51fd\u5206\u6790-\u65e0\u9650\u7ef4\u7684\u7ebf\u6027\u4ee3\u6570\uff08\u4e0a\uff09</li> <li>\u5982\u4f55\u7ecf\u8425\u793e\u4ea4\u5e73\u53f0\u6765\u83b7\u5f97\u66f4\u597d\u7684\u5b66\u672f\u4fe1\u606f\u6e90</li> </ul> </li> </ul>"},{"location":"academic/acad_media/","title":"\u5982\u4f55\u7ecf\u8425\u793e\u4ea4\u5e73\u53f0\u6765\u83b7\u5f97\u66f4\u597d\u7684\u5b66\u672f\u4fe1\u606f\u8f93\u5165\u6e90","text":""},{"location":"academic/acad_media/#_2","title":"\u5b66\u672f\u4e3b\u9875","text":"<p>\u5f88\u591a\u79d1\u7814\u4eba\u5458\u90fd\u4f1a\u6709\u81ea\u5df1\u7684academic profile\uff0c\u4e00\u822c\u6302\u8f7d\u5728github.io\u4e0a\uff0c\u89c1\u8fc7\u6709\u4e24\u79cd\u4e3b\u9875\u7ba1\u7406\u5de5\u5177\uff0c\u4e00\u79cd\u662fJekyll &amp; AcademicPages\uff08\u53c2\u8003ydai03.github.io/\uff09\uff0c\u53e6\u4e00\u79cd\u662fjemdoc+MathJax\uff08\u53c2\u8003jasondlee88.github.io/\uff09</p> <p>\u5982\u679c\u4f60\u60f3\u83b7\u5f97\u66f4\u5927\u66dd\u5149\u5ea6\u6216\u8005\u8ba9\u540c\u884c\u8ba4\u8bc6\u4f60\u7684\u5174\u8da3\u548c\u5de5\u4f5c\u5c31\u53ef\u4ee5\u8003\u8651\u5efa\u7acb\uff0c\u6559\u7a0b\u53ef\u76f4\u63a5\u53c2\u8003\u7ba1\u7406\u5de5\u5177\u5b98\u7f51\u3002</p>"},{"location":"academic/acad_media/#_3","title":"\u535a\u5ba2","text":"<p>\u9664\u4e86\u5b66\u672f\u4e3b\u9875\u5916\u4e5f\u53ef\u8fd0\u8425\u81ea\u5df1\u7684\u535a\u5ba2\uff0c\u7528\u4e8e\u5206\u4eab\u6280\u672f/paper reading/\u751f\u6d3b\u7b49\uff0c\u53ef\u4ee5\u53d1\u5728\u5b66\u672f\u4e3b\u9875\u4e0a\uff0c\u4e5f\u53ef\u4ee5\u53e6\u5916\u5efa\u4e00\u4e2a\uff0c\u6302\u8f7d\u5728github.io\u7684\u5b50\u76ee\u5f55\uff08\u5982blog\uff09\uff0c\u6709\u5f88\u591a\u4e0d\u540c\u7684\u535a\u5ba2\u7ba1\u7406\u5de5\u5177\uff0c\u6211\u4e2a\u4eba\u611f\u89c9hexo\uff08\u53c2\u8003blog.tonycrane.cc/\uff09\u548cmkdocs\uff08\u6bd4\u8f83\u7b80\u7ea6\uff0c\u53c2\u8003note.tonycrane.cc/\uff09\u6bd4\u8f83\u597d\uff0cPS:\u8fd9\u91cc\u53c2\u8003\u535a\u5ba2\u548cnote\u7684\u5185\u5bb9\u4e5f\u975e\u5e38\u9ad8\u8d28\u91cf\u53ef\u4ee5\u770b\u770bhh</p>"},{"location":"academic/acad_media/#_4","title":"\u63a8\u7279","text":"<p>\u53ef\u4ee5\u8bf4\u662f\u975e\u5e38\u9ad8\u8d28\u91cf\u7684\u4fe1\u606f\u8f93\u5165\u6e90\uff0c\u4e0d\u8fc7\u5f97\u5148\u628a\u53f7\u517b\u597d\uff0c\u9996\u5148\u6362\u4e0a\u6bd4\u8f83\u6b63\u89c4\u7684\u5934\u50cf\u548cid\uff08\u6bd4\u5982\u4f60\u7684\u771f\u540d\uff09\uff0c\u6362\u6210professional profile\u7136\u540ewebsite\u53ef\u4ee5\u586b\u4f60\u5efa\u597d\u7684\u5b66\u672f\u4e3b\u9875\uff0c\u5982\u679c\u521a\u5f00\u59cb\u96f6\u5173\u6ce8\u7684\u8bdd\uff0c\u53ef\u4ee5\u5148\u5173\u6ce8\u4f60\u611f\u5174\u8da3\u9886\u57df\u91cc\u6bd4\u8f83\u51fa\u540d\u7684\u5927\u725b\uff08\u4ed6\u4eec\u4f1a\u7ecf\u5e38\u53d1\u8868\u4e00\u4e9b\u6709\u610f\u601d\u7684\u89c2\u70b9\u53ef\u4ee5\u89c2\u6469\u5b66\u4e60\uff09\uff0c\u4ee5\u53caust\u7684\u8001\u5e08\uff08\u5982song yangqiu\uff09\uff0c\u8fd8\u6709\u540c\u5b66\uff08\u6c42\u4e2a\u5173\u6ce8(x @MH2023ML)\u7b49\uff0c\u7136\u540e\u518d\u770b\u5b66\u672f\u5927\u725b\u7684\u5173\u6ce8\u5217\u8868\u6216\u8005\u8f6c\u53d1\u8bc4\u8bba\u7684\u4eba\uff0c\u770b\u4ed6\u4eec\u7684\u5e16\u5b50\uff0c\u5982\u679c\u611f\u5174\u8da3\u5c31\u53ef\u4ee5\u5173\u6ce8\uff0c\u5982\u6b64\u8fed\u4ee3\u51e0\u8f6e\u5c31\u53ef\u4ee5\u5173\u6ce8\u5230\u611f\u5174\u8da3\u9886\u57df\u91cc\u51e0\u5341\u4e2a\u8001\u5e08\u6216PhD\uff0c\u4ee5\u53ca\u53ef\u4ee5\u641c\u7d22\u6700\u8fd1\u4f1a\u8bae\u7684tag\u5982#neurips2023 #cvpr2024\u7b49\uff0c\u4f1a\u6709\u4e00\u4e9b\u4eba\u5206\u4eab\u4ed6\u4eec\u505a\u7684\u5de5\u4f5c\u3002\u7136\u540e\u56de\u5230feed\uff0c\u73b0\u5728\u7684feed\u53ef\u80fd\u8fd8\u6bd4\u8f83\u6df7\u4e71\uff0c\u53ef\u4ee5\u9009\u62e9\u70b9\u8d5e\u611f\u5174\u8da3\u9886\u57df\u7684\u5185\u5bb9\uff0c\u5e76\u4e14dislike\u65e0\u5173\u5185\u5bb9\uff0c\u5237\u65b0\u51e0\u6b21\u53f7\u5c31\u517b\u597d\u4e86\u3002</p> <p>\u8fd9\u6837\u5c31\u62e5\u6709\u4e86\u6bd4\u8f83\u9ad8\u8d28\u91cf+\u4e00\u7ebf\u7684\u79d1\u7814\u4fe1\u606f\u6e90\uff0c\u5982\u679c\u60f3\u8ba9\u66f4\u591a\u5176\u4ed6\u79d1\u7814\u4eba\u5458\u5173\u6ce8\u4f60\uff0c\u53ef\u4ee5\u591a\u70b9\u8d5e/\u8f6c\u53d1/\u8bc4\u8bba\u8ba9\u5bf9\u65b9\u6ce8\u610f\u5230\uff1b\u4ee5\u53ca\u53ef\u4ee5\u53d1\u4e00\u4e2a\u7f6e\u9876\u5e16\u8bf4\u81ea\u5df1\u5df2\u5efa\u597d\u5b66\u672f\u4e3b\u9875\u6b22\u8fce\u5bf9\u65b9\u770b i.e. </p> <p>Example</p> <p>I am thrilled to announce the launch of my academic page: http://matheart.github.io \ud83c\udf86 Feel free to chat with me if you have similar research interests\ud83e\udd73\ud83e\udd73 #DeepLearning #HKUST #MachineLearning #Statistics #MLTheory</p>"},{"location":"academic/acad_media/#_5","title":"\u77e5\u4e4e","text":"<p>\u548c\u63a8\u7279\u7c7b\u4f3c</p>"},{"location":"academic/func_anal1/","title":"\u6cdb\u51fd\u5206\u6790-\u65e0\u9650\u7ef4\u7684\u7ebf\u6027\u4ee3\u6570\uff08\u4e0a\uff09","text":"<p>Note</p> <p>2023.10.18</p> <p>\u63a5\u4e0b\u6765\u4e24\u5929\u90fd\u6709\u671f\u4e2d\u8003\uff0c\u660e\u65e9\u8981\u8003\u5fc3\u7406\u5b66\uff0c\u540e\u5929\u4e0b\u5348\u8981\u8003\u6cdb\u51fd\uff0c\u4f46\u662f\u5f88\u7cbe\u795e\u6839\u672c\u4e0d\u60f3\u7761\u89c9\uff0c\u90a3\u5c31\u8d81\u73b0\u5728\u5927\u6982\u5199\u5199\u5427\uff0c \u7b49\u4e8e\u662f\u672c\u79d1\u6cdb\u51fd\u5206\u6790\u524d\u534a\u90e8\u5206\u7684\u7b80\u8981\u611f\u6027\u7684\u603b\u7ed3\uff0c\u6240\u4ee5\u5f88\u53ef\u80fd\u5305\u542b\u9519\u8bef\uff0c\u5c31\u5e0c\u671b\u5927\u5bb6\u591a\u52a0\u5305\u6db5\u4e86(x</p> <p>\u5b66\u4e60\u4e86\u534a\u5b66\u671f\u7684\u6cdb\u51fd\u5206\u6790 (MATH4063)\uff0c\u4e2a\u4eba\u611f\u89c9\u8fd9\u95e8\u8bfe\u50cf\u662f\u7ebf\u6027\u4ee3\u6570+\u6570\u5b66\u5206\u6790+\u62d3\u6251\uff0c\u5b9e\u5206\u6790\u91cc\u7684\\(L^p\\), \\(l^p\\)\u7a7a\u95f4\u7b97\u662f\u63d0\u4f9b\u4e86\u4e00\u4e9b\u4f8b\u5b50\uff0c\u672c\u8d28\u66f4\u50cf\u662f\u628a\"\u7ebf\u6027\u4ee3\u6570\"\u4ece\u6709\u9650\u7ef4\u63a8\u5e7f\u5230\u65e0\u9650\u7ef4\uff0c\u65e0\u9650\u7ef4\u6709\u51e0\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u5982\u5e8f\u5217\uff0c\u51fd\u6570\uff0c\u591a\u9879\u5f0f\u7b49\u3002\u5728\u5b66\u4e60\u7684\u8fc7\u7a0b\u4e2d\u6211\u611f\u53d7\u5230\u201c\u65e0\u9650\u7ef4\u201d\u4e0e\u201c\u6709\u9650\u7ef4\u201d\u6709\u672c\u8d28\u4e0a\u7684\u533a\u522b\uff08\u5c31\u7b97\u975e\u5e38\u5927\u7684\u6709\u9650\u7ef4\uff0c\u548c\u65e0\u9650\u7ef4\u4e5f\u4e0d\u662f\u4e00\u4e2a\u6982\u5ff5\uff09\uff0c\u5982\u8bc1\u660eHahn-Banach\u5b9a\u7406\uff08\u6cdb\u51fd\u5206\u6790\u7684\u57fa\u77f3\u4e4b\u4e00\uff09\uff0c\u5728\u5ef6\u62d3\u65f6\u9700\u8981\u7528\u5230Zorn's lemma\u8bf4\u660e\u5ef6\u62d3\u662f\u6781\u5927\u7684\uff0c\u4ee5\u53ca\u95ed\u5355\u4f4d\u7403S\u5728\u5f3a\u62d3\u6251\u662f\u975e\u7d27\u7684\uff0c\u5373\u4f7f\u6211\u4eec\u4fee\u6539\u62d3\u6251\uff08\u653e\u677e\u7d27\u7684\u6761\u4ef6\uff09\uff0c\u5b83\u5728\u5f31\u62d3\u6251\u4e0b\u4f9d\u7136\u662f\u975e\u7d27\u7684\uff1b\u4ee5\u53ca\u5728\u6cdb\u51fd\u5206\u6790\u91cc\u51fd\u6570f\u548c\u53c2\u6570x\u7684\u5730\u4f4d\u662f\u4e00\u6837\u7684\uff0c\u6240\u4ee5\u80fd\u7528\\(\\langle f, x \\rangle\\)\u751a\u81f3\\(\\langle x, f \\rangle\\)\u8fd9\u79cd\u8bb0\u53f7\u6765\u8868\u793a\uff0c\u51fd\u6570\\(f\\)\u4e5f\u80fd\u4f5c\u4e3a\u4e00\u4e2a\u5143\u7d20\u53bb\u7814\u7a76\uff0c\u6bd4\u5982\u653e\u5728\u51fd\u6570\u7a7a\u95f4\u91cc\u7814\u7a76\u5b83\u4eec\u7684\u6027\u8d28\uff0c\u4e5f\u80fd\u7814\u7a76\u5b83\u4eec\u7684\u5ea6\u91cf\u3002\u5bf9\u7279\u6b8a\u7684\u51fd\u6570\uff08\u7b97\u5b50\uff09\u4e5f\u80fd\u7814\u7a76\u5b83\u4eec\u7684\u5404\u79cd\u6027\u8d28\uff0c\u5982\u95ed\u7b97\u5b50\uff0c\u7b97\u5b50\u7684\u8c31\u7b49\u7b49\uff08\u8fd9\u4e9b\u5728\u6cdb\u51fd\u540e\u534a\u90e8\u5206\u5e94\u8be5\u624d\u8bb2\u5230\uff09\u3002</p> <p>\u6cdb\u51fd\u6709\u56db\u5927\u57fa\u77f3\uff0c\u4ee5\u4e0a\u63d0\u5230\u7684Hahn-Banach\u5b9a\u7406\u9891\u7e41\u51fa\u73b0\u5728\u6cdb\u51fd\u5404\u79cd\u5b9a\u7406\u7684\u8bc1\u660e\u5f53\u4e2d\uff0c\u4e00\u81f4\u6709\u754c\u539f\u7406\u4f53\u73b0\"\u7ebf\u6027\"\u7684\u5f3a\u5927\u4e4b\u5904\uff0c\u80fd\u4ecepointwise bounded\u63a8\u5230uniform bounded\uff0c\u800c\u8fd9\u5b9a\u7406\u53c8\u80fd\u63a8\u51fa\u5f00\u6620\u5c04\u5b9a\u7406\u548c\u95ed\u56fe\u50cf\u5b9a\u7406\uff0c\u5f00\u6620\u5c04\u5b9a\u7406\u7b49\u540c\u4e8e\u662f\u80fd\u628a\u5f00\u96c6\u6620\u5c04\u5230\u5f00\u96c6\uff08\u7b49\u4e8e\u628a\"\u8fde\u7eed\"imply\u7684\u4e1c\u897f\u53cd\u8fc7\u6765\uff09\uff0c\u95ed\u56fe\u50cf\u5b9a\u7406\u628a\u7ebf\u6027\u7b97\u5b50\u7684\u8fde\u7eed\u6027\u548c\u56fe\u50cf\u7684\u95ed\u7684\u6027\u8d28\u8054\u7cfb\u4e86\u8d77\u6765\uff08\u5e94\u8be5\u5728\u540e\u9762\u7b97\u5b50\u7684\u90e8\u5206\u4f1a\u8d77\u5230\u6bd4\u8f83\u591a\u7684\u4f5c\u7528\uff09\u3002</p> <p>\u8bb2\u5b8c\u8fd9\u51e0\u4e2a\u5b9a\u7406\u540e\uff0c\u5c31\u5f00\u59cb\u8bb2\u5f31\u62d3\u6251\u8fd9\u4e2a\u7ae0\u8282\uff0c\u4e2a\u4eba\u89c9\u5f97\u8fd9\u7ae0\u8282\u6700\u62bd\u8c61\uff0c\u4e3b\u8981\u662f\u8bb2\u5982\u4f55\u4fee\u6539\u62d3\u6251\u4f7f\u5f97\u5b83\u66f4\"\u7c97\u7cd9\"\uff0c\u4ea7\u751f\u66f4\u591a\u7d27\u96c6\u4ece\u800c\u6709\u66f4\u591a\u66f4\u597d\u7684\u6027\u8d28\uff0c\u5728\\(E\\), \\(E^*\\), \\(E^{**}\\)\u8d4b\u4e88\u7684\u5404\u79cd\u62d3\u6251\u4e0a\u63a8\u5404\u79cd\u5f00\uff0c\u95ed\uff0c\u7d27\u6027\u8d28\u786e\u5b9e\u633amind-blowing\u4e5f\u633a\u96be\u61c2\u7684\u2026\u2026\u8bc1\u660e\u4e5f\u7528\u5230\u4e0d\u5c11\u62d3\u6251\uff0c\u8ba9\u5728\u6570\u5206\u91cc\u53ea\u5b66\u8fc7\u4e00\u70b9\u70b9\u96c6\u62d3\u6251\u7684\u6211\u82b1\u4e86\u4e0d\u5c11\u65f6\u95f4\u8865\uff0c\u5f31\u62d3\u6251\u91cc\u7684\u90bb\u57df\u57fa\u91cc\u4e5f\u633a\u6709\u8da3\u7684\uff0c\u542c\u8001\u5e08\u8bf4\u7c7b\u4f3c\u4e8e\u5ea6\u91cf\u7a7a\u95f4\u91cc\u7684\u5f00\u7403\u3002</p> <p>\u5728\u5b66\u7684\u8fc7\u7a0b\u4e2d\u6211\u4e5f\u4f53\u4f1a\u5230\u201c\u7ebf\u6027\u201d\u7684\u5f3a\u5927\u4e4b\u5904\uff0c\u6bd4\u5982\u5728\u201c\u7ebf\u6027\u201d\u4e0b\u8fde\u7eed\u548c\u6709\u754c\u7b49\u4ef7\uff0c\u4ee5\u53ca\u975e\u5e38\u591a\u8bc1\u660e\u90fd\u9700\u8981\u4f9d\u8d56\u7ebf\u6027\u6765\u5b8c\u6210\u8bc1\u660e\uff0c\u6bd4\u5982\u5f00\u6620\u5c04\u5b9a\u7406\u91cc\u5c31\u662f\u8981\u4e0d\u65ad\u6784\u9020\\(\\{ z_i \\}\\)\u4f7f\u5f97\u5b83\u6ee1\u8db3\u4e00\u4e9b\u6027\u8d28\uff0c\u8fd9\u5c31\u9700\u8981\u7ebf\u6027\uff0c\u4ee5\u53ca\u67d0\u4e9b\u5b9a\u7406\u91cc\u8981\u8bc1\u660e\u4e00\u4e9bterm\u8981\u7b49\u4e8e0\uff0c\u5c31\u53ea\u9700\u8981\u8bf4\u660e\u4e00\u4e2avector \\(v\\)\u80fd\u4efb\u610f\u4f38\u7f29\uff0c\u4f7f\u5f97\u503c\u53ef\u4ee5\u4efb\u610f\u5927\u6216\u8005\u4efb\u610f\u5c0f\uff08\u5982\u679c\u975e0\uff09\uff0c\u4f1a\u4f7f\u5f97\u67d0\u4e9b\u4e0d\u7b49\u5f0f\u4e0d\u6210\u7acb\uff0c\u8fd9\u6837\u7684\u8bdd\u5c31\u53ea\u80fd\u7b49\u4e8e0\uff0c\u5f88\u96be\u60f3\u8c61\u975e\u7ebf\u6027\u6cdb\u51fd\u5206\u6790\u4f1a\u662f\u4ec0\u4e48\u6837\u5b50\uff08\u8fd9\u4f30\u8ba1\u4f1a\u662fPhD\u8bfe\u5185\u5bb9\u4e86\u5427\uff09\u3002</p> <p>\u5176\u5b9e\u542c\u4e86\u90a3\u4e48\u591a\u6211\u4f9d\u7136\u89c9\u5f97\u8fd9\u4e9b\u62bd\u8c61\u7406\u8bba\u597d\u50cf\u6ca1\u6709\u5e94\u7528\uff0c\u4e0d\u8fc7\u8001\u5e08\u8bf4\u5728\u540e\u9762\u4f1a\u4ecb\u7ecd\u4e00\u70b9PDE\u548c\u6cdb\u51fd\u5728PDE\u4e0a\u7684\u5e94\u7528\uff0c\u5e0c\u671b\u4f1a\u633a\u6709\u8da3\u7684\uff0c\u4e0a\u793c\u62dc\u80e1\u7ee7\u5584\u8001\u5e08\u4ee3\u8bfe\uff08\u8bf4\u5b9e\u8bdd\u8bb2\u7684\u5f88\u4e00\u822c\uff09\uff0c\u8bb2\u4e86\u4e9bHilbert\u7a7a\u95f4\uff08\u671f\u4e2d\u8003\u4e0d\u8003\uff09\uff0c\u4f46\u8fd9\u90e8\u5206\u5e94\u8be5\u633a\u6709\u7528\u7684\uff0c\u5e94\u8be5\u5728\u5e94\u7528\u6570\u5b66\u91cc\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u7814\u7a76\u4e0a\uff0c\u6cdb\u51fd\u5e94\u8be5\u4e5f\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u636e\u6211\u6240\u77e5\uff0c\u5728\u7edf\u8ba1\u4e0a\u6cdb\u51fd\u4e5f\u6709\u5e94\u7528\uff08\u53c2\u8003Mathematical Foundations of Infinite-dimensional statistical models\uff0c\u4f46\u6211\u73b0\u5728\u770b\u4e0d\u61c2\u5c31\u662f\u4e86\uff09\u3002</p>"},{"location":"cs/","title":"Computer Science","text":"<p>Warning</p> <p>The code is written under Mac environment. Please note that this note is only for recording what I have learnt so far, so they are for reference only and not 100% correct. Feel free to contact me if you got any questions. </p>"},{"location":"cs/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Machine Learning<ul> <li>Statistical Learning</li> <li>Generalization</li> <li>Foundations of Machine Learning</li> </ul> </li> <li>Computer Architecture</li> <li>Programming Languages<ul> <li>Rust</li> <li>C++</li> </ul> </li> <li>Algorithms</li> <li>Markdown</li> </ul>"},{"location":"cs/algo/","title":"Algorithm","text":"<p>To be implemented in 2023 Spring when I will be taking the COMP3711 course.</p>"},{"location":"cs/archi/","title":"Computer Architecture","text":""},{"location":"cs/archi/#computer-basics","title":"Computer Basics","text":""},{"location":"cs/archi/#number-system","title":"Number System","text":"<p>There are mainly three number systems: decimal, binary, and hexadecimal.</p> <p>Decimal (base 10) is used by people, binary (base 2) is used by computers, where 0 represents OFF state (no volatge) and 1 represents ON state (have voltage), hexadecimal (base 16) is a concise representation of binary numbers. </p> <p>Some examples: \\(3525_{10}\\), \\(100111_{2}\\), FA289B\\(_{16}\\). The convertions between them are already taught in high school.</p>"},{"location":"cs/archi/#units","title":"Units","text":"<p>There are several units e.g.: kilo, mega, giga, tera, peta. But notice that they mean different things in different scenarios. When dealing with size such as memory or file, 1 kilo = 2^10, 1 mega = 2^20. When dealing with rate/frequency such as # instructions per sec, # clock ticks per sec, network speed,  1 kilo = 10^3, 1 mega = 10^6.</p>"},{"location":"cs/archi/#class-of-computers","title":"Class of Computers","text":"<ul> <li>Personal Computers (PC)<ul> <li>General Purpose, variety of software</li> <li>Subject to cost / performance tradeoff</li> </ul> </li> <li>Server Computers  <ul> <li>Network based</li> <li>High capacity, performance, reliability</li> <li>Range from small servers to building sized</li> </ul> </li> <li>Supercomputers<ul> <li>High-end scientific and engineering calculations</li> <li>Highest capability but represent a small fraction of the overall computer market </li> </ul> </li> <li>Embedded Computers<ul> <li>Hidden as components of systems</li> <li>Stringest power / performance / cost constraints</li> </ul> </li> </ul>"},{"location":"cs/archi/#levels-of-abstractions","title":"Levels of Abstractions","text":"<p>Hardware =&gt; System Software (e.g.: Windows, Linux) =&gt; Application Software</p> <p>Both hardware and software are organized into hierarchical layers, in which lower-level details are hidden to offer a simpler view at the higher levels.</p> <p>There are levels of program code to cope with these layers and their interactions as well, namely binary machine language program, assembly language (e.g.: MIPS), and high-level language (e.g.: C/C++). </p> <p>The abstract interface between hardware and software is called <code>Instruction Set Architecture</code> (ISA) (e.g. Intel x86, ARM, MIPS). It can allow different implementations of varying cost and performance to follow the same instruction test architecture. Example: 80x86, Pentium, Pentium II, Pentium III, Pentium 4 all implement the same ISA.</p>"},{"location":"cs/archi/#components-of-computer","title":"Components of Computer","text":"<ul> <li>Input<ul> <li>To communicate with the computer</li> <li>Data and instructions transferred to the memory</li> </ul> </li> <li>Output<ul> <li>To communicate with the user</li> <li>Data is read from the memory</li> </ul> </li> <li>Memory<ul> <li>Large store to keep instructions and data</li> </ul> </li> <li>Processor<ul> <li>Datapath: processes data according to instructions</li> <li>Control: commands the operations of input, output, memory, and datapath according to the instructions</li> </ul> </li> </ul>"},{"location":"cs/archi/#technology-trend","title":"Technology Trend","text":"<p>Vacuum Tubes (1950s) =&gt; Transistors (1950s and 1960s) =&gt; Integrated Circuits (1960s and 70s) =&gt; Very Large Scale Integrated (VLSI) Circuit (1980s and on)</p> <p>Cost / performance is improving due to underlying technology development.</p> <p>Moore's law: the number of transistors on microclips doubles every two years.</p> <p>This makes novel applications possible such as AI, World Wide Web, search machines ...</p>"},{"location":"cs/markdown/","title":"Markdown","text":""},{"location":"cs/markdown/#first-paragraph","title":"First Paragraph","text":"<p>Info</p> <p>This place is reserved for self-studying markdown syntax.</p> <p>Debug</p> <p>bruh</p> <p>Help</p> <p>bruh</p> <p>Solution</p> <p>bruh</p> <p>Notice</p> <p>bruh</p> <p>Abstract</p> <p>bruh</p> Language English Mandarin Cantonese 1 Why \u4e3a\u4ec0\u4e48 \u9ede\u89e3 2 good morning \u65e9\u4e0a\u597d \u65e9\u6668 C++<pre><code>int gcd(int a, int b) {\n    return b == 0 ? a : gcd(b, a % b);\n}\n</code></pre> Python<pre><code>for i in range(100):\n    np.sum(a[i], axis = 0)\n</code></pre> Rust<pre><code>fn makes_copy(some_integer: i32) { // some_integer comes into scope\n    println!(\"{}\", some_integer);\n}\n</code></pre> <p>\u6e90\u7801\uff1a ridiculousfish/cdecl-blocks</p> <ul> <li>a<ul> <li>b</li> <li>c</li> <li>d</li> </ul> </li> </ul> <p>Hello, \\(a^2 + b^2 = c^2\\) $$ a^2 + c^2 $$</p>"},{"location":"cs/ml/","title":"Machine Learning","text":"<p>Warning</p> <p>The code is written under Mac environment. Please note that this note is only for recording what I have learnt so far, so they are for reference only and not 100% correct. Feel free to contact me if you got any questions. </p>"},{"location":"cs/ml/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Statistical Learning</li> <li>Generalization</li> <li>[Foundations of Machine Learning][ml/fml.md]</li> </ul>"},{"location":"cs/ml/fml/","title":"Foundations of Machine Learning","text":"<p>Info</p> <p>This book, Foundations of Machine Learning, serves as a general introduction to machine learning theory, it covers some basic tools of the field and also shows how to use the tools to justify and analyze the behevaior of machine learning algorithms.</p>"},{"location":"cs/ml/fml/#ch-2-the-pac-learning-framework","title":"Ch 2 The PAC Learning Framework","text":"<p>Introduction</p> <p>PAC's full name is Probably Approximately Correct, this framework is mainly used to mathematically analyze the lower bound of the sample size, in order to achieve a certain high possibility while the error is small to some extent.</p> <p>To present in a more formal way, we define concept as the mapping we are going to learn from \\(\\mathcal{X} \\to \\mathcal{Y}\\). For example, concept can be the set of points inside a rectangle (which is a classical example we are going to show later), and the concept class \\(C\\) is the set of concepts we wish to learn. We denote \\(O(n)\\) as an u.b. on the cost of the computational representation of any element \\(x \\in \\mathcal{X}\\), and size(\\(c\\)) as the maximal cost of the computational representation of \\(c \\in C\\).</p> <p>The learning problem is formulated as we have a hypothesis set \\(H\\) and we pick hypothesis \\(h\\) from it in order to approximate the ground truth concept (more formally, we say, target concept). Note that we always assume there is an underlying distribution \\(D\\), and the examples are independent and identically distributed (iid), with this assumption, it is much easier for us to derive the theorems.</p> <p>The hypothesis does not always exactly match the target concept, so there must be error, there are two types of error respectively, namely empirical error (\\(\\hat{R}(h)\\)), and generalization error (\\(R(h)\\)). Empirical error can be accessed while the generalization errror is not directly accessible to the learner.</p> <p>Given a hypothesis \\(h \\in H\\), a target concept \\(c \\in C\\), and an underlying distribution \\(D\\), then:</p> \\[\\begin{align} R(h) &amp;= \\ \\ \\ \\mathbb{E}_{x \\sim D}{\\  \\mathbf{1}_{h(x) \\neq c(x)}} = \\text{Pr}_{x \\sim D}{[h(x) \\neq c(x)]}\\\\ \\hat{R}(h) &amp;= \\frac{1}{m}\\sum_{i = 1}^{m}{\\ \\mathbf{1}_{h(x) \\neq c(x)}} \\end{align}\\] <p>With iid assumption we can prove that the expectation of empirical error is the generalization error.</p>"},{"location":"cs/ml/fml/#pac-learnable","title":"PAC-Learnable","text":"<p>Definition</p> <p>A concept class \\(C\\) is said to be PAC-learnable if there exists an algorithm \\(\\mathcal{A}\\) and a polynomial function poly \\((\\cdot, \\cdot, \\cdot, \\cdot)\\) s.t. for any \\(\\varepsilon, \\delta &gt; 0\\), for all distributions \\(D\\) on \\(X\\) and for any target concept \\(c \\in C\\), the following holds for any sample size \\(m \\geq\\) poly \\((1/\\varepsilon, 1/\\delta, n, \\text{size}(c))\\):</p> \\[\\text{Pr}_{S \\sim D^m} [R(h_S) \\leq \\varepsilon] \\geq 1 - \\delta\\] <p>It is called efficiently PAC-learnable if it further runs in poly \\((1/\\varepsilon, 1/\\delta, n, \\text{size}(c))\\).</p> <p>The intuition is that if the required sample size is not polynomial of those four \"parameters\" (for example it grows exponentially), in most practical case it is unrealistic to collect such amount of data.</p> <p>Also note that this framework does not have any additional assumptions about underlying distribution, but both training and test samples should be drawn from the same distribution.</p>"},{"location":"cs/ml/fml/#classical-example-learning-axis-aligned-rectangles","title":"Classical Example: Learning axis-aligned Rectangles","text":"<p>We would not elaborate the proof here but would highlight some of the key points. First is the algorithm \\(\\mathcal{A}\\) is to return the tightest rectangle containing all points with label 1 (denote as \\(R_S\\)), s.t. there is no False-positives and there's only False-negatives (and they are contained in \\(R\\)). </p> <p>With such construction, we can safely assume Pr\\([R] &gt; \\varepsilon\\), and construct four rectangles at the edge of \\(R\\) respectively with each area exactly equal to \\(\\varepsilon/4\\). We could show that if \\(R_S\\) meets all four small rectangles, the error area \\(\\leq \\varepsilon\\). Its contraposition shows it must miss one of the small rectangle, the remaining part is just to derive probability inequality regarding \\(\\text{Pr}_{S \\sim D^m}[\\text{R}(R_S) &gt; \\varepsilon]\\), set \\(\\delta\\) equal to the upper bound of it, then we can finally derive the lower bound of \\(m\\) i.e. \\(\\frac{4}{\\varepsilon}\\log \\frac{4}{\\delta}\\). We know that \\(n, \\text{size}(c)\\) is constant, so it is PAC-learnable.</p> <p>Such way of proof can be extended to more interesting problems (exercises in book), such as extending this result from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^n\\), learning concentric circles / triangles instead of rectangles, learn with noise etc.</p>"},{"location":"cs/ml/fml/#guarantees-for-finite-hypothesis-sets","title":"Guarantees for finite hypothesis sets","text":"<p>We extend the example above to more general settings, it is actually consistent i.e. the empirical error is 0 on training sample. In order to derive the bound more easily we need to assume that the hypothesis set is of finite size.</p>"},{"location":"cs/ml/fml/#consistent-case","title":"Consistent case","text":"<p>Theorem</p> <p>Let \\(H\\) be a finite set of functions mapping from \\(\\mathcal{X}\\) to \\(\\mathcal{Y}\\). Let \\(\\mathcal{A}\\) be an algorithm that for any target concept \\(c \\in H\\) and i.i.d. sample \\(S\\) returns a consistent hypothesis \\(h_S: \\hat{R}(h_S) = 0\\). Then for any \\(\\varepsilon, \\delta &gt; 0\\), the inequality \\(\\text{Pr}_{S \\sim D^m}[R(h_S) \\leq \\varepsilon] \\geq 1 - \\delta\\) holds if:</p> \\[m \\geq \\frac{1}{\\varepsilon}(\\log |H| + \\log \\frac{1}{\\delta})\\] <p>Another equivalent statement is that for any \\(\\varepsilon, \\delta &gt; 0\\), with probability at least \\(1 - \\delta\\), we have</p> \\[R(h_S) \\leq \\frac{1}{m}(\\log |H| + \\log \\frac{1}{\\delta})\\] <p>Want: We don't know what will be the \\(h_S\\), so we need to give an uniform convergence bound, bound Pr[\\(\\exists h \\in H : \\hat{R}(h) = 0 \\text{ and } R(h) &gt; \\varepsilon\\)] by \\(\\delta\\).</p> <p>Trivially we know the set is equal to the union of all hypothesis \\(h\\) that has \\(\\hat{R}(h) = 0 \\text{ and } R(h) &gt; \\varepsilon\\), so by union bound, it is bounded by \\(\\sum_{h \\in H}{\\text{Pr}[\\hat{R}(h) = 0 \\text{ and } R(h) &gt; \\varepsilon]}\\), this is further bounded by the conditional probability \\(\\sum_{h \\in H}{\\text{Pr}[\\hat{R}(h) = 0 | R(h) &gt; \\varepsilon]}\\).</p> <p>For each \\(h\\) we are able to bound that by \\((1 - \\varepsilon)^m\\) by making use of definition of \\(R(h)\\) and also i.i.d. property, this gives us the overall value is bounded by \\(|H|(1 - \\varepsilon)^m \\leq |H| e^{-\\varepsilon m}\\).</p> <p>When \\(m \\geq \\frac{1}{\\varepsilon}(\\log |H| + \\log \\frac{1}{\\varepsilon})\\), with some simple algebraic manipulation we can obtain \\(|H| e^{-\\varepsilon m} \\leq \\delta\\), this concludes the proof.    \\(\\square\\)</p> <p>The second statement shows that when the training sample size \\(m\\) increases, the upper bound of generalization error will be tighter, this theoretically justifies learning algorithms benefit from larger labeled training samples. However, the price to pay for coming up with a consistent algorithm is the use of a larger hypothesis set \\(H\\) containing target concepts, the upper bound invreases with \\(|H|\\) but the dependency is only logarithmic.</p>"},{"location":"cs/ml/fml/#some-examples","title":"Some examples","text":"<p>Conjunction of Boolean literals: \\(|H|\\) is \\(3^n\\) which is finite, we can also carry out a consistent algorithm, which means this is suitable for the theorem above. We can show \\(m \\geq ((\\log 3)n + \\log \\frac{1}{\\delta})\\)</p> <p>Universal Concept Class: To guarantee a consistent hypothesis, \\(|H| \\geq |U_n| = 2^{(2^n)}\\), this gives us the sample complexity bound \\(m \\geq \\frac{1}{\\varepsilon}((\\log 2) 2^n + \\log \\frac{1}{\\delta})\\), this is actually not a polynomial, so it is not PAC-learnable.</p> <p>k-term DNF formulae: \\(m \\geq \\frac{1}{\\varepsilon}((\\log 3)nk + \\log \\frac{1}{\\delta})\\). However, it is shown that this problem is RP, it is PAC-learnable but not efficient unless RP = NP.</p> <p>k-CNF formulae: We can reduce the formula into conjection of boolean literals by a bijection, so this implies the PAC-learanability of K-CNF formula. However, although DNF formula can be rewritten into K-CNF formula, itself is still not PAC-learnable. This is beacause the number of new variables needed for this transformation is in \\(O(n^k)\\). This apparent paradox deals with key aspects of PAC-learning, which include the cost of the representation of the concept and the choice of the hypothesis set.</p>"},{"location":"cs/ml/fml/#inconsistent-case","title":"Inconsistent Case","text":"<p>In practice, \\(H\\) may not contain the hypothesis consistent with the training sample, but inconsistent hypothesis with small number of errors on number of training samples is still useful. Hoeffding's inequality will be used to relate the generalization error and empirical error of a single hypothesis.</p>"},{"location":"cs/ml/fml/#ch-3-rademacher-complexity-and-vc-dimensions","title":"Ch 3 Rademacher Complexity and VC-dimensions","text":"<p>Introduction</p> <p>We deal with the infinite hypothesis size in this chapter, we can use various techniques to reduce it into finite sets of hypotheses can proceed as in the previous chapter. Using Rademacher Complexity based on McDiarmid's inequality can derive high-quality bounds. However, the computation of empirical Rademacher complexity is NP-hard for some hypothesis sets. So we will introduce the growth function and VC-dimension which is easier to compute.</p> <p>Formally any loss function \\(L\\) is \\(\\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}\\), we use \\(G\\) to denote family of loss functions associated to \\(H\\), </p>"},{"location":"cs/ml/generalization/","title":"Domain Generalization","text":"<p>Note</p> <p>This is a note on domain generalization in order to prepare for entering statml research group.</p>"},{"location":"cs/ml/generalization/#introduction","title":"Introduction","text":"<p>Many applications do not fulfill the iid assumption and the machine learning systems often fail to generalize out-of-distribution in some scenarios such as self-driving car system, medical data etc. This motivates the development of a new area called domain generalization, which gets access to multiple datasets collected under different domains and incorporates the invariances into a classifier, and make the classifier have better out-of-distribution performance comparing with previous literatures.</p>"},{"location":"cs/ml/generalization/#domainbed","title":"Domainbed","text":"<p>A popular benchmark framework for domain generalization.</p>"},{"location":"cs/ml/generalization/#data-sets","title":"Data sets","text":"<p>There are several data sets available in Domainbed as listed in the following and more:</p> <p></p> <p>In the conditional offer problem I am required to do experiments on the PACS data sets, there are four domains: art, clipart, product, and photo respectively. So the experients are required to repeat four times by setting one of them as testing domain and the remaining three are training domains, this is also called leave-one-out.</p>"},{"location":"cs/ml/generalization/#baseline-algorithms","title":"Baseline algorithms","text":""},{"location":"cs/ml/statml/","title":"Machine Learning","text":"<p>Info</p> <p>Some of the examples and sentences here are directly adopted from MATH4432 Statistical Machine Learning lecture slides. The note is not complete and may have frequent modifications, and the knowledge about regression analysis is ignored as this is not the focus of the course.</p> <p>Statistical learning (\u7edf\u8ba1\u5b66\u4e60) refers to learn from data, can be classified as supervised, semi-supervised and unsupervised:</p> <ul> <li>supervised (with labeled output): prediction, estimation</li> <li>unsupervised (without labeled output): clustering</li> <li>semi-supervised (large amount of unlabeled data and small amount of labeled data) </li> </ul> <p>For prediction there are two types as well, regression (\u56de\u5f52) refers to continuous or quantitative output value, otherwise would be classification (\u5206\u7c7b). </p> <p>Linear regression is the simplest regression method by using linear equations to approximate a certain function.</p>"},{"location":"cs/ml/statml/#mathematical-formulations","title":"Mathematical Formulations","text":"<p>\\(n\\): Number of distinct data points</p> <p>\\(p\\): Number of features / predictors / variables</p> <p>For instance, regarding <code>Wage</code>, there are 300 sample points where they are 10 factors like <code>year</code>, <code>age</code>, <code>race</code>, ... Then we have \\(n = 300\\), \\(p = 10\\).</p> <p>We can use matrix \\(\\textbf{X}\\) to denote the data, and vector \\(\\textbf{y}\\) to denote output:</p> \\[\\begin{aligned} \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1 p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2 p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n 1} &amp; x_{n 2} &amp; \\cdots &amp; x_{n p} \\end{array}\\right]  \\ \\ \\ \\mathbf{y}=\\left(\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{array}\\right) \\end{aligned}\\] <p>The rows of \\(\\textbf{X}\\) refer to each data point, we can denote it as \\(x_i\\):</p> xi=(xi1xi2\u22eexip) <p>The columns of \\(\\textbf{x}\\) refer to the collection of data points in terms of a certain feature, we can denote it as \\(\\textbf{x}_j\\):</p> xj=(x1jx2j\u22eexnj) <p>Therefore,</p> X=(x1 x2 \u22ef xp)=(x1Tx2T\u22eexnT) <p>We can give this formula: $$ \\textbf{y} = f(\\textbf{X}) + \\varepsilon $$ \\(\\textbf{y}\\) is the observed output while \\(\\textbf{X}\\) is the observed input, \\(f\\) is the ground truth function that maps \\(\\textbf{X}\\) into the ideal output, there's an error between observed and ideal output, and we have the assumption that \\(\\mathbb{E}(\\varepsilon)=0\\).</p> <p>A set of inputs \\(\\textbf{X}\\) is always available but it is not always easy for us to obtain the corresponding \\(\\textbf{y}\\). We denote \\(\\hat{\\textbf{y}}\\) as an estimate of \\(\\textbf{y}\\), and \\(\\hat{f}(\\textbf{X})\\) an estimate of \\(f(\\textbf{X})\\).</p>"},{"location":"cs/ml/statml/#basics","title":"Basics","text":""},{"location":"cs/ml/statml/#reducible-and-irreducible-error","title":"Reducible and irreducible error","text":"<p>Then with \\(\\mathbb{E}(\\varepsilon)=0\\) and an additional assumption that both \\(\\hat{f}\\) and \\(X\\) is fixed for a moment, we have:</p> \\[\\begin{align} \\mathbb{E}(Y-\\hat{Y})^2 &amp; = \\mathbb{E}[f(X) +\\varepsilon - \\hat{f}(X)]^2\\\\ &amp;= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + 2 \\mathbb{E}((f(X)-\\hat{f}(X))\\varepsilon) + \\mathbb{E}(\\varepsilon^2)\\\\ &amp;= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\mathbb{E}(\\varepsilon^2) - \\mathbb{E}(\\varepsilon)^2\\\\ &amp;= \\mathbb{E}[f(X) - \\hat{f}(X)]^2 + \\text{Var}(\\varepsilon) \\end{align}\\] <p>\\(\\mathbb{E}[f(X) - \\hat{f}(X)]^2\\) is the error that we could reduce by selecting the most appropriate model, but \\(\\text{Var}(\\varepsilon)\\) is the irreducible error.</p>"},{"location":"cs/ml/statml/#model-choosing","title":"Model choosing","text":"<p>To minimize the reducible error, we need to pick a suitable model to estimate model, there are two types of model, namely parametric and non-parametric.</p> <p>For the parametric method, we first make an assumption about the form of the ground truth function e.g. linear. After the model has been selected, we use the training data to fit the model (train the parameters).</p> <p>A simple example: we assume \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\). The the model is trained to find the values of the parameters: \\(\\beta_0, \\beta_1, \\beta_2, \\cdots, \\beta_p\\) s.t. \\(Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p\\). The most common approach is called ordinary least squares which we will introduce in next section.</p> <p>Non-parametric methods do not make explicit assumptions about the function form of \\(f\\), so it is more flexible than non-parametric methods. Instead, they seek an estimate of \\(f\\) s.t. it gets as close to the data points as possible without being too rough or wiggly.</p> <p></p> <p>As the photo has shown, the top-left is the ground truth function, the top-right is the parametric liner model (which is underfitting in this case), the bottom-left is the fitted spline model, and the bottom-right is a rough spline model with zero errors on the training data (which is called overfitting).</p> <p>The definition of underfitting (not flexible enough) is that it performs poor in both training and testing data, while the definition of overfitting (too flexible) is that it performs very well in training data but poor in the testing data. We will further illustrate this in the bias-varience tradeoff part.</p> <p></p> <p>The red curve is testing error and grey curve is training error. As you can see, when flexibility increases, the grey curve gradually fits into the noise, the training error reaches minimum point when flexibility equals to 5.</p> <p>Note</p> <p>Note that model flexibility is not always proportional to the number of parameters. This holds for linear regression though.</p> <p>Although we have many available models, there is no free lunch in statistics, which means there doesn't exist the so-called best model for all cases. On a particular data set, one specific method may work best, but some other method may work better on a similar but different set. </p>"},{"location":"cs/ml/statml/#measuring-the-quality-of-fit","title":"Measuring the quality of fit","text":"<p>We typically use mean-squared-error (MSE) given by</p> \\[\\text{MSE} = \\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}}\\] <p>There are train and test MSEs and our main interest is to choose the method that minimise test MSE.</p> <p>To avoid the curve being too flexible, we normally use the smoothing spline method, where we add a special regularisation term:</p> \\[\\frac{1}{n}{\\sum_{i=1}^{n}{(y_i - \\hat{f}(x_i))^2}} + \\lambda \\int \\hat{f}''(t) \\ dt\\] <p>\\(\\lambda\\) is the tuning parameter, when it is zero, we could allow the intergal value to be very large, so the model can be very flexible i.e. \\(y_i = \\hat{f}(x_i)\\), when it tends to infinity, as we want to minimize MSE, the model would be very rigid i.e. \\(y_i = \\hat{a}x_i + \\hat{b}\\). So we can adjust the flexibility of the model by tuning the parameter.</p>"},{"location":"cs/ml/statml/#bias-variance-tradeoff","title":"Bias &amp; Variance tradeoff","text":"<p>As it is impossible for us to obtain all data, we could only obtain some training data \\(D\\), but we want the model to work for all possible scenarios, so we want to find \\(\\hat{f}\\) to minimise</p> \\[\\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\] <p>(\\(\\hat{f}(x;D)\\) is the estimated function from the data set \\(D\\))</p> <p>Notice</p> <p>Actually we can take one additional expectation w.r.t. \\(x_0\\) here but it would make the formula too complicated.</p> <p>Then we can derive the formula:</p> \\[\\begin{align} &amp; \\ [f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =&amp; \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) + \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =&amp; \\ [f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) \\ | \\ X = x_0]^2 + [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ &amp;+ 2 \\ [f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0] \\ [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] \\end{align}\\] <p>The first term is a constant. Inside the second term \\(\\hat{f}(x_0;D)\\) is still a random variable. For the third term, \\([f(x_0)-\\mathbb{E}_{D}[\\hat{f}(x_0;D)] \\ | \\ X = x_0]\\) is a constant but \\([\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]\\) is a random variable.</p> <p>Then we take expectation w.r.t. \\(D\\) on both sides and we got the third term equal to 0 since </p> \\[\\mathbb{E}_{D} [\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0] = \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\mathbb{E}_{D}(\\hat{f}(x_0;D)) = 0\\] <p>And the final result is</p> \\[\\begin{align} &amp; \\ \\mathbb{E}_{D}[f(x_0) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2\\\\ =&amp; \\ \\underbrace{[f(x_0) - \\mathbb{E}_{D}(\\hat{f}(x_0;D))] \\ | \\ X = x_0]^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_{D}{[\\ \\mathbb{E}_{D}(\\hat{f}(x_0;D)) - \\hat{f}(x_0; D) \\ | \\ X = x_0]^2}}_{\\text{Variance}}\\\\ \\end{align}\\] <p>This formula tells us that we need to balance both bias and variance instead of only focusing on minimizing bias.</p> <p>Bias refers to the error that is introduced by approximating a real-life problem, which may ne extremely complicated, by a much simpler model. The formula is quite easy to understand, is the difference between the ground truth value and the predicted value.</p> <p>Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. Ideally the estimate for \\(f\\) should not vary too much between training sets. The formula is basically a slightly complex version of \\(\\mathbb{E}[(X - \\mathbb{E}(X))^2]\\).</p> <p>For simple model, it has low variance but large bias. Example: \\(Y = f(X) + \\varepsilon\\), we simply let \\(\\hat{f}(X) = 0\\), then variance is 0, but bias is very large \\(f(X)^2\\). For very flexible model, it has high variance but low bias. Example: \\(Y = f(X) + \\varepsilon = 0 + \\varepsilon\\), where the ground truth function is \\(0\\), we let \\(\\hat{f}(x_i) = y_i\\), which means it fits into the noise. Then \\(\\text{Var}(\\hat{f}(x_i)) = \\text{Var}(y_i) = \\text{Var}(\\varepsilon_i) = \\sigma_i^2\\), bias is \\(f(x_i) - \\mathbb{E}(\\hat{f}(x_i)) = f(x_i) - \\mathbb{E}(y_i) = 0\\).</p>"},{"location":"cs/ml/statml/#regression","title":"Regression","text":""},{"location":"cs/ml/statml/#simple-linear-regression","title":"Simple Linear Regression","text":"<p>Let \\((x_1,y_1)\\), \\((x_2, y_2)\\), \\(\\cdots\\), \\((x_n,y_n)\\) represent \\(n\\) observation pairs, and we use simple linear regression \\(\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i\\) to fit the data.</p> <p>The error is defined as \\(e_i = y_i - \\hat{y_i}\\), then the residual sum of squares (RSS) is  $$ e_1^2 + e_2^2 + \\cdots + e_n^2 = \\sum_{i=1}^{n}{(y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2} $$ To minimise the error, we simply let \\(\\frac{\\partial (\\text{RSS})}{\\partial \\hat{\\beta_0}} = \\frac{\\partial (\\text{RSS})}{\\partial \\hat{\\beta_1}} = 0\\), then with some algebraic operations we obtain: $$ \\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}{(x_i - \\overline{x})(y_i - \\overline{y})}}{\\sum_{i=1}^{n}{(x_i - \\overline{x})^2}} \\text{, }  \\hat{\\beta_0} = \\overline{y} - \\hat{\\beta_1} \\overline{x} $$ Actually, when we are drawing the contour map of RSS w.r.t. \\(\\beta_0, \\beta_1\\), we can obtain the following ellipsoid shape:</p> <p></p> <p>This is actually because of the squared term \\((y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2\\), term like this such as \\(\\beta_0^2 + \\beta_1^2 = 1\\) can be written in the quadratic form (\u4e8c\u6b21\u578b):</p> \\[\\begin{bmatrix}  \\beta_0  \\\\ \\beta_1 \\end{bmatrix}^{T} \\begin{bmatrix}   1&amp;0 \\\\   0&amp;1 \\end{bmatrix} \\begin{bmatrix}  \\beta_0  \\\\ \\beta_1 \\end{bmatrix}\\] <p>The quadratic form like \\(\\beta^{T}A \\beta = C\\) with A being positive and definite (\u6b63\u5b9a) has ellipsoid shape.</p> <p>After we have computed the values of coefficients, we can use our statistical knowledge to assess the accuracy of estimates. We assume the true relationship between \\(X\\) and \\(Y\\) is the following: $$ Y = f(X) + \\varepsilon, f(X) = \\beta_0 + \\beta_1 X $$ Then as the graph shows</p> <p></p> <p>Red line is the true relationship, which is known as population regression line, other lines are least squares lines computed based on separate random set of observations. Each line is different but on average the least squares lines are quite close to the population regression line.</p> <p>We can use the residual standard error (RSE) to provide an absolute measure of lack of fit of the linear model (number of freedoms equals to the total number of parameters minus the two we want to estimate): $$ \\text{RSE} = \\sqrt{\\frac{1}{\\text{degree of freedom}} \\text{RSS}} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^{n}{(y_i - \\hat{y_i})^2}} $$</p> <p>R^2 statistics provides an alternative measure of fit.</p> <p>$$ \\text{TSS} = \\sum{(y_i - \\overline{y})^2},  \\ \\text{RSS} = \\sum{(y_i - \\hat{y_i})^2} $$ And \\(R^2\\) is given by </p> \\[ R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\] <p>The value is always between 0 and 1, if it is closed to 0, it means the prediction has similar effect as mean value, so it doesn't fit the data well; when it is closed to 1, it means the RSS is very small, that means it fits the data well.</p>"},{"location":"cs/ml/statml/#multiple-linear-regression","title":"Multiple Linear Regression","text":""},{"location":"cs/ml/statml/#derivation-of-formula","title":"Derivation of formula","text":"<p>The equation becomes much more complicated than before: $$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon $$</p> <p>Then our goal is to find \\(\\hat{\\beta} = [\\hat{\\beta_0}, \\hat{\\beta_1}, \\cdots, \\hat{\\beta_p}]\\) be the least squares</p> \\[ \\hat{\\beta} = \\text{argmin}_{\\beta = [\\beta_0, \\beta_1, \\cdots, \\beta_p]}{\\sum_{i=1}^{n}{(y_i - \\beta_0 - \\beta_1 x_{i1} - \\cdots - \\beta_{p} x_{ip})^2}} \\] <p>We know</p> \\[\\begin{aligned} \\mathbf{X}=\\left[\\begin{array}{ccccc} \\ 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1 p} \\\\ \\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2 p} \\\\ \\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\ 1 &amp; x_{n 1} &amp; x_{n 2} &amp; \\cdots &amp; x_{n p} \\end{array}\\right]  \\ \\ \\ \\mathbf{y}=\\left(\\begin{array}{c} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{array}\\right) \\end{aligned}\\] <p>where the first column is representing the constant part (would time \\(\\beta_0\\) later).</p> <p>Then the sum we want to minimise can be simplified as  \\(\\sum_{i=1}^{n}{(y_i - x_{\\text{row i}} \\ \\beta)^2} \\Rightarrow ||\\mathbf{y} - \\mathbf{X} \\beta||_2^2\\)</p> <p>Note</p> <p>\\(x_{\\text{row i}}\\) is row vector, \\(\\beta\\) is column vector </p> <p>\\(\\mathbf{X}: n \\times (1 + p)\\), \\(\\mathbf{y}: n \\times 1\\), \\(\\beta: (1 + p) \\times 1\\)</p> \\[\\begin{align} &amp;||\\mathbf{y}-\\mathbf{X}\\beta||\\\\ =&amp; \\ (\\mathbf{y}-\\mathbf{X}\\beta)^{T} (\\mathbf{y}-\\mathbf{X}\\beta)\\\\ =&amp; \\ (\\mathbf{y}^T - \\beta^T \\mathbf{X}^T) (\\mathbf{y}-\\mathbf{X}\\beta)\\\\ =&amp; \\ \\mathbf{y}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} \\beta \\ - \\beta^T \\mathbf{X}^T \\mathbf{y} + \\beta^T \\mathbf{X}^T \\mathbf{X}\\beta\\\\  \\end{align}\\] <p>By dimension of \\(\\mathbf{X}\\), \\(\\mathbf{y}\\) and \\(\\beta\\), we know all four of the above are scalar, and in which \\(\\mathbf{y}^T \\mathbf{X} \\beta\\) is the transpose of \\(\\beta^T \\mathbf{X}^T \\mathbf{y}\\), so they are supposed to have the same value:</p> <p>$$ \\hat{\\beta} = \\text{argmin}_{\\beta}{ (\\mathbf{y}^T \\mathbf{y} - 2\\mathbf{y}^T \\mathbf{X} \\beta  + \\beta^T \\mathbf{X}^T \\mathbf{X}\\beta)} $$ We take partial derivative w.r.t. \\(\\beta\\):</p> \\[ 0 - 2 \\mathbf{X}^T \\mathbf{y} + 2 \\mathbf{X}^T \\mathbf{X} \\beta = 0 \\] <p>Note</p> \\[\\frac{\\partial a^T X}{\\partial X} = a\\] \\[\\frac{\\partial X^T A X}{\\partial X} = (A + A^{T}) X\\] <p>Then</p> \\[ \\mathbf{X}^T \\mathbf{X} \\beta = \\mathbf{X}^T \\mathbf{y} \\] <p>So we finally obtain</p> \\[\\begin{align} \\hat{\\beta} &amp; = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^{T} \\textbf{y}\\\\ \\hat{y} &amp; = X \\hat{\\beta} \\end{align}\\]"},{"location":"cs/ml/statml/#derive-the-mean-and-variance-of-beta-estimate","title":"Derive the mean and variance of beta estimate","text":"<p>By \\(\\mathbf{y} = \\mathbf{X}\\beta + \\varepsilon\\) and \\(\\varepsilon\\) is a random noise vector, we know \\(\\mathbf{y}\\) is a random vector, so \\(\\hat \\beta\\) is a random vector, and what follows is that \\(\\hat y\\) is a random vector as well. With this, we can compute the mean and variance of \\(\\hat \\beta\\).</p> \\[\\begin{align} \\mathbb{E}(\\hat \\beta) &amp;= \\mathbb{E} [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}]\\\\ &amp;= \\mathbb{E} [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{X} \\beta + \\varepsilon)]\\\\ &amp;= \\mathbb{E} [\\mathbf{X}^{-1} (\\mathbf{X}^T)^{-1} \\mathbf{X}^T \\mathbf{X} \\beta \\ + (\\mathbf{X}^T \\mathbf{X}^{-1})\\mathbf{X}^T \\varepsilon]\\\\ &amp;= \\beta + (\\mathbf{X}^T \\mathbf{X}^{-1})\\mathbf{X}^T\\mathbb{E}(\\varepsilon)\\\\ &amp;= \\beta \\end{align}\\] <p>Note</p> \\[\\text{Var}(\\mathbf{z}) = \\mathbb{E}(\\mathbf{z} \\mathbf{z}^T) - \\mathbb{E}(\\mathbf{z}) \\mathbb{E}^{T}(\\mathbf{z})\\] \\[\\begin{align} \\text{Var}(\\hat \\beta) &amp;= \\text{Var}\\ [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}]\\\\ &amp;= \\text{Var}\\ [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{X}\\beta + \\varepsilon)]\\\\ &amp;= \\text{Var}\\ [\\beta + (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon]\\\\ &amp;= \\text{Var}\\ [(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon]\\\\ &amp;= \\mathbb{E}[(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon \\ \\cdot \\varepsilon^T \\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1}] -\\underbrace{\\mathbb{E}[(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon]}_{0} \\cdot \\mathbb{E}^{T}[(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\varepsilon]\\\\ &amp;= (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\ \\mathbb{E}[ \\varepsilon \\cdot \\varepsilon^T] \\ \\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1}\\\\ &amp;= (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\  \\sigma_{\\varepsilon}^{2} \\ \\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1}\\\\ &amp;=  \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^T \\mathbf{X})^{-1} \\end{align}\\] <p>Derivation</p> <p>By independence of each element inside the random vector \\(\\varepsilon\\), \\(\\text{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^{2} \\ I_{n \\times n}\\).</p> \\[ \\sigma_{\\varepsilon}^{2} \\ I_{n \\times n} = \\text{Var}(\\varepsilon) = \\mathbb{E}(\\varepsilon \\varepsilon^T) - \\mathbb{E}(\\varepsilon) \\mathbb{E}^{T}(\\varepsilon) = \\mathbb{E}(\\varepsilon \\varepsilon^T) \\] <p>e.g.: \\(\\varepsilon = \\begin{pmatrix} \\varepsilon_1\\\\ \\varepsilon_2  \\end{pmatrix}\\), \\(\\text{cov}\\ \\varepsilon = \\begin{pmatrix} \\sigma_{\\varepsilon}^{2} &amp; 0\\\\ 0 &amp; \\sigma_{\\varepsilon}^{2} \\end{pmatrix}\\)</p>"},{"location":"cs/ml/statml/#geometrical-interpretation-of-least-squares-regression","title":"Geometrical Interpretation of least squares regression","text":"<p>Like the graph suggests, \\(\\text{col} \\ \\mathbf{X}\\) is the hyperplane spanned by column vectors of \\(\\mathbf{X}\\). \\(\\mathbf{X} \\hat\\beta\\) could be considered as the linear combination of column vectors. Then our goal is to find \\(\\hat \\beta\\) to minimise the norm of \\(\\mathbf{y} - \\mathbf{X} \\hat{\\beta}\\), which can only be achieved when it is perpendicular to the hyperplane, that is, \\(\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\hat{\\beta}) = 0\\).</p> <p>Verification:</p> \\[\\begin{align} &amp;\\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y})\\\\ =&amp; \\ \\mathbf{X}^T (\\mathbf{I} - \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T) \\mathbf{y}\\\\ =&amp; \\ (\\mathbf{X}^T - \\mathbf{X}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T) \\mathbf{y}\\\\ =&amp; \\ 0 \\end{align}\\] <p>There are several important issues we need to consider for linear regression. </p> <p>The first issue is that whether there is a relationship between the response and predictors, in other word, is at least one of the predictors useful in predicting the response. We can test the null hypothesis \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\) versus the alternative \\(H_a\\): at least one \\(\\beta_j\\) is non-zero. This can be performed by using the F-statistics. Sometimes we only to test a particular subset of coefficients though. In hypothesis testing, type I error (expected error) and type II error (false negative) matter as well.</p> <p>The second issue is deciding on important variables, this process is called variable selection. Ideally we can do that by trying a lot of different models and evaluating their qualities using methods like Akaike information criterion (AIC), Bayesian information criterion (BIC), adjusted \\(R^2\\) etc. But in reality we cannot really try and test so many models, there are three classical approaches for this task: forward selection, backward selection, mixed selection. Lasso is one of the approach appeared in later years.</p> <p>The third issue is model fit.</p> <p>The fourth issue is prediction.</p>"},{"location":"cs/ml/statml/#classification","title":"Classification","text":"<p>This is the special case where outputs are discrete. Logistic regression, linear discrimant analysis and KNN would be investigated.</p> <p>// image here</p> <p>Under such scenario, linear regression does not work ideally, so we need to introduce new models.</p>"},{"location":"cs/ml/statml/#logistic-regression","title":"Logistic Regression","text":"<p>We let</p> \\[\\begin{cases} \\mathbb{P}(y_i = 1 | x_i) = \\frac{1}{1 + \\exp(-x_i^T \\beta)} \\textbf{ (sample version)}\\\\ \\mathbb{P}(Y = 1 | X) = \\frac{1}{1 + \\exp(-\\beta^T X)} \\textbf{ (population version)} \\end{cases}\\]"},{"location":"cs/pl/","title":"Programming Languages","text":""},{"location":"cs/pl/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Rust</li> <li>C++</li> </ul>"},{"location":"cs/pl/#the-proramming-paradigms","title":"The Proramming Paradigms","text":""},{"location":"cs/pl/c%2B%2B/","title":"C++","text":"<p>Notice</p> <p>This is COMP2012H Revision Note. Please it doesn't record all the knowledge taught, instead it only covers the part that I am not so clear about.</p>"},{"location":"cs/pl/c%2B%2B/#basics","title":"Basics","text":""},{"location":"cs/pl/c%2B%2B/#type","title":"type","text":"<p>both char and bool occupie one byte, for ascii it only has 128 characters, raw strings are of <code>const char *</code> type, all floating literals is treated as double by default unless there is a suffix 'f'. A narrowing conversion will result in a warning, can use <code>static-cast</code> to make it disappear.</p>"},{"location":"cs/pl/c%2B%2B/#const-vs-define","title":"const vs #define","text":"<p><code>const</code> can be type-checked during compilation, and memory is not allocated for constant definition with only a few exceptions. Simply <code>const int x = 10</code> still stores x in memory, but not in O3 optimization. If we print &amp;x at the end, then both unoptimised and O3 optimised program stores x in memory.</p> <p>for <code>#define</code>, it simply substitutes value inside the program.</p>"},{"location":"cs/pl/c%2B%2B/#variable-naming","title":"Variable naming","text":"<p>Allowed Characters: 0-9, a-z, A-Z, _ The first character cannot be a digit cannot use reserved word identifiers start with '_' are always used for system variables</p>"},{"location":"cs/pl/c%2B%2B/#reference","title":"Reference","text":"<p><pre><code>int a;\ndouble &amp;m = a;\n</code></pre> This results in error: non-const lvalue reference to type 'double' cannot bind to a value of unrelated type 'int'. The casting from int to double result in creating a temporary variable, and we cannot create non-const lvalue reference to a remporary variable.</p> <p>++x is lvalue, x++ is rvalue</p>"},{"location":"cs/pl/c%2B%2B/#operators","title":"Operators","text":"<p>For the modulo operator %:  <pre><code>5 % 2 = 1\n5 % -2 = 1\n-5 % 2 = -1\n-5 % -2 = -1\n</code></pre></p>"},{"location":"cs/pl/c%2B%2B/#control-flow","title":"Control Flow","text":"<p>Remember to add break / return for <code>switch</code>, default is for not catching anything, not any cases can have the same value, the expression must be evaluated to an integral value ie (integer, char, bool)</p>"},{"location":"cs/pl/c%2B%2B/#function","title":"Function","text":"<p>Return type is not part of the signature, if included, hard to infer which type to return. https://chortle.ccsu.edu/java5/Notes/chap34A/ch34A_14.html</p> <p>Overloading <pre><code>int f(int a, double b) {return 1989;}\nint f(double a, int b) {return 2022;}\nint main() {\n    cout &lt;&lt; f(89.64,19.89);\n\n    return 0;\n}\n</code></pre> will pop out an error: call of overloaded 'f(double, double)' is ambiguous</p>"},{"location":"cs/pl/c%2B%2B/#array","title":"Array","text":"<p>The array elements cannot be reference variables, because references are not objects and doesn't occupy the memory so doesn't have the address. </p> <p>Reference to an array, <code>int *b = a</code> would make it lose information about the underlying array (eg: iterators), should better use <code>int (&amp;b)[4] = a</code>, for functions: <code>int (&amp;func())[5]</code> <pre><code>// int *b = a;\nint (&amp;b)[4] = a;\nfor (int v: a) cout &lt;&lt; v &lt;&lt; endl;\ncout &lt;&lt; \"=========\\n\";\nfor (int v: b) cout &lt;&lt; v &lt;&lt; endl;\n</code></pre></p> <pre><code>int a[10];\n\nint main() {\n    cout &lt;&lt; typeid(a).name() &lt;&lt; \" \" &lt;&lt; typeid(&amp;a).name() &lt;&lt; \" \" &lt;&lt; typeid(&amp;a[0]).name() &lt;&lt; endl;\n    cout &lt;&lt; a &lt;&lt; \"  \" &lt;&lt; &amp;a &lt;&lt; \" \" &lt;&lt; &amp;a[0] &lt;&lt; endl;\n}\n// A10_i PA10_i Pi\n// 0x558fe4de8160  0x558fe4de8160 0x558fe4de8160\n</code></pre> <p>char [] vs char * <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    char s[6] = \"hello\";\n    cout &lt;&lt; reinterpret_cast&lt;void *&gt;(s) &lt;&lt; \" \" &lt;&lt; &amp;s &lt;&lt; endl;\n    const char *ss = \"hello\";\n    cout &lt;&lt; reinterpret_cast&lt;const void *&gt;(ss) &lt;&lt; \" \" &lt;&lt; &amp;ss &lt;&lt; endl;\n}\n// 0x7ffcad02ac0a 0x7ffcad02ac0a\n// 0x402006 0x7ffcad02ac00\n</code></pre> ss is a pointer the address of a pointer is not equal to the address it's holding. s is an array, and &amp;arr == arr, but not the same for pointers</p>"},{"location":"cs/pl/c%2B%2B/#recursion-vs-non-recursive-approach","title":"Recursion vs Non-recursive Approach","text":"<p>Recursion needs more memory and more computational name - has to memorize its current state, and passes control from the caller to the callee. - sets up a new data structure (you may think of it as a scratch paper for rough work) called activation record which contains information such as * where the caller stops * what actual parameters are passed to the callee * create new local variables required by the callee function * the return value of the function at the end - removes the activation record of the callee when it finishes. - passes control back to the caller.</p>"},{"location":"cs/pl/c%2B%2B/#struct","title":"Struct","text":"<p>struct-struct assignment by default is memberwise copy (bit by bit), even array can be copied by deep copy, for pointer it is shallow copy only. <pre><code>#include &lt;iostream&gt;\n\nusing namespace std;\n\nstruct example {\n    int hi;\n    int arr[4];\n    int *ptr;\n};\n\nint main()\n{\n    example bruh = {5, {1,2,3,4}, new int (5)};\n\n    example copy_bruh = bruh;\n    cout &lt;&lt; bruh.arr &lt;&lt; endl;\n    cout &lt;&lt; copy_bruh.arr &lt;&lt; endl;\n    cout &lt;&lt; endl;\n\n    cout &lt;&lt; bruh.ptr &lt;&lt; endl;\n    cout &lt;&lt; copy_bruh.ptr &lt;&lt; endl;\n\n    return 0;\n}\n/*\n0x7fffc8c90c44\n0x7fffc8c90c64\n\n0x56498b106eb0\n0x56498b106eb0\n*/\n</code></pre></p>"},{"location":"cs/pl/c%2B%2B/#g-compilation-command-makefile","title":"g++ compilation command &amp; makefile","text":"<p><code>myclass.h</code>, <code>libmyclass.a</code> (library file of the object code of implementation of all class member functions, this enforces information hiding)</p> <p>Build library command: <code>g++ -c xxx.cpp</code> first, then <code>ar rsuv libxxx.a xxx.o xxx.o xxx.o</code>. Compilation Command: <code>g++ -o main main.cpp -L -lmyclass</code></p>"},{"location":"cs/pl/c%2B%2B/#oop","title":"OOP","text":""},{"location":"cs/pl/c%2B%2B/#c-class","title":"C++ Class","text":"<p>Be careful for deleting <code>char*</code>, if it is a raw string, it should be <code>delete []</code>. <code>delete []</code> will call the class destructor on each array element in reverse order.</p> <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nint times = 0;\nstruct A {\n    int v;\n    A() {v = times++;}\n    ~A() {cout &lt;&lt; v &lt;&lt; endl;}\n};\n\nint main() {\n    A *arr = new A [5];\n    delete [] arr;\n}\n</code></pre> <p>Forward definition (same applies to function) <pre><code>class Cell;\n\nclass List {\n    int size;\n    Cell *data;\n    Cell x; // need Cell\n}\n\nclass Cell {\n    int info;\n    Cell *next;\n}\n</code></pre></p> <p>Non-static data members that are not having like <code>float real = 1.3;</code>, <code>float imag {0.5}</code> will have the values of their default member initializer, and undefined if there's no.</p> <pre><code>class A {\n  private:\n    int v;\n  public:\n    A() { cout &lt;&lt; v &lt;&lt; endl;}\n};\n\nint main() {\n    A *arr = new A [5];\n    delete [] arr;\n}\n</code></pre> <p><code>inline</code> is used for inside-header implementation, if implementation is inside class, the keyword is optional, otherwise it is compulsory. Another usage of inline is that by declaring a very short function <code>inline</code>, we serve a hint to the compiler to make it inline as function calling is expensive, but the compiler still has the freedom to choose it or not.</p> <p><code>protected</code>: accessible to - member functions and friends of the class, as well as - member functions and friends of its derived classes (subclasses)</p> <p>We can use <code>Word movie = {1, \"Titanic\"}</code> only if all data members are public</p> <p>Default behavior: copy constructor: perform memberwise assignment by calling the assignment operator</p> <p><code>A a();</code> does not actually mean initialization</p> <p>MIL: The order of the members in the list doesn\u2019t matter; the actual execution order is their order in the class declaration. Advantage: avoid using assignment operator if it is not appropriately defined (error-prone)</p> <p><code>reinterpret_cast&lt;void *&gt;(\")</code>, output the address even though it is char *</p>"},{"location":"cs/pl/c%2B%2B/#construction-and-destruction","title":"Construction and Destruction","text":"<p>has-a-relationship vs own-a-relationship Has-a-relationship: <pre><code>class A {};\nclass B {\n    A a;\n    B() = default; // B(): a() {}\n};\n</code></pre> Own-a-relationship <pre><code>class A {};\nclass B {\n    A* a;\n    B() = default; \n};\n</code></pre> The Clock object is constructed in the Postoffice constructor, but it is never destructed, since we have not implemented that, we need to manually delete it in the destructor.</p>"},{"location":"cs/pl/c%2B%2B/#inheritance-polymorphism","title":"Inheritance &amp; Polymorphism","text":"<p>Remember to include the father into MIL: <pre><code>struct A {\n    int val;\n    A(int val): val(val+10) {}\n    virtual void print_label() { cout &lt;&lt; \"A \" &lt;&lt; val &lt;&lt; \"\\n\"; }\n};\n\nstruct B: A {\n    int val;\n    B(int val) : A(val), val(val) {}\n    void play() {\n        cout &lt;&lt; \"play\\n\";\n    }\n    virtual void print_label() override { cout &lt;&lt; \"B \" &lt;&lt; val &lt;&lt; \"\\n\"; }\n};\n</code></pre></p> <p>Construction Order: 1. its parent 2. its data members (in order of their appearance) 3. itself</p> <p><pre><code>struct A {\n    virtual void print() const {\n        cout &lt;&lt; \"A\\n\";\n    }\n};\n\nstruct B : A {\n    virtual void print() const override {\n        cout &lt;&lt; \"B\\n\";\n    }\n};\n\nvoid plv(A a) {\n    a.print();\n}\n\nvoid plr(const A&amp; a) {\n    a.print();\n}\n\nvoid plp(const A* a) {\n    a-&gt;print();\n}\n\nint main() {\n    B b;\n    plv(b);\n    plr(b);\n    plp(&amp;b);\n}\n</code></pre> Using virtual function only helps with pointer + reference to avoid slicing, can't help with PBV. (this uses the dynamic binding technique, determine the called function at run time).</p> <p>A virtual function is declared using the keyword virtual in the class definition, and not in the member function implementation, if it is defined outside the class. Once a member function is declared virtual in the base class, it is automatically virtual in all directly or indirectly derived classes. Even though it is not necessary to use the virtual keyword in the derived classes, it is a good style to do so because it improves the readability of header files.</p> <p>static_cast vs dynamic_cast vs reinterpret_cast There's no any checking of static_cast and it does not consult RTTI so it runs faster than dynamic_cast. Dynamic_cast only works on pointer and reference of polymorphic class (with virtual functions), it checks and returns a null pointer if conversion of pointer fails (a valid complete object of the target type), for reference it will trigger runtime error.</p> <p>It's fine to use static_cast for upper casting ie inherited=&gt;base, but for base=&gt;inherited better use dynamic_cast.</p> <p>Do not rely on the virtual function mechanism during the execution of a constructor. <pre><code>class Base {\n  public:\n    Base() { cout &lt;&lt; \"Base's constructor\\n\"; f(); }\n    virtual void f() { cout &lt;&lt; \"Base::f()\" &lt;&lt; endl; }\n};\n\nclass Derived : public Base {\n  public:\n    Derived() { cout &lt;&lt; \"Derived's constructor\\n\"; }\n    virtual void f() override { cout &lt;&lt; \"Derived::f()\" &lt;&lt; endl; }\n};\n\nint main() {\n    Base* p = new Derived;\n    cout &lt;&lt; \"Derived-class object created\" &lt;&lt; endl;\n    p-&gt;f();\n}\n</code></pre> This is not a bug, but necessary \u2014 how can the derived object provide services if it has not been constructed yet? Similarly, if a virtual function is called inside the base class destructor, it represents base class\u2019 virtual function: when a derived class is being deleted, the derived-specific portion has already been deleted before the base class destructor is called!</p> <p>pointers and references to an ABC can be declared, but use it as PBV is prohibited.</p> <p>public / protected / private inheritance 1. Public inheritance preserves the original accessibility of inherited members: public \u21d2 public protected \u21d2 protected private \u21d2 private 2. Protected inheritance affects only public members and renders them protected. public \u21d2 protected protected \u21d2 protected private \u21d2 private 3. Private inheritance renders all inherited members private. public \u21d2 private protected \u21d2 private private \u21d2 private</p> <p>Private and protected inheritance do not allow casting of objects of derived classes back to the base class.</p>"},{"location":"cs/pl/c%2B%2B/#generic-programming","title":"Generic Programming","text":"<p><pre><code>/* General case */\ntemplate &lt;typename T&gt;\nconst T&amp; larger(const T&amp; a, const T&amp; b)\n{ cout &lt;&lt; \"general case: \"; return (a &lt; b) ? b : a; }\n\n/* Exceptional case */\ntemplate &lt;&gt;\nconst char* const &amp; larger(const char* const &amp; a, const char* const &amp; b)\n{ cout &lt;&lt; \"special case: \"; return (strcmp(a, b) &lt; 0) ? b : a; }\n</code></pre> const pointers match <code>const T&amp;</code>, if delete the second const it would result in type mismatch</p> <p><pre><code>// Here, x is a reference to an array of N objects of type T\ntemplate &lt;typename T, int N&gt;\nint f(T (&amp;x) [N]) { return N; }\n</code></pre> Return size of an array</p>"},{"location":"cs/pl/c%2B%2B/#operator-overloading","title":"Operator Overloading","text":"<p>= copy assignment operator vs copy constructor We can implement copy assignment operator by copy constructor (like copy-swap) or vice versa. <pre><code>Word(const Word&amp; w): str{nullptr} { cout &lt;&lt; \"Copy: \"; *this = w; }\n</code></pre></p> <p>copy assignment need to delete old data (rmb to avoid self-assignment) actually we can use copy and swap <pre><code>dumb_array&amp; operator=(const dumb_array&amp; other) {\n    dumb_array temp(other); // call the copy constructor\n    swap(*this, temp); // this content would be deleted after the function call\n\n    return *this;\n}\n</code></pre></p> <p>+=, = returns const reference avoid situation like a = b = c, this implicitly means (a.equal(b)).equal(c), which is not intended (it makes a = b, then a = c)</p> <p>+: member function vs global function <pre><code>Vector Vector::operator+(const Vector &amp;a);\nVector operator+(const Vector&amp; a, const Vector &amp;b);\n</code></pre></p>"},{"location":"cs/pl/c%2B%2B/#data-structure","title":"Data Structure","text":""},{"location":"cs/pl/c%2B%2B/#rvalue-reference-and-move-semantics","title":"rvalue Reference and Move Semantics","text":"<p>Revisit that a variable has dual rules: lvalue (read-write) or rvalue (read-only). More kinds of values: xvalue, glvalue, prvalue, this would be introduced later.</p> <p>rvalue Reference Definition <pre><code>T &amp;&amp; &lt;variable&gt; = &lt;temporary object&gt;\n</code></pre></p>"},{"location":"cs/pl/rust/","title":"Rust","text":"<p>Abstract</p> <p>Rust is a statically typed language so it must know the types of all variables at compile time. The language does not have a GC (Garbage Collector) but achieve the purposes by its unique functionalities called ownership and lifetime. Many of the examples and sentences here are directly adopted from The Rust Programming Language. </p>"},{"location":"cs/pl/rust/#basics","title":"Basics","text":""},{"location":"cs/pl/rust/#compile-and-run","title":"Compile and Run","text":"<p>When there's only one file, we can use <code>rustc main.rs</code> to compile (just like <code>javac</code> in <code>Java</code>) and then use <code>./main.rs</code> to run it.</p> <p>If things get complicated, we could make use of the powerful package manager of Rust, Cargo. It is easy to use, only several commands can satisfy most of the daily usage.</p>"},{"location":"cs/pl/rust/#useful-commands","title":"Useful Commands","text":"<p>Create a new project <pre><code>$ cargo new hello_cargo\n</code></pre> Build (compile) the project <pre><code>$ cargo build\n</code></pre> Compile and Run the project <pre><code>$ cargo run \n</code></pre></p>"},{"location":"cs/pl/rust/#hello-world-program","title":"Hello World Program","text":"<p><pre><code>fn main() {\n    println!(\"Hello, world!\");\n}\n</code></pre> As you can see, the syntax here is quite similar to C/C++, there are some differences though. <code>fn</code> here refers to function, and also <code>!</code> is added behind <code>println</code> as <code>println</code> in Rust is a macro. (The meaning of macro would be introduced later).</p>"},{"location":"cs/pl/rust/#compilation-runtime-error-of-rust","title":"Compilation / Runtime error of Rust","text":"<p>Rust can give very powerful static checking during compilation and it can help you figure out most of the bugs before actually running the code with useful error message. Runtime error in Rust is called <code>panic</code>.</p>"},{"location":"cs/pl/rust/#variables-and-mutability","title":"Variables and Mutability","text":"<p>we use <code>let</code> to define new variables, and we call this kind of value assignment as variable binding. Like in C++ there are variables and constants, in Scala there are <code>var</code> and <code>val</code>, we have mutable and immutable variables in Rust as well , but the variables are by default immutable as immutability has a lot of advantages (check Wikipedia page for detailed explanations). To make the variable mutable, we should add the keyword <code>mut</code> at the beginning. <pre><code>let x = 5;\nlet mut y = 6;\nconst THREE_HOURS_IN_SECONDS: u32 = 60 * 60 * 3;\n</code></pre></p> <p>Notice</p> <p>However, note that constants and immutable variables are different.  The naming convention for constants is using all upper cases. Constants are valid for the entire time when a program runs, within the scope they were declared in. Also, constants can only be set to a constant expression, not the result of a value that could only be computed during runtime.</p> <p>Variable scope is a concept that is frequently used, it refers to the range within a program for which an item is valid. </p> <p>For example: <pre><code>{\n    let s = \"hello\";\n}\n</code></pre> Inside the curly bracket there defines a scope, and <code>s</code> is valid inside, after going out of the scope, it becomes invalid, just like the local variable.</p> <p>We can declare a new variable with the same name as previous variable and we call this kind of operation as shadowing. <pre><code>let y = 5;\nlet y = y + 1;\n\n{\n    let y = y * 2;\n    println!(\"The value of y in the inner scope is: {y}\");\n}\n</code></pre> Like this one, we first initialize the immutable variable <code>y</code> by binding 5 to it. Then we shadow <code>y</code> by <code>y + 1</code>. Inside the curly brackets, we shadow <code>y</code> again, when the scope is over, the inner shadowing ends. So the output value should be 6.</p> <p>The following code can run smoothly: <pre><code>let spaces = \"   \";\nspaces = spaces.len();\n</code></pre> However, if we add <code>mut</code> to <code>spaces</code>. It would pop out errors because the compiler perceives it as mutating the type of the variable instead of shadowing, which is not allowed.</p>"},{"location":"cs/pl/rust/#data-type-i","title":"Data Type I","text":"<p>Info</p> <p>Here I will only introduce some basic data types, more complicated ones like String, Vector, Map would be introduced later as <code>Data Type II</code>.</p> <p>As shown previously, we don't always need to write out the type explicitly (<code>let y = 5;</code>) unless the compiler requires more about the type information of the variable. If doing so, it would be like: <code>let y: i32 = 5;</code>, it is called type annotation.</p> <p>There are two data type subsets, namely Scalar Types and Compound Types.</p>"},{"location":"cs/pl/rust/#scalar-type","title":"Scalar Type","text":""},{"location":"cs/pl/rust/#integer-type","title":"Integer Type","text":"<p><code>u32</code> is one of the integer types, where <code>u</code> refers to unsigned, <code>32</code> refers to the number of bits. Similarly, there are <code>i32</code>, <code>u16</code>, <code>u32</code>, <code>i128</code>, ... By basic CS knowledge, it is trivial to calculate the range of each type.  The <code>isize</code> and <code>usize</code> types depend on the architecture of the computer your program is running on, which is denoted in the table as \u201carch\u201d: 64 bits if you\u2019re on a 64-bit architecture and 32 bits if you\u2019re on a 32-bit architecture. We can call the functions <code>usize::MIN</code>, <code>usize::MAX</code>, <code>isize::MIN</code>, <code>isize::MAX</code>.</p> <p>The number literals can be represented under different basis. For example: <code>0xff</code>, <code>0o77</code>, <code>0b1100_0100</code>, <code>b'A'</code>, <code>345_678</code> etc. One of the unique features of Rust is that it can insert <code>_</code> inside the numbers to improve the readability like the binary number given above.</p> <p>We still need to handle the integer overflow issue in Rust. </p> <p>Wrap in all modes with the <code>wrapping_*</code> methods, such as <code>wrapping_add</code> Return the None value if there is overflow with the <code>checked_*</code> methods Return the value and a boolean indicating whether there was overflow with the <code>overflowing_*</code> methods Saturate at the value\u2019s minimum or maximum values with <code>saturating_*</code> methods</p> <pre><code>let of_x : u8 = 233;\nlet of_y : u8 = 133;\nof_x.wrapping_add(of_y); // 110\nof_x.checked_add(of_y); // None\nof_x.overflowing_add(of_y).1; // true\nof_x.saturating_add(of_y)); // 255\n</code></pre>"},{"location":"cs/pl/rust/#floating-point-type","title":"Floating-point Type","text":"<p>Only <code>f32</code> and <code>f64</code>.</p> <p>The boolean type and char type are similiar to C/C++.</p>"},{"location":"cs/pl/rust/#compound-type","title":"Compound Type","text":""},{"location":"cs/pl/rust/#tuple-type","title":"Tuple Type","text":"<p>Tuple has fixed length but allows its elements to have different types.  We can use pattern matching to destructure a tuple value as following: <pre><code>fn main() {\n    let tup = (500, 6.4, 1);\n\n    let (x, y, z) = tup;\n\n    println!(\"The value of y is: {y}\");\n}\n</code></pre> We could also access the components of them using the following syntax: <pre><code>tup.0; tup.1; tup.2;\n</code></pre></p>"},{"location":"cs/pl/rust/#array-type","title":"Array Type","text":"<p>The array still has fixed length but all elements inside must have the same type. It is not as flexible as the vector type as it has varible length. However, the arrays are useful if you want your data to be allocated on the stack rather than the heap. <pre><code>let a: [i32; 5] = [1, 2, 3, 4, 5];\nlet b = [3; 5]; // [3, 3, 3, 3, 3]\n</code></pre> We could access the elements by using the following syntax: <pre><code>a[0]; a[1]; a[2];\n</code></pre></p> <p>Rust can pop out runtime errors when we have invalid array element access. <pre><code>use std::io;\n\nfn main() {\n    let a = [1, 2, 3, 4, 5];\n\n    println!(\"Please enter an array index.\");\n\n    let mut index = String::new();\n\n    io::stdin()\n        .read_line(&amp;mut index)\n        .expect(\"Failed to read line\");\n\n    let index: usize = index\n        .trim()\n        .parse()\n        .expect(\"Index entered was not a number\");\n\n    let element = a[index];\n\n    println!(\"The value of the element at index {index} is: {element}\");\n}\n</code></pre> We neglect the very detail of this program at this moment, we only care about if we input <code>6</code> as index, the following error would pop out while C / C++ cannot: <pre><code>thread 'main' panicked at 'index out of bounds: the len is 5 but the index is 10', src/main.rs:19:19\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n</code></pre> This actually shows Rust's memory safety principles in action. It is very hard to debug if having invalid array elements access in C/C++ because it would trigger unexpected modification of data in other side of the program.</p>"},{"location":"cs/pl/rust/#function","title":"Function","text":"<p>A typical example: <pre><code>fn plus_two(x: i32, y: i32) -&gt; i32 {\n    x + y\n}\n</code></pre> We use <code>: i32</code> to give type annotation on the incoming arguments, and <code>-&gt;</code> refers to the return type. <code>()</code> (unit type) stands for nothing is returned.</p> <p>You may notice no <code>;</code> is put behind <code>x + y</code>, this shows the difference between statement and expression.</p> <p>Statements are instructions that perform a certain action, while expressions evaluate to a resulting value. Like the one above is an expression, where <code>;</code> should noe be added.</p> <p>Another example: <pre><code>let x = (let y = 6);\n</code></pre> This one is incorrect, if you want to bind the value of y to x, the correct code should be: <pre><code>let x = {\n    let y = 6;\n    y\n};\n</code></pre></p>"},{"location":"cs/pl/rust/#comments","title":"Comments","text":"<p>Comments in Rust are just similiar to C/C++. <pre><code>// Hello,\n// we are comments!\n</code></pre> Documentation Comments would be written later here.</p>"},{"location":"cs/pl/rust/#control-flows","title":"Control Flows","text":""},{"location":"cs/pl/rust/#if-expressions","title":"<code>if</code> Expressions","text":"<p>One Example (no parenthesis is needed): <pre><code>if number &lt; 5 {\n    println!(\"condition was true\");\n} else {\n    println!(\"condition was false\");\n}\n</code></pre></p> <p>Note that the following code cannot compiled unlike C/C++: <pre><code>fn main() {\n    let number = 3;\n\n    if number {\n        println!(\"number was three\");\n    }\n}\n</code></pre> The integer cannot be directly casted into bool. So should write <code>if number != 0</code>.</p> <p>Nested if is the same as C/C++.</p> <p>There's one useful syntactic sugar: <pre><code>let number = if condition { 5 } else { 6 };\n</code></pre></p> <p>Reminders</p> <p>The types of results of two expressions should be the same since <code>number</code> can only have one type.</p>"},{"location":"cs/pl/rust/#loops","title":"loops","text":""},{"location":"cs/pl/rust/#loop","title":"loop","text":"<p>This is the endless loop which can only be stopped by the <code>break</code> command: <pre><code>loop {\n    bruhbruhbruh;\n\n    if flag {\n        break;\n    }\n}\n</code></pre> If there are nested loops, we could label the loops and indicate their names during break. <pre><code>fn main() {\n    let mut count = 0;\n    'counting_up: loop {\n        println!(\"count = {count}\");\n        let mut remaining = 10;\n\n        loop {\n            println!(\"remaining = {remaining}\");\n            if remaining == 9 {\n                break;\n            }\n            if count == 2 {\n                break 'counting_up;\n            }\n            remaining -= 1;\n        }\n\n        count += 1;\n    }\n    println!(\"End count = {count}\");\n}\n</code></pre></p> <p>Besides, we could use <code>while</code> and <code>for</code> loop as well:</p> <p><code>While</code> loop: <pre><code>while number != 0 {\n    println!(\"{number}!\");\n\n    number -= 1;\n}\n</code></pre> <code>for</code> loop: <pre><code>let a = [10, 20, 30, 40, 50];\n\nfor element in a {\n    println!(\"the value is: {element}\");\n}\n</code></pre></p> <p><pre><code>for number in (1..4) {\n    println!(\"{number}!\");\n}\n</code></pre> Here <code>(1..4)</code> refers to <code>[1, 4)</code> just like Python, to refer to <code>[1, 4]</code>, we should use <code>(1..=4)</code> instead.</p>"},{"location":"cs/pl/rust/#ownership","title":"* Ownership","text":"<p>Info</p> <p>Ownership is Rust\u2019s most unique feature and has deep implications for the rest of the language. It enables Rust to make memory safety guarantees without needing a garbage collector, so it\u2019s important to understand how ownership works.</p> <p>It governs how a Rust program manages memory.</p> <p>Stack and Heap are two important data structures in memory management.</p> <ul> <li>Stack is FILO, all data stored on the stack must have a known, fixed size.</li> <li>Heap is less organized and stores data with unknown size at compile time or a size that might change. </li> </ul>"},{"location":"cs/pl/rust/#efficency-of-putting-new-data","title":"Efficency of putting new data","text":"<p>The memory allocator of heap would find an empty spot that is big enough if new data to be inserted, this is called allocating on the heap. Therefore, pushing to the stack is faster than allocating on the heap as the allocator does not need to search for a place to store new data, the new location is always at the top of stack.</p>"},{"location":"cs/pl/rust/#efficency-of-accessing-data","title":"Efficency of accessing data","text":"<p>It is still faster on the stack as it does not need to follow a pointer to get there.</p> <p>The main purpose of ownership is keeping track of what parts of code are using what data on the heap, minimizing the amount of duplicate data on the heap, and cleaning up unused data on the heap so you don\u2019t run out of space are all problems that ownership addresses. </p> <p>There are three ownership rules:</p> <ul> <li>Each value in Rust has an owner.</li> <li>There can only be one owner at a time.</li> <li>When the owner goes out of scope, the value will be dropped.</li> </ul>"},{"location":"cs/pl/rust/#data-type-ii","title":"Data Type II","text":""},{"location":"cs/pl/rust/#string-type","title":"String Type","text":"<p>The <code>String</code> type is far more complicated than it seems. <pre><code>let s1 = String::from(\"hello\");\nlet s2 = \"world\".to_string();\n</code></pre> We can convert raw string to the <code>String</code> type either by calling <code>String::from</code> or <code>.to_string()</code>.</p> <p>The internal structure looks like this:</p> <p></p> <p>The left part is stored on stack, consisting of: the length and capacity (we can ignore capacity at this moment) of the string, and the pointer <code>ptr</code> pointing to string content on the heap. We store it on the heap because the length of string might be changed later (e.g.: extension).</p> <p>We use different methods to append new raw string / char. <pre><code>let mut s1 = String::from(\"foo\");\nlet s2 = \"bar\";\ns1.push_str(s2);\nprintln!(\"s2 is {}\", s2);\n\nlet mut s = String::from(\"lo\");\ns.push('l');\n</code></pre></p> <p>We will cover the shallow and deep copying issue.</p>"},{"location":"cs/pl/rust/#vector","title":"Vector","text":""},{"location":"cs/pl/rust/#map","title":"Map","text":""},{"location":"daily/2023/","title":"2023\u5e74\u603b\u7ed3","text":"<p>\u6700\u540e\u4e00\u5929\u77ed\u6682\u5730\u56de\u987e\u4e0b\u4eca\u5e74</p>"},{"location":"daily/update/","title":"\u4e00\u70b9\u5c0fupdate","text":"<p>Notice</p> <p>2023.10.13</p> <p>Hello\u5927\u5bb6\u597d\uff0c\u597d\u4e45\u4e0d\u89c1\uff01\uff08\u867d\u7136\u6211\u89c9\u5f97\u4e5f\u6ca1\u4ec0\u4e48\u4eba\u4f1a\u770b\u8fd9\u4e2a\u535a\u5ba22333\uff09</p> <p>\u5df2\u7ecf\u597d\u4e45\u6ca1\u7ef4\u62a4\u8fc7\u8fd9\u4e2a\u7f51\u7ad9\u4e86\uff0c\u770b\u5230\u670b\u53cb\u7684note\u8fbe\u523030w\u5b57\u4e86\u624d\u60f3\u8d77\u4e86\u81ea\u5df1\u8fd8\u6709\u4e2a\u535a\u5ba2\uff0c\u90a3\u5c31\u8d81\u4eca\u5929day-off (\u5b9e\u9645\u4e0a\u8fd8\u6709\u95e85212\u4f46\u662f\u662f\u4e0b\u5348\u56db\u70b9\u534a\u624d\u5f00\u59cb\u4e0a\u8bfe\uff0c\u5c31\u4e0d\u60f3\u4e3a\u4e86\u4e00\u8282\u8bfe\u6765\u56de\u5b66\u6821\u4e86) \u7b80\u5355\u66f4\u65b0\u4e00\u4e0b\u7f62</p> <p>\u5176\u5b9e\u8fd9\u91cc\u539f\u672c\u662f\u6211\u5b66\u4e60\u8bfe\u7a0b\u6216\u8005\u81ea\u5b66\u65f6\u653e\u7b14\u8bb0\u7684\u5730\u65b9\uff0c\u4f46\u662f\u5df2\u7ecf\u4e00\u5e74\u591a\u6ca1\u6709\u66f4\u65b0\u4e86\uff08\u56e0\u4e3a\u61d2\uff09\uff0c\u800c\u4e14\u4e5f\u6ca1\u6709\u7528markdown\u8bb0\u7b14\u8bb0\u7684\u4e60\u60ef\uff0c\u6240\u4ee5\u5c31\u8fd8\u662f\u628a\u5b83\u6536\u8d77\u6765\u4e86\uff08\u5982\u679c\u5927\u5bb6\u8fd8\u60f3\u8981\u7b14\u8bb0\u7684markdown\u7684\u8bdd\u53ef\u4ee5\u8054\u7cfb\u6211\uff09</p> <p>\u4e2a\u4eba\u89c9\u5f97\u535a\u5ba2\u662f\u4e00\u4e2a\u5f88\u73cd\u8d35\u7684\u8bb0\u5f55\u81ea\u5df1\u5728\u67d0\u4e2a\u9636\u6bb5\u60f3\u6cd5\u7684\u5730\u65b9\uff0c\u867d\u7136\u5728\u670b\u53cb\u5708/QQ\u7a7a\u95f4/ig/\u7f51\u6613\u4e91\u6216\u8005\u5404\u79cd\u793e\u4ea4\u5a92\u4f53\u4e5f\u80fd\u8d77\u5230\u76f8\u5173\u7684\u4f5c\u7528\uff0c\u4f46\u8fd9\u4e9b\u5a92\u4f53\u66f4\u591a\u7684\u662f\u8bb0\u5f55\u4e86\u5f53\u65f6\u7684\u5fc3\u60c5\uff0c\u76f8\u5bf9\u957f\u7684\u611f\u60f3\u8fd8\u662f\u535a\u5ba2\u6bd4\u8f83\u9002\u5408\u4e9b\uff0c\u800c\u4e14\u6ca1\u90a3\u4e48\u591a\u4eba\u770b\u7740\u4e5f\u4e0d\u4f1a\u90a3\u4e48\u793e\u6b7b\u3002\u5df2\u7ecf\u60f3\u597d\u51e0\u4e2a\u60f3\u804a\u5f88\u4e45\u7684\u4e3b\u9898\u4e86\uff0c\u6709\u7a7a\u53ef\u4ee5\u66f4\u65b0\u4e00\u4e0b\uff08\u5e0c\u671b\u4e0d\u8981\ud83d\udc26\u6389\uff0c\u63a5\u4e0b\u6765\u53ef\u80fd\u662f\u7ed9\u4e24\u5e74\u5927\u5b66\u751f\u6d3b\u505a\u4e2a\u7b80\u8981\u603b\u7ed3\u7136\u540e\u5199\u4e00\u4e0b\u76ee\u524d\u5927\u4e09\u4e0a\u7684\u73b0\u72b6\uff0c\u4ee5\u53ca\u5199\u4e00\u4e0b\u81ea\u5df1\u600e\u4e48\u6210\u4e3aUP\u4e3b\u7684\uff0c\u548cmk\u7684\u53d1\u5c55\u8fc7\u7a0b</p> <p>\u4ee5\u53ca\u9009\u62e9\u7ee7\u7eed\u66f4\u65b0\u535a\u5ba2\u4e5f\u662f\u56e0\u4e3ainspired by\u4e00\u4e9b\u670b\u53cb\u505a\u7684\u5f88\u6709\u610f\u601d\u7684\u535a\u5ba2/\u7b14\u8bb0\u5982\u9e64\u7fd4\u4e07\u91cc\u7684\u7b14\u8bb0\uff0c\u70e7\u98ce\u7684\u7b14\u8bb0\u7b49\uff08\u6709\u7a7a\u6574\u4e00\u4e0b\u53cb\u94fe</p> <p>\u597d\uff0c\u5148\u53bb\u8d76\u51e0\u4e2adue\u518d\u56de\u6765\u7ee7\u7eed\u66f4\u65b0\uff01</p>"}]}